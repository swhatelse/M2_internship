# -*- coding: utf-8 -*-
# -*- mode: org -*-
#+startup: beamer
#+STARTUP: overview
#+STARTUP: indent
#+TAGS: noexport(n)

#+Title: Autotuning: Study of existing
#+AUTHOR:      Steven QUINITO MASNADA

#+EPRESENT_FRAME_LEVEL: 2

#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [11pt,xcolor=dvipsnames,presentation]
#+OPTIONS:   H:2 num:t toc:nil \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t

#+LATEX_HEADER: \usedescriptionitemofwidthas{bl}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[american]{babel}
#+LATEX_HEADER: \usepackage{ifthen,figlatex,amsmath,amstext,gensymb,amssymb}
#+LATEX_HEADER: \usepackage{boxedminipage,xspace,multicol}
#+LATEX_HEADER: %%%%%%%%% Begin of Beamer Layout %%%%%%%%%%%%%
#+LATEX_HEADER: \ProcessOptionsBeamer
#+latex_header: \mode<beamer>{\usetheme{Madrid}}
#+LATEX_HEADER: \usecolortheme{whale}
#+LATEX_HEADER: \usecolortheme[named=BrickRed]{structure}
# #+LATEX_HEADER: \useinnertheme{rounded}
#+LATEX_HEADER: \useoutertheme{infolines}
#+LATEX_HEADER: \setbeamertemplate{footline}[frame number]
#+LATEX_HEADER: \setbeamertemplate{headline}[default]
#+LATEX_HEADER: \setbeamertemplate{navigation symbols}{}
#+LATEX_HEADER: \defbeamertemplate*{headline}{info theme}{}
#+LATEX_HEADER: \defbeamertemplate*{footline}{info theme}{\leavevmode%
#+LATEX_HEADER:   \hbox{%
#+LATEX_HEADER:     \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
#+LATEX_HEADER:       \usebeamerfont{author in head/foot}\insertshortauthor
#+LATEX_HEADER:     \end{beamercolorbox}%
#+LATEX_HEADER:   \begin{beamercolorbox}[wd=.41\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
#+LATEX_HEADER:     \usebeamerfont{title in head/foot}\insertsectionhead
#+LATEX_HEADER:   \end{beamercolorbox}%
#+LATEX_HEADER:   \begin{beamercolorbox}[wd=.09\paperwidth,ht=2.25ex,dp=1ex,right]{section in head/foot}%
#+LATEX_HEADER:     \usebeamerfont{section in head/foot}\insertframenumber{}~/~\inserttotalframenumber\hspace*{2ex} 
#+LATEX_HEADER:   \end{beamercolorbox}
#+LATEX_HEADER:   }\vskip0pt}
#+LATEX_HEADER: \setbeamertemplate{footline}[info theme]
#+LATEX_HEADER: %%%%%%%%% End of Beamer Layout %%%%%%%%%%%%%
#+LATEX_HEADER: \usepackage{verbments}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{url} \urlstyle{sf}

#+LATEX_HEADER: \let\alert=\structure % to make sure the org * * works of tools
#+BEAMER_FRAME_LEVEL: 2


#+LATEX_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Topic}\tableofcontents[currentsection]\end{frame}}

* Related work on autotuning
** Dongarra
types de problème, type d'approch
** Wild
types de problème, type d'approche (code transformation Orio,
optimization)
- Discrete derivative-free constraint optimization
- Correlation between peformance and *code optimization across*
  *platforms* with different kernels
- Machine learning to learn good combination of optimization
  parameters on one plateform
** Fursin
- Discrete optimization
- Multi-objective optimization (e.g: size and performances)
- Possible to build model of compiler flags that works across
  applications
- Use of machine learning technics
- Models build using random search
- Training overhead reduce by Collective optimization
** Touati
** ASK
** BOAST
   - specific generation/metaprogramming  of kernels
   - requires involvement of the developper and code restructuration
   - allows optimizations that no compiler would dare
   - At the moment, no strategy for tuning code 
* Optimization overview
** Autotuning optimization problem
- *Nonsmooth* and *Empirical* objective function and constraints
- Both discrete and continuous parameters
- Large optimization space with potential interactions between parameters

** Search space strategy
*** A priori on the objective function
- Exploit information about the problem (e.g., convexity, locality)
- Derivative based methods (*local* search generaly based on *gradient* descent)
  - Non convex (, hence local minimum): randomized strategies (e.g., simulated annealing)
  - If derivation is not available (function too complex)
     - Pattern search (e.g., Nelder-Mead)
     - Mesh Adaptive Direct search
  - If derivation is not possible (empiric function): estimate with regressions (e.g., surrogates-based search)
  - If evaluation is costly: meta-models (e.g., krigging) but derivation of interpolation is dubious...
- Additional difficulty depending on whether the parameters are contrained or not

*** "No a priori" (or other kind of a priori)
- Other kind of (discrete) structure (e.g. permutation, binary vector, tree, ...)
- Different notion of locality, hence need to cover a largev part of the search space
- Based on heuristics: Naive sampling, Genetic algorithm, tabu search, ant colonies, swarm 

*** Many combinations of heuristics
  GPS: SEARCH, POLL, ...
  

* And in practice...
** Laplacian
   code, parameters, ...
   OpenCL
** Brute force exploration on Adonis
  - how much time (full, per configuration)
  - Results: actually not that stable
** Efficiency of the random sampling
  - Need for complex exploration scheme?  
  
  
  
  
* Models, exploration, notes :noexport:
  
  
  
  

** Mixed strategy
*** Global and local search
- Two phases 
- Global \to escape bad local optima
- Local \to refine solution
- E.g pattern search, simulated annealing
*** Derivative and derivative-free
- Partial knowledge
- Reduce exploration time
- E.g.: Generalized Pattern Search 

** Generalized Pattern Search
- Extended pattern search version
- For unconstrained and linearly constrained problems
- Iteration over two phases:
  - Global search \to SEARCH
    - Sampling the space to find interesting regions \to building a mesh
    - Try to improve current optimal elsewhere
    - Possible to use any methods \to Genetic Algo, surrogates based
      search,  etc...
  - Local search \to POLL
    - Exploiting interesting region to refine the solution
- Uses derivative informations to speedup POLL phase
    
* Models
** Reuseable
- "Deconstructing Iterative Optimization" \to Common working compiler
  flags combination across program 
- "Exploiting Performance Portability in Search Algorithms for
  Autotuning" \to correlation between code optimization and speedup
  across plateforms
- Build appromixations \to surrogates-based search (trust-regions
  algorithm) 
** Building knowledge
*** Machine learning
- Similar applications \to similar behaviors \to similar optimizations
- Building knowledge over iterative optimization
- Training overhead \to Collective optimization
- Used in GCC \to Milepost GCC

* Idea
** Guidelines
- Characterization of the autotuning optimization search problem
- Which algorithm are the most suited for each kind of problems
- Devise an adaptive approach



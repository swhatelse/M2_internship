# -*- coding: utf-8 -*-
# -*- mode: org -*-
#+startup: beamer
#+STARTUP: overview
#+STARTUP: indent
#+TAGS: noexport(n)

#+Title: Autotuning: Study of existing
#+AUTHOR:      Steven QUINITO MASNADA

#+EPRESENT_FRAME_LEVEL: 2

#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [11pt,xcolor=dvipsnames,presentation]
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t

#+LATEX_HEADER: \usedescriptionitemofwidthas{bl}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[american]{babel}
#+LATEX_HEADER: \usepackage{ifthen,figlatex,amsmath,amstext,gensymb,amssymb}
#+LATEX_HEADER: \usepackage{boxedminipage,xspace,multicol}
#+LATEX_HEADER: %%%%%%%%% Begin of Beamer Layout %%%%%%%%%%%%%
#+LATEX_HEADER: \ProcessOptionsBeamer
#+latex_header: \mode<beamer>{\usetheme{Madrid}}
#+LATEX_HEADER: \usecolortheme{whale}
#+LATEX_HEADER: \usecolortheme[named=BrickRed]{structure}
# #+LATEX_HEADER: \useinnertheme{rounded}
#+LATEX_HEADER: \useoutertheme{infolines}
#+LATEX_HEADER: \setbeamertemplate{footline}[frame number]
#+LATEX_HEADER: \setbeamertemplate{headline}[default]
#+LATEX_HEADER: \setbeamertemplate{navigation symbols}{}
#+LATEX_HEADER: \defbeamertemplate*{headline}{info theme}{}
#+LATEX_HEADER: \defbeamertemplate*{footline}{info theme}{\leavevmode%
#+LATEX_HEADER:   \hbox{%
#+LATEX_HEADER:     \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
#+LATEX_HEADER:       \usebeamerfont{author in head/foot}\insertshortauthor
#+LATEX_HEADER:     \end{beamercolorbox}%
#+LATEX_HEADER:   \begin{beamercolorbox}[wd=.41\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
#+LATEX_HEADER:     \usebeamerfont{title in head/foot}\insertsectionhead
#+LATEX_HEADER:   \end{beamercolorbox}%
#+LATEX_HEADER:   \begin{beamercolorbox}[wd=.09\paperwidth,ht=2.25ex,dp=1ex,right]{section in head/foot}%
#+LATEX_HEADER:     \usebeamerfont{section in head/foot}\insertframenumber{}~/~\inserttotalframenumber\hspace*{2ex} 
#+LATEX_HEADER:   \end{beamercolorbox}
#+LATEX_HEADER:   }\vskip0pt}
#+LATEX_HEADER: \setbeamertemplate{footline}[info theme]
#+LATEX_HEADER: %%%%%%%%% End of Beamer Layout %%%%%%%%%%%%%
#+LATEX_HEADER: \usepackage{verbments}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{url} \urlstyle{sf}

#+LATEX_HEADER: \let\alert=\structure % to make sure the org * * works of tools
#+BEAMER_FRAME_LEVEL: 2


#+LATEX_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Topic}\tableofcontents[currentsection]\end{frame}}

* Related work on autotuning
*** Dongarra
# types de problème, type d'approch
*** Wild
# types de problème, type d'approche (code transformation Orio,
# optimization)
- Discrete, derivative-free, and constraint optimization
- Correlation between peformance and *tuning across* *platforms*
- Learning good combination on one architecture and try to
  apply to another one 
- Strong correlation with similar architecture
#+BEGIN_LaTeX
\begin{figure}[tbh]
\centering
\vspace{-1.5mm}
\includegraphics[scale=0.3]{./img/20160302/correlation.png}
\includegraphics[scale=0.25]{./img/20160302/correlation2.png}
\end{figure}
#+END_LaTeX
*** Fursin
**** Milepost
- Discrete & Multi-objective optimization (e.g: size and performances)
- Possible to build model of compiler flags that works across
  applications
- Use of machine learning technics
- Models build using random search
# How is this information exploited?
- Training overhead reduce by Collective optimization
*** Touati :noexport:
*** ASK
- Accuracy of a model relies on the sampling
- Partitioning search into regions with different level of variance
- Region with more variance are allocated more samples

#+BEGIN_LaTeX
\begin{figure}[tbh]
\centering
\vspace{-1.5mm}
\includegraphics[scale=0.2]{./img/20160302/HSV_example.png}
\end{figure}
#+END_LaTeX

*** OpenTuner
- *Discontinuous*, *non-smooth* optimization
- Efficient of a search technic depends on the problem
- Adatping the search method to the particularities of the search
  space
- Testing multiple methods at the same time and keep those which
  performs better. Improvment are shared between methods.

*** BOAST
- specific generation/metaprogramming of kernels
- requires involvement of the developper and code restructuration
- allows optimizations that no compiler would dare
- At the moment, no strategy for tuning code 
* Optimization overview
*** Autotuning optimization problem
- *Nonsmooth* and *Empirical* objective function and constraints
- Both discrete and continuous parameters
- Large optimization space with potential interactions between parameters

** Search space strategy
*** A priori on the objective function
- Exploit information about the problem (e.g., convexity, locality)
- Derivative based methods (*local* search generaly based on *gradient* descent)
  - Non convex (hence local minimum): randomized strategies (e.g., simulated annealing)
  - If derivation is not available (function too complex)
     - Direct search (e.g., Nelder-Mead, pattern search)
     - Mesh Adaptive Direct search
  - If derivation is not possible (empiric function): estimate with regressions (e.g., surrogates-based search)
  - If evaluation is costly: meta-models (e.g., krigging) but derivation of interpolation is dubious...
- Additional difficulty depending on whether the parameters are constrained or not

*** "No a priori" (or other kind of a priori)
- Other kind of (discrete) structure (e.g. permutation, binary vector, tree, ...)
- Different notion of locality, hence need to cover a larger part of the search space
- Based on heuristics: Naive sampling, Genetic algorithm, tabu search, ant colonies, swarm 

*** Many combinations of heuristics
Generalized Pattern Search: 
- Global exploration \to SEARCH
  - Possible to use any technics (e.g. randomized, surrogate-based, etc...)
- Local exploration \to POLL

* And in practice...
** Laplacian
   code, parameters, ...
   OpenCL
** Brute force exploration on Adonis
  - how much time (full, per configuration)
  - Results: actually not that stable
** Efficiency of the random sampling
  - Need for complex exploration scheme?  
  
  
  
  
* Models, exploration, notes :noexport:
  
  
  
  

** Mixed strategy
*** Global and local search
- Two phases 
- Global \to escape bad local optima
- Local \to refine solution
- E.g pattern search, simulated annealing
*** Derivative and derivative-free
- Partial knowledge
- Reduce exploration time
- E.g.: Generalized Pattern Search 

** Generalized Pattern Search
- Extended pattern search version
- For unconstrained and linearly constrained problems
- Iteration over two phases:
  - Global search \to SEARCH
    - Sampling the space to find interesting regions \to building a mesh
    - Try to improve current optimal elsewhere
    - Possible to use any methods \to Genetic Algo, surrogates based
      search,  etc...
  - Local search \to POLL
    - Exploiting interesting region to refine the solution
- Uses derivative informations to speedup POLL phase
    
* Models :noexport:
** Reuseable
- "Deconstructing Iterative Optimization" \to Common working compiler
  flags combination across program 
- "Exploiting Performance Portability in Search Algorithms for
  Autotuning" \to correlation between code optimization and speedup
  across plateforms
- Build appromixations \to surrogates-based search (trust-regions
  algorithm) 
** Building knowledge
*** Machine learning
- Similar applications \to similar behaviors \to similar optimizations
- Building knowledge over iterative optimization
- Training overhead \to Collective optimization
- Used in GCC \to Milepost GCC

* Idea :noexport:
** Guidelines
- Characterization of the autotuning optimization search problem
- Which algorithm are the most suited for each kind of problems
- Devise an adaptive approach



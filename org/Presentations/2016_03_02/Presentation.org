# -*- coding: utf-8 -*-
# -*- mode: org -*-
#+startup: beamer
#+STARTUP: overview
#+STARTUP: indent
#+TAGS: noexport(n)

#+Title: Autotuning: Study of existing
#+AUTHOR:      Steven QUINITO MASNADA

#+EPRESENT_FRAME_LEVEL: 2

#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [11pt,xcolor=dvipsnames,presentation]
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t

#+LATEX_HEADER: \usedescriptionitemofwidthas{bl}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[american]{babel}
#+LATEX_HEADER: \usepackage{ifthen,figlatex,amsmath,amstext,gensymb,amssymb}
#+LATEX_HEADER: \usepackage{boxedminipage,xspace,multicol}
#+LATEX_HEADER: %%%%%%%%% Begin of Beamer Layout %%%%%%%%%%%%%
#+LATEX_HEADER: \ProcessOptionsBeamer
#+latex_header: \mode<beamer>{\usetheme{Madrid}}
#+LATEX_HEADER: \usecolortheme{whale}
#+LATEX_HEADER: \usecolortheme[named=BrickRed]{structure}
# #+LATEX_HEADER: \useinnertheme{rounded}
#+LATEX_HEADER: \useoutertheme{infolines}
#+LATEX_HEADER: \setbeamertemplate{footline}[frame number]
#+LATEX_HEADER: \setbeamertemplate{headline}[default]
#+LATEX_HEADER: \setbeamertemplate{navigation symbols}{}
#+LATEX_HEADER: \defbeamertemplate*{headline}{info theme}{}
#+LATEX_HEADER: \defbeamertemplate*{footline}{info theme}{\leavevmode%
#+LATEX_HEADER:   \hbox{%
#+LATEX_HEADER:     \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
#+LATEX_HEADER:       \usebeamerfont{author in head/foot}\insertshortauthor
#+LATEX_HEADER:     \end{beamercolorbox}%
#+LATEX_HEADER:   \begin{beamercolorbox}[wd=.41\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
#+LATEX_HEADER:     \usebeamerfont{title in head/foot}\insertsectionhead
#+LATEX_HEADER:   \end{beamercolorbox}%
#+LATEX_HEADER:   \begin{beamercolorbox}[wd=.09\paperwidth,ht=2.25ex,dp=1ex,right]{section in head/foot}%
#+LATEX_HEADER:     \usebeamerfont{section in head/foot}\insertframenumber{}~/~\inserttotalframenumber\hspace*{2ex} 
#+LATEX_HEADER:   \end{beamercolorbox}
#+LATEX_HEADER:   }\vskip0pt}
#+LATEX_HEADER: \setbeamertemplate{footline}[info theme]
#+LATEX_HEADER: %%%%%%%%% End of Beamer Layout %%%%%%%%%%%%%
#+LATEX_HEADER: \usepackage{verbments}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{url} \urlstyle{sf}

#+LATEX_HEADER: \let\alert=\structure % to make sure the org * * works of tools
#+BEAMER_FRAME_LEVEL: 2

#+LATEX_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Topic}\tableofcontents[currentsection]\end{frame}}

#+LATEX_HEADER: %\usepackage{biblatex}
# #+LATEX_HEADER: \bibliography{../../biblio.bib}
# #+LATEX_HEADER: \usepackage{cite}

* Laboratory book
*** Laboratory book
- Available at https://github.com/swhatelse/M2_internship
- Journal and notes
* Related work on autotuning
*** Jack Dongarra
# S. Moore : University of Tennessee / Oak Ridge National Laboratory
# types de problème, type d'approch
- ATLAS \to specialized autotuner for dense linear algebra
- Relaxed 1-D linear search \to require to know good starting point and
  interaction between optimizations 
*** Stefan M. Wild - Aragonne National Laboratory
# A. Roy, P. Balaprakash, P. D. Hovland
# types de problème, type d'approche (code transformation Orio,
# optimization)
- Discrete, derivative-free, and constraint optimization
- Correlation between peformance and *tuning across* *platforms*
  \cite{RoyBalHovWil2015} 
- Learning good combinations on one architecture and try to
  apply to another one 
- Strong correlation with similar architectures
#+BEGIN_LaTeX
\begin{figure}[tbh]
\centering
\vspace{-1.5mm}
\includegraphics[scale=0.3]{../../img/20160302/correlation.png}
\includegraphics[scale=0.25]{../../img/20160302/correlation2.png}
\end{figure}
#+END_LaTeX
*** Grigory Fursin
# Y. Kashnikov, A. W. Memon, M. O'Boyle et al
- Discrete & Multi-objective optimization (e.g: size and performances)
- Possible to build model of compiler flags that works across
  applications \to Milepost GCC \cite{fursin:hal-00685276}
- Use of machine learning technics
- Models build using random search
# How is this information exploited?
- Training overhead reduce by Collective optimization \to cTunning
  \cite{memon:hal-00944513} 
*** ASK
# Exascale Computing research / LRC ITACA / Intel Corporation
- Exascale Computing research, UVSQ \cite{deoliveiracastro:hal-00952307} 
- Accuracy of a model relies on the sampling 
- Partitioning search into regions with different level of variance
- Region with more variance are allocated more samples

#+BEGIN_LaTeX
\begin{figure}[tbh]
\centering
\vspace{-1.5mm}
\includegraphics[scale=0.18]{../../img/20160302/HSV_example.png}
\end{figure}
#+END_LaTeX

*** OpenTuner
- MIT - CSAIL \cite{Ansel:2014:OEF:2628071.2628092} 
- *Discontinuous*, *non-smooth* optimization
- Efficient of a search technic depends on the problem 
- Adatping the search method to the particularities of the search
  space 
- Testing multiple methods at the same time and keep those which
  performs better. Improvment are shared between methods.

*** BOAST
- specific generation/metaprogramming of kernels
- requires involvement of the developper and code restructuration
- allows optimizations that no compiler would dare
- At the moment, no strategy for tuning code 
* Optimization overview
*** Autotuning optimization problem
- *Nonsmooth* and *Empirical* objective function and constraints
- Both discrete and continuous parameters
- Large optimization space with potential interactions between parameters

** Search space strategy
*** A priori on the objective function
- Exploit information about the problem (e.g., convexity, locality)
- Derivative based methods (*local* search generaly based on *gradient* descent)
  - Non convex (hence local minimum): randomized strategies (e.g., simulated annealing)
  - If derivation is not available (function too complex)
    - Direct search (e.g., Nelder-Mead, pattern search)
    - plus randomized strategies
  - If derivation is not possible (empirical function): estimate with regressions (e.g., surrogates-based search)
  - If evaluation is costly: meta-models (e.g., krigging) but derivation of interpolation is dubious...
- Additional difficulty depending on whether the parameters are constrained or not

*** "No a priori" (or other kind of a priori)
- Other kind of (discrete) structure (e.g. permutation, binary vector, tree, ...)
- Different notion of locality, hence need to cover a larger part of the search space
- Based on heuristics: Naive sampling, Genetic algorithm, tabu search, ant colonies, swarm 

*** Many combinations of heuristics
Generalized Pattern Search \cite{DBLP:journals/mp/AbramsonAD04}: 
- Global exploration \to SEARCH
  - Possible to use any technics (e.g. randomized, surrogate-based,
    etc...)
  - Try find better solution than the local optimal elsewhere
- Local exploration \to POLL
  - Try refine the solution by exploring a mesh
- Use derivative informations when available to speedup the exploration

* And in practice...
*** Laplacian
# code, parameters, ...
- Parameters:
  - x_component_number [1,2,4,8,16]
  - vector_length [1,2,4,8,16]
  - y_component_number [1,2,3,4]
  - temporary_size [2,4]
  - vector_recompute [true,false]
  - load_overlap [true,false]

- OpenCL Nvidia implementation

#  - how much time (full, per configuration)
#  - Results: actually not that stable

*** Brute force exploration on GPU NVIDIA
- Search space = 800
- Each version tested 4 time on 4 image sizes.
  
#+BEGIN_LaTeX
\begin{figure}[tbh]
\centering
\vspace{-1.5mm}
\includegraphics[scale=0.4]{../../img/not_ordered.png}
\end{figure}
#+END_LaTeX
*** One possible order 
#+BEGIN_LaTeX
\begin{figure}[tbh]
\centering
\vspace{-1.5mm}
\includegraphics[scale=0.4]{../../img/another_order.png}
\end{figure}
#+END_LaTeX
*** And another one
#+BEGIN_LaTeX
\begin{figure}[tbh]
\centering
\vspace{-1.5mm}
\includegraphics[scale=0.4]{../../img/ordered_1.png}
\end{figure}
#+END_LaTeX

*** Can we achieve an efficient "search space simplification" ?
- Facilitates the search
- Do we still need for complex exploration scheme ?
- Comparison between search technics which are correctly adapted to
  the autotuning search problem, with a correctly presented problem
  (especially with the random search with high number of parameters)
* Summary
*** Objectives
- Find how to present the problem to have the nicest shape as possible
  in order to facilitate the search
- Characterization of the autotuning optimization search problems
- Which algorithms are the most suited for each kind of problems
- Devise an adaptive approach

* End

*** End
Thank you for your attention 

*** References
:PROPERTIES:
:BEAMER_OPT: fragile,allowframebreaks,label=
:END:   
\bibliographystyle{alpha}
\bibliography{../../biblio}

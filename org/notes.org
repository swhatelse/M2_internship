#+TITLE:       Notes
#+AUTHOR:      Steven QUINITO MASNADA
#+BABEL:       :tangle yes

* Topic
** Overview
   The goal of optimization in autotuning is try to reach the best
   speedup by finding what is the best combination of optimization
   parameters. To know if a combination of optimization parameters
   brings a speedup, one need to evaluate it. The difficult is that we
   can not really know by advance what are the best combinations,
   moreover the search space can be huge and the brute force approach
   is not viable. The autotuning optimization is a generally modelized
   as mathematical optimization problem where we want to minize or
   maximize y = f(x_0, x_1, ..., x_n) with x being an optimization
   parameters. The search space being huge it not possible is not
   possible to try every possiblities, it exists methods to find good
   optimization parameters using only part of the search space. Some
   are more effective than other due to the characteristics of the
   problem. We can distinguish three main categories of approach. The
   first one does not rely on any kind of informations about the 
   problem like random search or genetic algorithms works. The second
   one use partial informations or "on the fly" about the problem like
   direct search methods. And the last one use informations build on
   top of models.  
** Problem
   - We are not really sure on how the different methods compare to
     each others because of the lack of informations about the
     conditions of experiments on some papers or just because they are
     not comparison with the state of the art methods.  
     
   - It is not trivial to know what is the most suited optimization
     methods because we do not really how the problem looks like, it
     is very application and platform dependent. We only know its
     global characteristics, non-smooth objective and constraints.

   - The first thing is that we do not really why genetic algorithm
     perform well.
   
   - The second is genetic algorithms are a random approach and do not
     use any knwoledge about the problem. Knowing that it might be
     possible to do things smarter and it might exist methods that use
     knwoledge about the problem and that are more efficient than
     a random approach.

* Goal
  - Try to characterize the problem of optimization.
    If we know the shape of the problem we can use this knowledge to be
    more efficient.
  - Understand why certain methods works better for some class of problem.
  - Find efficient solution and implement it as an optimizer.
* State of the art
** Heuristic-based search methods
*** Random
*** Genetic algorithms
*** Simulated Annealing
     - Made for discrete search space.
     - Explore the space using neighbours (states that are similars to
       the considered state).
     - The algo visit neighbours to find better values.
     - Can accept worse neighbours
     - Temperature \to time
     - Energie \to result return by the system
     - Start Global to slowly go to local.
     - The temperature defined the probability of accepting worse
       value \to avoid to get stuck in a bad local optima. As long as
       the temperature decrease the probability of accepting worse
       value decrease and make smaller moves.

*** Particule swarm 
*** Nelder Mead Simplex
     - Direct search methods
     - Derivative-free
** Statistic-based methods
*** Surogates-based search
     - Use approximation models about what could be the behavior of an
       application in order to make an estimation of the correct
       parameters. The effectiveness relies on the accuracy of the
       model. The idea is to use knowledge about the problem to know
       where to concentrate the search. 
     - The difficult part here is the construction of the model
       # How to build a model?
     - Some methodology using this approach:
       - _Response surface methodology_
         - Relationship between response variables and explanatory
           variables through a set of experiments
         - Model = second degree polynomial
       - _Kriging_ 
       - _Support Vector Machine_
         - Classification \to gives in which category belong the problem
           and thus to which model suit the most.
       - _Trust-region algorithm_
         Try to fit a quadratic model into a "trusted region". If it
         fits the region is expanded(increases the size) else
         contracted(decreases the size).
* Biblio
** TODO A comparison of Search Heuristics for Empirical Code Optimization
*** Summary
    - Comparing:
      - Random \to simple and effective
      - Simplex
      - Particule Swarm
      - Orthogonal
      - Genetic Algorithm
      - Simulated Annealing
    - Combining code optimization and compilation flags
*** Link
    http://netlib.org/utk/people/JackDongarra/PAPERS/gco_search.pdf
*** Bibtex
    #+BEGIN_SRC 
    @conference {icl:418,
	title = {A Comparison of Search Heuristics for Empirical Code Optimization},
	booktitle = {The 3rd international Workshop on Automatic Performance Tuning},
	year = {2008},
	note = {5},
	pages = {421-429},
	publisher = {IEEE},
	organization = {IEEE},
	address = {Tsukuba, Japan},
	author = {Keith Seymour and Haihang You and Jack Dongarra}
    }
    #+END_SRC
** DONE Can Search algorithms save large-scale automatic performance tuning?
*** Summary
    - Discrete derivative-free constraint nonconvex optimization
    - *Formulation* of the autotuning search problem *as mathematical*
      *optimization problem*.
    - *Algorithms* need to be *adapted to the autotuning problem* \to na√Øve
      Nelder Mead simplex vs modified one gives better results. 
      Due to the fact the normal version is made for continuous
      variables and here they only tested discrete variables.
    - Random search seems to be efficient for problems when the
      problem has lots of parameters that give good results. It also
      tends to have a bigger rate of failure (compilation or runtime
      errors) because does not keep track of hidden incorrect
      combination of parameters. 
*** Questions
**** What is the importance of the formalization in a mathematical optimization problem? 
     # I didn't really get how they use this particularity. 
     It helps to modelize the problem as a function and to correctly
     find what to take into account. For example what is the most
     suited metric for the objective function \to for optimization
     problem it is more logical to take a metric we know the bound.
     E.g. time to compute a pixel (bounded by zero) vs Flops (unknown
     bound) 
*** Link
    http://ac.els-cdn.com/S1877050911002924/1-s2.0-S1877050911002924-main.pdf?_tid=4f7211d8-c9b7-11e5-ab07-00000aacb35d&acdnat=1454422665_1e1560e8379ea8cb8f740e08b18b05bf
*** Bibtex
    #+BEGIN_SRC 
    @article{Balaprakash20112136,
        title = "Can search algorithms save large-scale automatic performance tuning?",
        author = "Prasanna Balaprakash and Stefan M. Wild and Paul D. Hovland",
        journal = "Procedia Computer Science",
        volume = "4",
        pages = "2136 - 2145",
        year = "2011",
        note = "Proceedings of the International Conference on Computational Science, ICCS 2011",
        issn = "1877-0509",
        doi = "10.1016/j.procs.2011.04.234"
    }
    #+END_SRC
** DONE An Experimental Study of Global and Local Search Algoritms in Empirical Perfomance Tuning
*** Summary
    - Study the comparison between global and local search
      - Random
      - Genetic Algorithm
      - Simulated annealing
      - Nelder Mead simplex
      - Surrogate based search \to trust-region algorithm
    - Strong time constraint \to getting the best variant in a short
      time
    - Local algo
      - Nelder Mead
      - Surrogates based search
      - Very efficient if we know where to search
        Initial parameters have to be chosen carefully \to sensitive
    - Global algo
      - Generally longer due to their explorative nature
      - Reducing their degree of exploration improve their results
        But again here we need to know where to search
*** Questions / remarks
    - Average on only 10 run maybe not enough \to missing confidence interval
    - We cannot really link the different experiment they did because
      each time they benchmark a different application.
    - We already know that they use a version of the simplex adapted
      to the autotuning problem but how well adapted are the other
      algorithms? Especially GA and SA. In the last experiment
      reducing the exploration degree henances their
      performances. Does that mean that for the previous experiences
      GA and SA are not well adapted and there is some biais?
      Generally we lack information about how are tuned GA and SA 
      so we cannot really make some conclusion.
    - If we restrict to much the factor of exploration of GA and Sa
      are they equivalent to local search?
*** Link
    http://www.mcs.anl.gov/papers/P1995-0112.pdf
*** Bibtex
    #+BEGIN_SRC 
    @incollection{PBSWPHLNCS13,
    title       = "An Experimental Study of Global and Local Search Algorithms in Empirical Performance Tuning",
    author      = "Prasanna Balaprakash and Stefan M. Wild and Paul D. Hovland",
    booktitle   = "High Performance Computing for Computational Science - VECPAR 2012, 
    10th International Conference, Kobe, Japan, July 17-20, 2012, Revised Selected Papers.",
    series      = "Lecture Notes in Computer Science",  
    editors     = "M.J. Dayd\'e, O. Marques, K. Nakajima",    
    year        = "2013",
    publisher   = "Springer",
    pages       = "pp. 261--269",
    doi         = "10.1007/978-3-642-38718-0_26",
    isbn        = "978-3-642-38717-3"
    }
    #+END_SRC
** A Batch, Derivative-Free Algorithm for Finding Multiple Local Minima
*** Link
    http://www.mcs.anl.gov/papers/P5228-1114.pdf
** TODO A Taxonomy of Constraints in Simulation-Based Optimization
*** Link
    http://arxiv.org/pdf/1505.07881.pdf
** TODO Empirical Performance modeling of GPU kernels using active learning
*** Link
    http://www.mcs.anl.gov/papers/P4097-0713_1.pdf
** TODO Empirical Performance Tuning of Dense Linear Algebra Sofware
*** Summary
*** Link
    http://www.netlib.org/utk/people/JackDongarra/PAPERS/master.pdf
*** Bibtex
   #+BEGIN_SRC 
   
   #+END_SRC
** TODO Deconstructing Iterative optimization
*** Summary
    - Compiler flags optimizations only
    - It is possible to learn a combination of optimization from data
      set that suit most of other data set \to analyzing the datasets.
    - Interesting to see how they study :
      - If the iterative optimization is efficient across datasets 
        - They collected a big sample, 
        - Found what are the best optimizations 
        - Kept common optimizations 
        - apply it to others samples
      - Why it is efficient \to by analyzing the results.
*** Link
    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.5061&rep=rep1&type=pdf
*** Remarks
    - Interesting to see how I can get into the problem, what kind of
      question I could ask to myself, and for the methodology.
** Direct search
*** Templating and Automatic Code for Performance with Python
**** Summary
    - Tested 3 variant of NOMAD but no comparison with empirical
      approach.       
**** Questions/Remarks
    - They claim they don't use a empirical or heuristic method but
      they use the mesh-adaptive direct search and from what I
      understand it is clearly a heuristic method
    - No comparison with other state of the art approaches (empirical
      methods)
    - This paper does not seems to to directly bring
      interesting stuffs, it is more presentation of another code
      generator. But there are interesting link of mesh-adaptive
      direct search.
**** Link
    http://www.gerad.ca/~orban/_static/templating.pdf
**** Bibtex
    #+BEGIN_SRC 
    @book{orban2011templating,
    title={Templating and Automatic Code Generation for Performance with Python},
    author={Orban, D. and Groupe d'{\'e}tudes et de recherche en analyse des d{\'e}cisions (Montr{\'e}al, Qu{\'e}bec)},
    series={Cahiers du G{\'E}RAD},
    url={https://books.google.fr/books?id=QfwutwAACAAJ},
    year={2011},
    publisher={Groupe d'{\'e}tudes et de recherche en analyse des d{\'e}cisions}
    }
    #+END_SRC
*** "Direct Search" Solution of Numerical and Statistical Problems
**** Summary
     - Two type of moves:
       - Exploratory \to get knowledge(inference about successful or not
         moves). Sampling the space. Try to find better solution
         elsewhere. Moves in directions defined by patterns of a size
         that evolves (increases if sucessful move else decreases).
         Define an area for the current iterate.
       - Pattern \to Use knowledge to minimize f. Search around a base
         point.
**** Questions/Remarks
     - I don't really get how a pattern is built.
**** Link
    http://delivery.acm.org/10.1145/330000/321069/p212-hooke.pdf?ip=194.199.27.221&id=321069&acc=ACTIVE%20SERVICE&key=7EBF6E77E86B478F%2E9B0CC472860F67C6%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=751594268&CFTOKEN=50107111&__acm__=1455030106_ccd8cb9aa66e698e79840f74cfa6aa91
**** Bibtex
    #+BEGIN_SRC 
    @article{Hooke:1961:DSS:321062.321069,
    author = {Hooke, Robert and Jeeves, T. A.},
    title = {`` Direct Search'' Solution of Numerical and Statistical Problems},
    journal = {J. ACM},
    issue_date = {April 1961},
    volume = {8},
    number = {2},
    month = apr,
    year = {1961},
    issn = {0004-5411},
    pages = {212--229},
    numpages = {18},
    url = {http://doi.acm.org/10.1145/321062.321069},
    doi = {10.1145/321062.321069},
    acmid = {321069},
    publisher = {ACM},
    address = {New York, NY, USA},
    } 
    #+END_SRC
*** On The Convergence of Pattern Search Algorithm
**** Summary
**** Link
     http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.4715&rep=rep1&type=pdf
**** Bibtex
*** Generalized pattern searches with derivative information
**** Summary
    - Direct search / pattern search methods
    - For unconstrained and linearly constrained problems
    - Two phases:
      - SEARCH \to global exploration to find interesting regions
        - Mesh construction :
          - possible to use any technics \to Genetic algo, surrogate
            based searches, etc...
          - Try to improve the current optimal
          - Exploratory move not restricted by the size of the mesh \to
            more various exploration at the begining.
      - POLL \to local exploration to examine interesting regions
        (around a base). Points to visit are define by a pattern
        
    - Henanced version of pattern search \to Use derivative information
      when available to speed the POLL phase
**** Questions/remarks
**** Links
    http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=35C815204C5D2F2BD69ADA2BD527763A?doi=10.1.1.381.255&rep=rep1&type=pdf
**** Bibtex
    #+BEGIN_SRC bibtex :tangle ./biblio.bib
      @article{DBLP:journals/mp/AbramsonAD04,
      author    = {Mark A. Abramson and
                   Charles Audet and
                   John E. Dennis Jr.},
      title     = {Generalized pattern searches with derivative information},
      institution = {Air Force Institute of Technology (Department of Mathematics and Statistics) and 
                     Ecole Polytechnique de Montr√©al (GERAD) and 
                     Rice University (Department of Computational and Applied Mathematics)},
      journal   = {Math. Program.},
      volume    = {100},
      number    = {1},
      pages     = {3--25},
      year      = {2004},
      url       = {http://dx.doi.org/10.1007/s10107-003-0484-5},
      doi       = {10.1007/s10107-003-0484-5},
      timestamp = {Wed, 06 May 2015 19:49:45 +0200},
      biburl    = {http://dblp.uni-trier.de/rec/bib/journals/mp/AbramsonAD04},
      bibsource = {dblp computer science bibliography, http://dblp.org}
      }
    #+END_SRC
*** Mesh Adaptive Direct Search Algorithms for Constrained Optimization
**** Summary
     - Extended version of Generalized Pattern Search
     - Infinte set of directions
     - Works with nonsmooth functions
**** Questions/remarks
**** Link
    http://epubs.siam.org/doi/pdf/10.1137/040603371
**** Bibtex
    #+BEGIN_SRC 
    @ARTICLE{Audet04meshadaptive,
    author = {Charles Audet and J. E},
    title = {Mesh adaptive direct search algorithms for constrained optimization},
    journal = {SIAM Journal on optimization},
    year = {2004},
    volume = {17},
    pages = {2006}
    }
    #+END_SRC
** DONE Crowdtuning: Systematizing auto-tuning using predictive modeling and crowdsourcing
*** Summary
    - Combine machine learning and statistical analysis
      The idea is that some problems have similarities and require
      similar optimizations. Assuming that we can use previous 
      knowldege (model) to find what could be the best configurations
      to explore first.
    - Learn correlation between code optimization, compilation flags,
      hardware, and programs
    - Collecting big sample to modelize and predict behavior
*** Link
    https://hal.inria.fr/hal-00944513/document
*** Bibtex
    #+BEGIN_SRC bibtex :tangle ./biblio.bib
      @inproceedings{memon:hal-00944513,
      TITLE = {{Crowdtuning: systematizing auto-tuning using predictive modeling and crowdsourcing}},
      AUTHOR = {Memon, Abdul Wahid and Fursin, Grigori},
      URL = {https://hal.inria.fr/hal-00944513},
      institution = {UVSQ and INRIA Saclay},
      BOOKTITLE = {{PARCO mini-symposium on ''Application Autotuning for HPC (Architectures)''}},
      ADDRESS = {Munich, Germany},
      YEAR = {2013},
      MONTH = Sep,
      PDF = {https://hal.inria.fr/hal-00944513/file/paper.pdf},
      HAL_ID = {hal-00944513},
      HAL_VERSION = {v1},
      }
    #+END_SRC
** DONE Milepost GCC: machine learning enabled self-tuning
*** Summary
    - Learning over iterative optimization
    - Can target multi-objective optimization
    - Using random search to build the models
    - Used two models:
      - Probabilistic:
        - Attributes are independants
        - Use probability distribution of good solutions and take the
          mode. 
        - To learn the model:
          - They first learn the distribution of good solution (one
            that bring a speedup more than 98%) on each programs (in
            the training set)using uniform sampling. 
          - They estimate the distribution of a program by using the
            euclidian distance (take the closest program \to 1 nearst
            neighbors) 
      - Transductive:
        - Analyze interaction between attributes
        - Model built using a decision tree
        - Correlation of compilation optimization and program
          characteristics that give good speedup
        - Easier to analyze than probabilistic model
    - Using the solutions of the closest neighbors works for the same
      plateform
*** Questions/remarks
**** For the probabilistic model, do they use a different uniform sampling on each program? 
**** Why in the probabilistic model good solutions are those that give 98% of speedup  and in the transductive model it is 95% ?
**** Using the solutions of the closest neighbors works for the same plateform, does this work in cross platform context?
**** The models are built using a random sampling, 
*** Links
    http://fursin.net/papers/fkmp2011.pdf
*** Bibtex
    #+BEGIN_SRC bibtex :tangle ./biblio.bib
      @article{fursin:hal-00685276,
      TITLE = {{Milepost GCC: Machine Learning Enabled Self-tuning Compiler}},
      AUTHOR = {Fursin, Grigori and Kashnikov, Yuriy and Memon, Abdul Wahid and Chamski, Zbigniew and Temam, Olivier and Namolaru, Mircea and Yom-Tov, Elad and Mendelson, Bilha and Zaks, Ayal and Courtois, Eric and Bodin, Fran{\c c}ois and Barnard, Phil and Ashton, Elton and Bonilla, Edwin and Thomson, John and Williams, Christopher K. I. and O'Boyle, Michael},
      URL = {https://hal.inria.fr/hal-00685276},
      institution = {INRIA Saclay (HiPEAC) and 
                     University of Versailles and
                     University of Edinburgh},
      JOURNAL = {{International Journal of Parallel Programming}},
      PUBLISHER = {{Springer Verlag}},
      VOLUME = {39},
      PAGES = {296-327},
      YEAR = {2011},
      DOI = {10.1007/s10766-010-0161-2},
      HAL_ID = {hal-00685276},
      HAL_VERSION = {v1},
      }
    #+END_SRC
** DONE Exploiting Performance Portability in Search Algorithms for Autotuning
*** Summary
    - Correlation between peformance and *code optimization across*
      *platforms* with different kernels
    - Supervised machine learning to build the model \to recursive
      partitionning approach \to random forest
    - Two strategy:
      - Pruning \to *Bad* optimizations an machine A are expected to be
        *bad* on machine B.
        Use the model from machine A, sample at random machine B,
        make prediction and evaluate only set that are *smaller* than a
        thresold. 
      - Biasing \to *Good* optimizations an machine A are expected to be
        *good* on machine B
        Almost like pruning but try unevaluated configurations with the
        *smallest* predicted value.
    - Keeping the most interesting search space and then exploring at
      random (obvisously more efficient approach can be used)
    - The more the architectures similar are the more correlated the
      results. But what is the more important is the correlation
      between high performing combination.
    - They found correlation in high peforming combination between
      intel CPU(Sandy Bridge and Westmere), IBM Power7 and Xeon Phi
      but not really with ARM. 
*** Questions / remarks
    - I don't if I can really trust their results because they didn't
      mention the number of time they repeated the experiment and it
      might be some variablity especially for the random search.
    - It is strange that the model-free version is very close to the
      model-based one and some time it is able to find good
      configuration faster even if the search time is longer.
      Are their models well suited?
*** Link
    http://www.mcs.anl.gov/papers/P5397-0915.pdf
*** Bibtex
    #+BEGIN_SRC bibtex :tangle ./biblio.bib 
    @techreport{RoyBalHovWil2015,
    author = {A. Roy and P. Balaprakash and P. D. Hovland and S. M. Wild},
    date-added = {2015-09-11 18:59:31 +0000},
    date-modified = {2015-09-22 03:02:04 +0000},
    institution = {Argonne National Laboratory},
    number = {ANL/MCS-P5400-0915},
    title = {Exploiting performance portability in search algorithms for autotuning},
    year = {2015}
    }
    #+END_SRC
** TODO Solving Derivative-Free Nonlinear Least Squares Problems with POUNDERS
*** Link
    http://www.mcs.anl.gov/papers/P5120-0414.pdf
** TODO Multi-Objective Optimization of HPC Kernels for Performance, Power, and Energy
** TODO The Speedup Test
*** Summary
*** Remarks/Questions
*** Link
    https://hal.archives-ouvertes.fr/file/index/docid/443839/filename/SpeedupTestDocument.pdf
*** Bibtex
    #+BEGIN_SRC 
    @article{touati:hal-00764454,
    TITLE = {{The Speedup-Test: A Statistical Methodology for Program Speedup Analysis and Computation}},
    AUTHOR = {Touati, Sid-Ahmed-Ali and Worms, Julien and Briais, S{\'e}bastien},
    URL = {https://hal.inria.fr/hal-00764454},
    NOTE = {Article first published online: 15 OCT 2012},
    JOURNAL = {{Concurrency and Computation: Practice and Experience}},
    PUBLISHER = {{Wiley}},
    VOLUME = {25},
    NUMBER = {10},
    PAGES = {1410-1426},
    YEAR = {2013},
    DOI = {10.1002/cpe.2939},
    KEYWORDS = {Program performance evaluation and analysis ; code optimisation ; statistics},
    PDF = {https://hal.inria.fr/hal-00764454/file/Speedup-Test-Article-freestyle.pdf},
    HAL_ID = {hal-00764454},
    HAL_VERSION = {v1},
    }
    #+END_SRC
** TODO OpenTuner: An Extensible Framework for Program Autotuning
*** Summary
    - Adapting the search method to particularities of the search
      space.
    - The search space have to be structured
    - Testing multiple methods at the same time and keep those which
      performs better. Improvment are shared between methods.
*** Remarks/Questions
*** Link
    http://groups.csail.mit.edu/commit/papers/2014/ansel-pact14-opentuner.pdf
*** Bibtex
    #+BEGIN_SRC bibtex :tangle ./biblio.bib
      @inproceedings{Ansel:2014:OEF:2628071.2628092,
      author = {Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and Ragan-Kelley, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman},
      title = {OpenTuner: An Extensible Framework for Program Autotuning},
      booktitle = {Proceedings of the 23rd International Conference on Parallel Architectures and Compilation},
      series = {PACT '14},
      year = {2014},
      isbn = {978-1-4503-2809-8},
      location = {Edmonton, AB, Canada},
      pages = {303--316},
      institution = {Massachusetts Institute of Technology},
      numpages = {14},
      url = {http://doi.acm.org/10.1145/2628071.2628092},
      doi = {10.1145/2628071.2628092},
      acmid = {2628092},
      publisher = {ACM},
      address = {New York, NY, USA},
      keywords = {autotuner, optimization},
      } 
    #+END_SRC
** TODO ASK: Adaptative sampling Kit for Performance Characterization
*** Summary
    - Empirical, blackbox function
    - Excessive pruning \to missing optimal solutions.
    - Dividing the search space into area with different variance
      upper bound using classication, regression trees and ANOVA.
    - Regions with more variance are allocated more sample.
*** Link
*** Bibtex
    #+BEGIN_SRC bibtex :tangle ./biblio.bib
      @inproceedings{deoliveiracastro:hal-00952307,
        TITLE = {{ASK: Adaptive Sampling Kit for Performance Characterization}},
        AUTHOR = {De Oliveira Castro, Pablo and Petit, Eric and Beyler, Jean Christophe and Jalby, William},
        URL = {https://hal.archives-ouvertes.fr/hal-00952307},
        BOOKTITLE = {{Euro-Par 2012 Parallel Processing}},
        ADDRESS = {Greece},
        institution = {Exascale Computing Research and 
                       University of Versailles (USVQ) and
                       Intel Corporation},
        PUBLISHER = {{springer}},
        VOLUME = {7484},
        PAGES = {89-101},
        YEAR = {2012},
        DOI = {10.1007/978-3-642-32820-6\_11},
        KEYWORDS = {performance ; sampling ; adaptive},
        HAL_ID = {hal-00952307},
        HAL_VERSION = {v1},
      }
    #+END_SRC
** TODO Annotation-Based Empirical Performance Tuning Using Orio
*** Summary
    - Only use random search, Nelder Mead Simplex and simulated
      annealing for the moment
** TODO Precimonious: Tuning Assistant for Floating-Point Precision
*** Summary
    - Tuning the level of precision of floating point variables
    - Use of delta debugging search
** TODO [ [[file:~/Dropbox/IMAG/M2/Stage/BOAST/papers/nitro_ipdps2014.pdf][nitro_ipdps2014]] ] Nitro: A Framework for Adaptive Code Variant Tuning
*** Link
    [[http://www.cs.utah.edu/~sauravm/docs/nitro_ipdps2014.pdf]]
*** Summary
    The solution proposed on this paper is an framework for
    applications for which the characteristics of the input have a
    strong influence on the choice of the code variants. They use
    machine learning technics to build models that estimate the best
    version that suits the data. 
    They emphasize on the expression of the code variant and the
    meta-informations about the characteristics of the data.
    Hence, expressing them correctly is important in order to build an
    accurate model that will map the data to best estimated variant.
    There is two steps, determining the characteristics of the data and
    them find the best variants.
    To build the model, supervised learning is used in the training
    phase using support vector machines.
    They distribute the overhead of the training using incremental
    learning technics.
** TODO [ [[file:~/Dropbox/IMAG/M2/Stage/BOAST/papers/SIGOPS_paper.pdf][SIGOPS_paper]] ] An Effective Git And Org-Mode Based Workflow For Reproducible Research
*** Link
    https://hal.inria.fr/hal-01112795/file/SIGOPS_paper.pdf
*** Bibtex
    #+BEGIN_SRC bibtex
      @article{stanisic:hal-01112795,
        TITLE = {{An Effective Git And Org-Mode Based Workflow For Reproducible Research}},
        AUTHOR = {Stanisic, Luka and Legrand, Arnaud and Danjean, Vincent},
        URL = {https://hal.inria.fr/hal-01112795},
        JOURNAL = {{Operating Systems Review}},
        PUBLISHER = {{Association for Computing Machinery}},
        VOLUME = {49},
        PAGES = {61 - 70},
        YEAR = {2015},
        DOI = {10.1145/2723872.2723881},
        KEYWORDS = {Org-mode ; Git ; Reproducible Research ; Starpu-simgrid},
        PDF = {https://hal.inria.fr/hal-01112795/file/SIGOPS_paper.pdf},
        HAL_ID = {hal-01112795},
        HAL_VERSION = {v1},
      }
    #+END_SRC
** TODO [ [[file:~/Dropbox/IMAG/M2/Stage/BOAST/papers/extremal_quant_reg.pdf][extremal_quant_reg]] ] EXTREMAL QUANTILE REGRESSION
*** Link
    http://arxiv.org/pdf/math/0505639.pdf
** TODO [ [[file:~/Dropbox/IMAG/M2/Stage/BOAST/papers/Brewer95.pdf][Brewer95]] ] High-Level Optimization via Automated Statistical Modeling
** TODO [ [[file:~/Dropbox/IMAG/M2/Stage/BOAST/papers/ipdps09.pdf][ipdps09]] ] A Scalable Auto-tuning Framework for Compiler Optimization
* Mathematical notions
** Non-smooth
   Discontinous and non differentiable functions. 
   Abrupt bends.
** Convexe sets
   A convexe set is a where we can take any pair of point of the set
   drawing a segment between them and any point of the segment is in
   the set.
   Exemple of convexe set:
     - affine set
     - cone
     - polyhedra
     - norm ball and and cones
* Questions
** Autotuning is an optimization problem but what are its specificities if it had some? 
   Because if there are some specificities, we can then modelize and
   approximite what are the best parameters. 
** What is the good representation to use?
   The order of the paramters have an importance to the representation
   of the problem.
*** Is there other thing than the order of the parameter that we could take into account?
** Can we use information about the code from the compiler to make some infererences and guess what are the promising optimizations?
   Maybe coupling this with the information about the platform.
** What is the size of the search space of BOAST?
    There is no given size, it depends on the user's problem
** Are we interested in using knowledge from other platforms and applications or only making prediction "on the fly" of the application we want to optimize?
** Is linear regression a surrogates-based models?

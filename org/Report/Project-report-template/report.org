#+TITLE: Semi-Automatic Performance Optimization of HPC Kernels
#+LANGUAGE: en
#+Author: Steven QUINITO MASNADA
#+TAGS: noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LaTeX_CLASS: memoir
#+LaTeX_CLASS_OPTIONS: [12pt, a4paper]
#+OPTIONS: H:5 title:nil author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil
#+BABEL: :session *R* :cache yes :results output graphics :exports both :tangle yes 

#+LATEX_HEADER:\usepackage[french,english]{babel}
#+LATEX_HEADER:\usepackage [vscale=0.76,includehead]{geometry}                % See geometry.pdf to learn the layout options. There are lots.
# #+LATEX_HEADER:\geometry{a4paper}                   % ... or a4paper or a5paper or ... 
# #+LATEX_HEADER:\geometry{landscape}                % Activate for for rotated page geometry
# #+LATEX_HEADER:\OnehalfSpacing
# #+LATEX_HEADER: \setSingleSpace{1.05}
# #+LATEX_HEADER:\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
#+LATEX_HEADER:\usepackage{amsmath}
#+LATEX_HEADER:\usepackage{fullpage}
#+LATEX_HEADER:\usepackage{mathptmx} % font = times
#+LATEX_HEADER:\usepackage{helvet} % font sf = helvetica
#+LATEX_HEADER:\usepackage[latin1]{inputenc}
#+LATEX_HEADER:\usepackage{relsize}
#+LATEX_HEADER:\usepackage{listings}

#+BEGIN_LaTeX
%Style des têtes de section, headings, chapitre
\headstyles{komalike}
\nouppercaseheads
\chapterstyle{dash}
\makeevenhead{headings}{\sffamily\thepage}{}{\sffamily\leftmark} 
\makeoddhead{headings}{\sffamily\rightmark}{}{\sffamily\thepage}
\makeoddfoot{plain}{}{}{} % Pages chapitre. 
\makeheadrule{headings}{\textwidth}{\normalrulethickness}
%\renewcommand{\leftmark}{\thechapter ---}
\renewcommand{\chaptername}{\relax}
\renewcommand{\chaptitlefont}{ \sffamily\bfseries \LARGE}
\renewcommand{\chapnumfont}{ \sffamily\bfseries \LARGE}
\setsecnumdepth{subsection}


% Title page formatting -- do not change!
\pretitle{\HUGE\sffamily \bfseries\begin{center}} 
\posttitle{\end{center}}
\preauthor{\LARGE  \sffamily \bfseries\begin{center}}
\postauthor{\par\end{center}}

\newcommand{\jury}[1]{% 
\gdef\juryB{#1}} 
\newcommand{\juryB}{} 
\newcommand{\session}[1]{% 
\gdef\sessionB{#1}} 
\newcommand{\sessionB}{} 
\newcommand{\option}[1]{% 
\gdef\optionB{#1}} 
\newcommand{\optionB}{} 

\renewcommand{\maketitlehookd}{% 
\vfill{}  \large\par\noindent  
\begin{center}\juryB \bigskip\sessionB\end{center}
\vspace{-1.5cm}}
\renewcommand{\maketitlehooka}{% 
\vspace{-1.5cm}\noindent\includegraphics[height=14ex]{logoINP.png}\hfill\raisebox{2ex}{\includegraphics[height=7ex]{logoUJF.jpg}}\\
\bigskip
\begin{center} \large
Master of Science in Informatics at Grenoble \\
Master Math\'ematiques Informatique - sp\'ecialit\'e Informatique \\ 
option \optionB  \end{center}\vfill}
% End of title page formatting

\option{$<$option-name$>$}
%\title{ Project Title }%\\\vspace{-1ex}\rule{10ex}{0.5pt} \\sub-title} 
\author{Author Name}
\date{ $<$Defense Date$>$} % Delete this line to display the current date
\jury{
Research project performed at $<$lab-name$>$ \\\medskip
Under the supervision of:\\
$<$supervisor's first-name and last-name, supervisor's institution$>$\\\medskip
Defended before a jury composed of:\\
$[$Prof/Dr/Mrs/Mr$]$ $<$first-name last-name$>$\\
$[$Prof/Dr/Mrs/Mr$]$ $<$first-name last-name$>$\\
$[$Prof/Dr/Mrs/Mr$]$ $<$first-name last-name$>$\\
$[$Prof/Dr/Mrs/Mr$]$ $<$first-name last-name$>$\\
}
\session{$[$June$]$\hfill 2016}
#+END_LaTeX

#+BEGIN_LaTeX
\selectlanguage{english} % french si rapport en français
\frontmatter
\begin{titlingpage}
\maketitle
\end{titlingpage}

%\small
\setlength{\parskip}{-1pt plus 1pt}

\renewcommand{\abstracttextfont}{\normalfont}
\abstractintoc
\begin{abstract} 
Text 
\end{abstract}
\abstractintoc
\renewcommand\abstractname{R\'esum\'e}
\selectlanguage{english}% french si rapport en français

\cleardoublepage

\tableofcontents* % the asterisk means that the table of contents itself isn't put into the ToC
\normalsize

\mainmatter
\SingleSpace

#+END_LaTeX

# #+BEGIN_abstract
#   Blablabla
#   \newpage
# #+END_abstract

* Plan 
** Introduction [3/3]
*** Why is performance optimization difficult
   - In HPC code optimization crucial to exploit very complex hardware.
     Cannot wait for the next generation to bring speedup because it
     does not (Frequency not higher but more cores and henanced ISA). 
     - many cores \to heavy parallelism \to need to program parallel
     - pipelining ILP \to 
     - vector support \to SIMD \to need to work with vector
     - cache hierarchies \to need to exploit data locality
     - GPUs! \to different way of programming (than CPU)
   - HPC plaforms have many \ne hardware \to code optimizations not portable.
     Porting application to another platform is time consumming and
     can be very tricky.
   - Many attempts in the last decade to automate the generation of
     optimized code
*** Code generation and opportunities
    - The compiler approach: loop unrolling, vectorization, automatic
      parallelization, loop nest transformation, etc. Yet, many
      opportunities are not exploited as it is too difficult to
      exploit them automatically. Sometimes, the source code has to be
      rewritten in a slightly different way to enable the compiler to
      be effective
    - Parametric optimization:
      - The source-to-source transformation (C to C, Fortran to Fortran,
        ...). Framework for transform code. Orio. Serge
        Guelton. Difficile mais limité à un seul langage, et
        exploitation d'accelérateurs différents difficile. Ça ne se
        permettra jamais de changer le mapping des données en mémoire
      - Meta-programming approach: allow the programmer to propose
        optimizations that the compiler would not be allowed to do
        (because of the language or because it would require information
        on the application that cannot be given to the compiler).
        # But it is also the case with source-to-source transformation
        # right?    
*** Optimizing: the auto-tuning approach
    - Many optimization options: compiler flags, source-to-source
      transformations, higher-level modifications (tile/block/vector
      size). Each combination represents an implementation.      
    - Auto-tuning: consider all this as a huge optimization problem
      and try to find the best combination. Many techniques (genetic
      algorithms, simulated annealing, tabu search, machine learning,
      ...) depending on the problem constraints. But mainly two
      problems:
      - the time needed for such optimization
      - knowing whether further optimizations could be expected or not
        (peak performance is generally useless and the optimization
        process is so complex that it's hard to know how it really
        performed) is difficult and même si tu sais qu'il devrait être
        possible de faire mieux, tu sais pas vraiment où, comment( cf
        of genetic algo on the full search space), ...
*** Our goal
    Investigate the design of a semi-automatic optimization framework,
    where the applicaiton developer has control and understands how
    the optimization works and guides it.

    We relied on BOAST, a metaprogramming... (Semi automatic approach
     \to gives back power to the user, framework ruby generating
     portable code in C, Fortran, OpenCL. DSL)  and investigate various
     statistical techniques inspired from the design of expeirments
     field that emphasizes on reducing experiment cost.
*** My contribution
    - (Complex methods used but no explanation on why they work)
    - Prevent biased measurement
    - Try a simple approach and try to understand it deeply
      - Getting knowledge from the problem to guide the user:
        - Take into account hypothesis \to use the knowledge of the user
          1. Sampling the space
          2. Model find 
             - Removing useless factors
             - Refine the model \to add quadratic terms, 1/x,
               interactions, etc...
          3. Fix parameters to prune the search space and add removed
             factors.
          4. Back to 1 until we are able to fix all parameters values   
          
        - Linear regression methods to model the search space \to
          finding good model based on hypothesis. Allow the user to
          check this hypothesis. And understand the problem.
          - Try OLS \to problem with regression of expectation
            heteroscedacity + non uniform noise
          - Solution \to quantile regression
            - Pb with rq \to error to compute std. err, etc...
            - Used iterated weighted least square 
              Pb to make inferences \to biased R-squared and std. error
              # Are std.err biased to?
              How to compute CI?
              
        - Modeling
          - Start generic \to go specific
            Over specification \to biased
          - Sampling is crucial \to Design of experiments \to reducing number of experiments
            What design of experiment to use?
            - Random
            - LHS
            - Screening
            - D-Optimal
            How to use them? Copying with constraint
            - Start without hypothesis on the model otherwise \to biais
            - Add point with hypothesis \to D-Opt
            
*** Structure of the report
** Problem analysis [0/2]
   - Huge search space \to need to explore only part of it \to
     optimization problem.
   - Interactions between parameters
   - Non-smooth and empirical objective function
   - Combination of discrete and continuous parameters
   - Constraint optimizations
     Represent unfeasible points.
        
** Context [0/2]
*** HPC/architectures     
    - Crucial for science and business
    - To get performance \to exploit hardware \to take characteristics into account
      - Many cores
      - GPUs
      - vector support
      - cache levels
      - ILP \to break instruction dependencies
    - Architecture \ne from a HPC to another
      Specialized code \to not portable
*** Compilation
*** Auto-tuning
*** BOAST
** State of the art on Autotuning [2/4]
   - What is autotuning
     paramters \to represents different version/implementation

  # Maybe an overview of machine learning in general
  - Reuse knowledge of previous experience (generalization) \to machine
    learning. For different problem \to re-usability. 
    What is machine learning and why it is useful in auto-tuning.
    Generally exhaustive search costly training phase \to
    reducing impact. Classification \to which strategy to apply.
    - Small vs. Big
    - Milepost GCC \to learning characteristics of a program to
      predict what are the good combinations, optimization
      across programs. Predict good configuration using the
      distribution of good combination by taking the mode.
      Reuse knowledge across programs
    - Stefan Wild \to Learning combination across platform
      Worked for similar platforms. Search space pruning \to random
      search.
      Reuse knowledge across platforms
    - Opentuner \to which optimization technics for a given problem
      because the efficiency of a technics depends on the
      structure of the problem.
    - Incremental training \to Nitro using active learning
    - Collective tuning \to crowdtuning, Milepost
      Models stored in a common database and continuously updated.

  Optimization: exhaustive search is unfeasible.

  - "Direct search". The efficiency (ability to find the
    (near)-optimal solution and possibly in the fewest possible
    experiments) depends on the structure of the problem.
    - Main techniques:
      - Gradient descent: ferrari, a priori = local, geometry, convexity.
        - Issues: 
          - partly wrong hypothesis (geometry, convexity): simulated
            annealing, many local searches (genetic algorithms in some
            sense)
          - experimental estimation (empirical function)  :
            surrogates, etc. *local* approximation
            Usefull to remove the noise and facilitate the search
          - derivative estimation: Nelder Mead Simplex
        - \to many heuristics that combine all or part of the different
          previous approaches depending on how much the various
          hypothesis are wrong or not. Their efficiency highly depends
          on these hypothesis.
    - Some people have thus developed framework to characterize the
      optimization space.
      - ASK \to Emphasis on the sampling because important for the
        accuracy of the model \to complex sampling pipeline with
        different surrogate methods( bayesian regression,
        interpolation, etc... ). _Global modeling requires complex
        models and numerous experiments_.
    Illustration with a few tools:
    - Orio \to source to source annotation based autotuner 
      - random search, Nelder Mead Simplex and simulated annealing.
      - greeding algorithm for local search at the end of gobal.
    - OPAL \to Use direct search combinations of heuristics \to
      Mesh-adaptive direct-search \to pattern search.
      Global *and* local search \to work by iterative phase:
      - Sampling the space \to finding region of interest
      - Refining the solution
    - In some cases, the problem structure is known and one has an
      idea of where the optimal solution is but the structure of the
      space in this neighborhood is too complex. Some fall back to
      Exhaustive search \to Atlas Linear search, know where to search \to
      need to know the problem well.

  Primary Goals:
  - semi-automatic, almost interactive ? more global approach where
    the relevance of the hypothesis can be evaluated
  - optimize at low cost, need to prune the search space
  - from previous experience, generalization from an arch to another
    seems very difficult

  Somehow similar approach:
  - Getting knowledge on the fly \to regression, interpolation
    - Brewer \to linear regression for the modelization to predict
      objective function and root finding  or kind of greedy
      descent for the optimization.
      Find correct model automatically
      Not recent paper \to architecture have evolved. Is linear
      regression still ok?
** State of the art design of experiments [0/2]
   - Using less point as possible:
     - LHS
       For continuous space
     - D-Optimal
       Require to know the model
     - Fractional design
       Screening design \to Take the extreme values
** Methods and material [0/2]
  - Reproducible work
    - Lab book on github  
    - Literate programming 
    - org mode
  - OpenCL
  - Result validation against bruteforce
  - Comparison with random, gradiant search, and genetic algorithm
*** Case study
    # Maybe this should go in experiments
****  Laplacian
      - Optimizations explanation
        - Vectorization \to vector length
        - Synthetize loading \to load overlap
        - Tilling \to y component number
        - Number of threads \to elements number
        - Size of temporary results \to temporary size
          Reducing pressure on registers? If high usage of registers?
          If not high usage of registers overhead of casting?
        - Size of a work group \to threads number
        - Shape of work group \to lws y
      - 23100 combinations
      - Minimization
      - Test 5 sizes of images \to mean
**** Matrix product?
      - Optimizations explanation

** Contribution [0/12]
*** Envisioned general approach[0/1]
    1. DoE
       - Sampling the space wisely
       - Use linear regression OLS:
         - remove factors from the model
         - model and optimize
    2. Loop back to 1 to refine the model
*** Controlling measurement [0/1]
    - Time per pixel \to total time / number of pixel. Because we test
      different size of image.
    - min(x_1,...,x_10) ? how to protect against potential warm-up
      - Energy saving mode of current hardware(CPU and GPUs)
      - Mostly present just after the compilation of the kernel.
      - 4 runs \to take the minimum
    - randomizing to protect against bias, even for full search
      space. But run and image size not randomized.
*** Linear regression of expectation: why it cannot work and how it can be circumvented [1/3]
**** Least Squared regression and non uniform noise  
    - Assumptions:
      - homoscedasticity but pb we have heteroscedasticity
        - Why is it a problem?
          - Unbiased coefficient estimate but biased std error and thus
            R-squared \to more difficult know if a model is correct
          - But it is still ok if the error law is the same everywhere
      - But we don't know anything about the noise and normal
        distribution of the noise is assumed. We cannot do anything
        about that because in our case the noise come from complex
        interactions between parameters.
        Possible to reduce it by fixing values but it is not always
        possible to do that e.g. if for all the parameters the noise
        falls the same law. But we still have some difficult to find
        model due to the other parameters.        
    - Tracks general tendency of the impact of factors
    - 2 cases:
      - heteroscedasticity + same error law \to minimum can be predict
      - heteroscedasticity + different error law \to minimum and mean
        uncorrelated \to minimum can not be predict
**** Using quantile regression
     - Interested in extremal values \to minimum
       - 5th and 95th percentile \to good estimation for extreme values
     - Ways of computing quantile regression
       - empirical quantiles \to linear regression on a quantile
       - Least absolute values
       - Iterated weighted least squares 
         - But optimist R-squared
         - Don't know how to interpret the standard error
*** Model choice and refinement [0/2]
    - Hypothesis based on the kernel
      The expert knows his kernel and have hypothesis of how the
      optimization will influence the performances.
      - Explanation of the impact of the parameters \to justification of
        the model \to hypothesis
        - elements_number
        - y_component_number
        - etc...
    - Hypothesis testing:
      - Try \ne hypothesis
        - First start to eliminate factor that have no impact
        - Remove then from the model
        - Try to find interactions
      - Keep the more accurate and the simplest

    - Test parameters independently and remove useless ones. 
    - Iterative refinement \to try to find the interactions.
    - Determines the quality of the prediction
      - We cannot use R-squared \to biaised because of the iterative
        approach.
      - Visual checking \to yek! How can I do visualization on more than
        3D? I can not make regression for each factor because it's not
        the same than one regression including all the factors. But we
        could optimize each parameters independently.
      
*** Importance of the search space expression [0/1]
    - Easier modelization
    - Better capture of the search space features
*** Using less point as possible [0/4]
    - Design of experiment
      - Random
      - Screenning design
        Not suitable for constrained search space \to lot of point cannot
        be reached because test those at the border. Constraints have
        to be expressed in the objective function
      - LHS
        Good starting point \to no hypothesis point are choosen
        uniformly but more wisely than a random sampling.
        Generally for continuous factors \to convert to discrete \to is it
        still wiser than random? 
      - D-optimal
        Can be used to find the model but use it careful \to no
        hypothesis at the begining otherwise it introduces some biais.
        it selects points that
        explain the model \to there many possible models, it depends
        which points are choosen.
        Usefull to make refinement \to when the model is already known.
    - Copying with constraints
** Experiments [0/10]
   - Bench min of 4 runs \to warm up effect
*** Laplacian
**** Search space characteristics
     - Qualitative observation in term of speed up
**** Comparison with random and genetic algo
** Future work [0/2]
   - Find more suited design of experiments technics
   - Validate approach on more complex kernel and different
     architectures
   - Automatization
** Conclusion [0/2]
   And finally I saved the world...

* Introduction
** Why is performance optimization difficult?
  From genome sequencing to [...] including climate modeling, [...]
  all this problematic have something in common, the need of huge
  power of computation. And High Performance Computing (HPC) is
  the most effective solution. It has brought the science to another
  level and now it is a tool that scientists cannot live without like
  for example to simulate [...] or to analyze peta-octets of data. The
  expectations of scientists in term of performances are higher and
  higher as they need to run more and more heavy computations. To take
  advantage of the power of an HPC it is mandatory to correctly tune
  an application. This is a every complicated task because today's HPCs
  are extremely complex machines. Moreover It is not possible to wait for
  the next generation of hardware to bring automatically a speedup as
  it was the case at the beginning because the frequency doesn't
  increase anymore and in contrary tends to decrease. Specifically
  because we went from multi-cores to many-cores architectures and
  for 2020 exascale platforms, supercomputers with millions of cores,
  are expected in order to reach the exaflops. Thus, scientists have
  to take into account this massive parallelism when writing
  programs. Furthermore, he also has to take care about the
  dependencies of the instructions to fully occupy the pipeline. If
  there is any vector support he should adapt his code to work on
  vector instead single variable. In addition the architecture provide
  different cache hierarchy and it is crucial to use them efficiently to
  exploit data locality.

  # Pas satisfait de cet partie...
  # Hardware can also be of type Single Instruction Multiple DATA
  # (SIMD) and provide the support for vector operations and additional
  # operations can be performs for free. 
  # If processors have quickly gotten
  # faster the memory did not followed the same evolution and 
  # has all the difficults to keep the pace. 
  # Thus it is one of the most
  # Another difficult the developer has to face is that memory is one of
  # the most important bottleneck on current systems and and to
  # circumvent this issue the developer has its disposal different cache
  # hierarchies to reduce the memory access. To do so it has to exploit
  # explicitly the data locality.
  # The order of the instruction have all an impact in the execution
  # pipeline. 

  Finally to add a little more complexity we also use GPUs require
  which are totally different from the CPUs. As a result performance 
  optimization is difficult to achieve adding to that there are many
  HPCs platforms with different hardware hence one end up with 
  optimizations working well on one supercomputer and bad on another
  one. The code must be specific to the platform target and porting
  applications is very time consumming and can also be very tricky.
** Code generation and opportunities  
  In the last decade many attempt have been made to automate the
  generation of optimized code. The first approach is to rely on the
  compiler to perform the optimizations. Compilers are capable of
  detecting instructions that can be vectorized or parallelized. They
  are also capable of many loop optimizations such  loop unrolling,
  nest transformation, software pipelining, etc... Yet it exists many
  other opportunities to perform optimizations but it is to difficult
  to exploit them automatically. Moreover, it is sometimes necessary
  to rewrite the code in a slightly different way to enable the
  compiler to be effective. That is why frameworks such Orio\cite{}
  for source-to-source transformation have been developed. This
  approach generally use annotations to describe the optimizations. It
  allows to bring user's knowledge in the process of generation of an
  optimized code. The drawbacks are that the it is restricted to one
  language because the input and output languages are the same and it
  is difficult to exploit different accelerators. Also it does not
  allow operations that change the memory mapping such transposing a
  matrix. The meta-programming approach goes further by giving more
  flexibility to the programmer as it provides a higher level of
  abstraction. It consists in using high level languages to
  descriptions the computation and the optimizations. This allow the
  programmer to propose optimizations that the compiler would not be
  allowed to do.[...] But it requires to rewrite the application.  
** Optimizing: the auto-tuning approach
  The problem is, usually there are many optimization options, there
  are the compiler flags, code generation parameters (e.g. the size of
  the a tile, block or vector). Each combinations of parameters is a
  generated implementation of a program and the auto-tuning consider
  all this as a huge optimization problem and try to find the best
  combination of parameters. The search space can be huge, and the
  exhaustive search is prohibitive. Hence many techniques have been
  used such genetic algorithm, simulated annealing, tabu search,
  machine learning. But these kind of methods have some
  limitations. First the number of combination tested is not optimal,
  thus the time to perform the optimization can still be very long. In
  addition to this, it is difficult to know whether further
  optimizations could be expected or not and how to get them. Because
  it is complicated to estimate the quality of an
  optimization. Comparing to the peak performance is generally 
  meaningless and it is hard to know how the combination really
  performed because the best optimization is unknown. As a result the
  user is exclude from the tuning process by the lack of 
  feed back and any valuable information.
** Our Goal
   The idea is to give some power back to the user by investigating
   the design of semi-automatic optimization framework, where the
   application developer has control and understands how the 
   optimizations works and guides it. For this, we relied on
   BOAST\cite{}, a metaprogramming from framework in ruby that can
   generate portable code in C, Fortran and OpenCL. It provides a
   domain specific language to describe the kernel and the
   optimizations and embeds a complete chain of tools to compile, run,
   benchmark and check the validity of a kernel. We investigate
   various statistical techniques inspired from the design of
   experiments that emphasizes on reducing experiment cost.
** My contribution
   My contribution during this internship was to try an approach 
   that take into account the hypothesis the developer is doing to
   make a model of the impact of the parameters in order to guide the
   user in the tuning process. More precisely we investigated if
   linear regression and design experiments could bring accurate
   information using the least point as possible.  

   Our approach consists in the following steps:
   1. Explore the search space at very specific place
   2. Find the more accurate and simplest model by refinement and
      removing useless factors
   3. Fixing parameters to prune the search space and add removed factors
   4. Back to 1 until we are able to fix all the factors values.
   
   In the first time, we wanted to see if the linear regression was
   suited to modeling the problem of code optimization. For modeling
   computer phenomena, linear models are generally enough to get
   accurate prediction because the models are not too 
   complex. We tested this approach on a simple kernel that compute 
   the Laplacian of an image. We found that the linear regression is
   able to be accurate enough while having simple models that traduce
   how the different optimization parameters can acts. However we also
   figured out regression of expectation is not suited with current
   architectures as it was the case two decades ago\cite{}. Regression
   of expectation suppose that our data are homoscedastics and follows
   the same error law. There are no guaranty about it, thus there are
   cases where the minimum does not follow the same evolution as the
   mean. As we are interested at the minimum value the regression of
   expectation cannot  be used to model the evolution of the minimum
   when the data are heteroscedastics and do not follow the same error
   law. Hence to circumvent this burden, quantile regression seemed
   more suited, and we tried to use it in our initial approach. 
   Quantile regression created some additional difficulties compared
   to the standard linear regression. We use the iterated weighted
   least squared to compute it. Even if the coefficient computed are
   accurate, the main concern with this methods is that it is
   difficult to make inferences because we have biased 
   R-squared and standard error. This was mainly problematic for the
   validation and refinement of the model.

   In the second time, we try find a way to build simple and accurate
   model. To do so, sampling the search space correctly is crucial,
   that is why, we investigated to find what are the most suited
   design of experiments and how to use them efficiently. We tested
   different sampling strategies: 
   - Random
   - LHS
   - Screening design
   - D-Optimal design
   One important point to find correct model is that the model and the
   sampling should start with the least underlying hypothesis as
   possible because over-specification could induce some biais. The
   idea is to start with generic model and strategy such a LHS design
   and when we have some certainty about the model, try more specific
   sample by adding point with a D-Optimal design.

** Structure  of the report
   The second part of this report expose the problem of the
   optimization auto-tuning problem. The third part is dedicated to
   the study of the state of the art methods in auto-tuning and design
   of experiments. The fourth part contain details about the methods
   and materials. The fiveth part presents my contribution during the
   internship. The sixth shows some results. And the last part
   presents the future work.
* Problem analysis
* Context
** HPC architectures
** Compilation
** BOAST
* State of the art
** Auto-tuning
   In auto-tuning one can find two major categories of
   approaches. Some has focus on the is of machine learning techniques
   Machine learning is used to build models over a large training set to make
   predictions. Thus, there is a will of generalization, the knowledge  
   is reuse from previous experiences. It is used to identify category
   of programs that have the same characteristics, and to determine
   what is the best action to apply for this category of programs.

   This approach has been proven successful by the project Milepost
   GCC from Grigori Fursin\cite{fursin:hal-00685276}, which is now part of GCC. He used
   machine learning to learn characteristics of programs and the
   distributions of combinations that gives the most speedup. The idea
   is that good performing combinations have high probability to bring
   good speedup for similar programs. This allowed to reuse knowledge
   across programs.
   
   Stefan Wild et al. focused porting of optimization between similar
   platforms\cite{RoyBalHovWil2015}. They study the correlation between platform and
   the performance of combinations parameters. They used machine
   learning to build performance model of platform and this model
   to approximate performance of another platform. The more the
   combinations performance are correlated between two platforms the
   more the accurate the predictions. They managed to find correlations
   between intel CPU, IBM Power but this approach fails with too
   dissimilar platforms (ARM in their case).

   As efficiency of a search strategy is dependent on the structure of
   the search, machine learning can be used to learn what search
   methods to use according to the characteristics of the search
   space. That is the approach taken by the auto-tuning framework
   Opentuner\cite{Ansel:2014:OEF:2628071.2628092}.  

   The main drawback with machine learning techniques is that they
   need to be trained on a large amount of instances to be effective
   enough. To mitigate this problem, some, such the framework
   Nitro\cite{Muralidharan:2014:NFA:2650283.2650550} uses active
   learning to distribute the training overhead.

   Another approach is to distribute the training overhead over the
   different users, it is called
   crowdtuning\cite{memon:hal-00944513}. Informations are collected in
   a shared database and machine learning is applied to continuously
   update the model. 
   
   Other have worked more around the optimization side to find more
   suited search techniques that are able to find the near-optimal
   solution by exploring the least points of the search space
   possible. Many techniques are applied to the auto-tuning
   problems. Some of them use the derivatives such gradient
   descent which is a kind of local search techniques. It exploits the
   locality of the search space and has particularity to converge
   quickly to a the optimal solution but it requires that the search
   space has a specific geometry and convexity of the objective
   function. But these hypothesis are not necessarily true\ref{fig:obj-func-ex}. The
   objective function may not be convex, hence with many local optimum
   and a local search search would be stuck in a local optimum. The
   problem is that local optimum can be far from the global
   optimum. That is why, to escape from this, global search are more
   suited such the simulated annealing, or genetic algorithm (kind of
   multiple local search).    

   #+begin_src R :results output graphics :file img/function_examples.png :exports results :width 600 :height 400 :session
     library(polynom)
     default <- par()
     par(mfrow = c(2, 2), oma = c(0, 0, 0, 0))
     plot(poly.calc(1:2), xlim=range(-10:10))
     plot(poly.calc(-1:5))
     plot(abs, xlim=range(-5,5))
     par(default)
   #+end_src

   #+CAPTION: Objective function characterics
   #+LABEL: fig:obj-func-ex
   #+RESULTS:
   [[file:img/function_examples.png]]

   Another concern is that, the objective function is an empirical
   function, hence it can be necessary to build a surrogate. This is
   usefull to remove the noise and as a result it facilitates the
   search. Also the derivative estimation may not be always possible and
   derivated-based searches cannot work, and the alternative is to use
   derivative-free based searches such as Nelder Mead Simplex.
   The previous search methods are used in
   Orio\cite{Hartono:2009:AEP:1586640.1587666}, a source to source 
   auto-tuner. It uses random search and simulated annealing as global
   search methods and Nelder Mead Simplex as local search. 

   The efficiency of the previous approach is highly dependent on how
   much the hypothesis about he search space are wrong and sometimes
   it is difficult know how it looks. For this reason some have worked
   on generic heuristics that combine all or part of the previous aspects
   such as pattern search\cite{Hooke:1961:DSS:321062.321069} which is
   a derivative-free based search that combines global search that
   explore the space in a finite set of direction to find  
   regions of interest and local search to examine regions of
   interest. This kind of methods allow to make less hypothesis and
   require less knowledge about the search space. This approach has
   been used in OPAL\cite{orban2011templating}, a meta-programming
   framework. It uses the mesh-adaptive
   direct-search\cite{Audet04meshadaptive}, it is an extention of the
   pattern search. It can explore in an infinte set of directions
   unlike pattern search and use derivative information when available
   to speedup the search.

   While some people developed framework to characterize the search
   space such as ASK\cite{deoliveiracastro:hal-00952307} in order to
   have a better understanding of it. This tool emphasis on the
   sampling because it is crucial for build an accurate model. It
   provides a complex sampling pipeline with different surrogate
   methods (Bayesian regression, interpolation, etc...)

   In some cases, the problem structure is well know and one has an
   idea of where is the optimal solution but the structure of the
   space in this neighborhood is too complex. The approach
   taken in Atlas \cite{Whaley:1998:ATL:509058.509096} is to focus
   only in one part of the search space to perform an exhaustive
   search. But this require know the problem well and where to search.

   In general auto-tuners exclude the user from the optimization process

** Design of experiments
* Methods
  In order for this work to be usefull for someone else a laboratory
  book is available publicly on
  github\footnote{https://github.com/swhatelse/M2\_internship}. It
  contains detailed about installation and configuration steps. It
  keeps tracks of every experiments including their description and
  analysis. It is structure in a chronological way and thus follows
  the natural evolution of the work. This gives the possibility to
  easily understand what have been done at each step and why.
  Every pieces of codes is explained using literate programming which
  is very straight forward using the org-mode of emacs.
  The github repository also contains the complet set of scripts and
  data used for experiments giving the possibility to anyone to re-run
  the same experiments using the same data.

  The experiments are run on one machine with GPU Nvidia K40 using the
  driver 340.32 and two CPUs Intel E5-2630.
** Case study
   # Maybe cite Brice paper for this part
   
   In order to elaborate our approach, we took a very simple example
   which is a kernel that computes the Laplacian of an image. There
   are multiple optimization that can be done to enhance the
   performance of this kernel. 

   The first optimization we can use is the vectorization, this allows
   to take advantage of hardware capable of executing one instruction
   on multiple data at a time and instead of computing one data, so
   multiple data are computed for the same cost. Thus we can specify
   the length of the vector and we must find what is the correct
   length of the vector. 

   To perform vectorization we need to load more data and some data
   overlap with each other, to reduce the number of load we can
   synthetize those data from other, this is the second optimization
   we can have. 

   Another optimization to henance the performs of the kernel can be
   to use smaller type for intermediary results, reducing the pressure
   on the registers.

   We also can determine the number of threads use to performs the
   computation. More threads can lead to better parallelism but also
   more threads overhead. We do this by specifying the number of
   component a thread will work on. We need know what is the correct
   size of the job for a thread.
   
   After specifying the quantity of work per thread we can specify how
   this work is organized by specifying the tilling. It gives how the
   components are distributed in the y axis.

   There are also two parameters that are important for any
   kernel. First we have the number of threads in work group and then
   the organization of the threads in the work group. These parameters
   defines the work distribution at coarse grain and have an impact on
   the threads scheduling, data sharing. This leads to better usage of
   the resources and it worth to tune it carefully.

   All theses optimizations give us search space of 23100 combinations
   to minimize the time to compute one pixel.
* Contribution
** Envisioned general approach
   #+BEGIN_LaTeX
   \begin{figure}[tbh]
   \centering
   \includegraphics[width=.8\linewidth]{./img/process.pdf}
   \caption{\label{fig:1}Workflow}
   \end{figure}
   #+END_LaTeX
** Controlling measurement
   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data_detailed_v2.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file img/warm_up.png :exports results :width 800 :height 600 :session
     library(plyr)
     library(ggplot2)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     d2 <- df[df$lws_y == 2 & df$elements_number == 1 & df$threads_number == 32,]
     
     df2 = ddply(d2,.(run_index,vector_length,image_size_index), summarize, 
                      mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))
     
     
     ggplot(d2) +
         # geom_jitter(aes(x=factor(run_index), y=time_per_pixel, color=factor(load_overlap), shape=factor(temporary_size))) + 
         geom_point(aes(x=factor(run_index), y=time_per_pixel)) + 
         geom_errorbar(data=df2,aes(x=factor(run_index),y=mean, ymin=mean-err, ymax=mean+err)) +
         facet_grid(vector_length ~ image_size_index, scales="free_y", labeller=label_both) 
   #+end_src

   #+RESULTS:
   [[file:img/warm_up.png]]

** Linear regression of expectation: why it cannot work and how it can be circumvented
   #+begin_src sh :results output :exports none
       ruby ../../../scripts/format_data.rb ../../../data/2016_03_11/pilipili2/19_13_54/Data19_13_54_linear.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file img/lm.png :exports results :width 800 :height 400 :session 
     library(ggplot2)
     library(plyr)
     library(gridExtra)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     err_x_comp = ddply(df,c("x_component_number"), summarize,
                        mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))


     err_v_len = ddply(df,c("vector_length"), summarize,
                       mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))

     p1 <- qplot(df$vector_length, df$time_per_pixel) + 
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_errorbar(data=err_v_len,aes(x=vector_length,y=mean, ymin=mean-err, ymax=mean+err),colour="red") +
         ggtitle("Impact of the vector length") +
         labs(y="time per pixel in seconds", x="vector length") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))

     p2 <- qplot(df$x_component_number, df$time_per_pixel) + 
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_errorbar(data=err_x_comp,aes(x=x_component_number,y=mean, ymin=mean-err, ymax=mean+err),colour="red") +
         ggtitle("Impact of number of component on the x-axis") +
         labs(y="time per pixel in seconds", x="x component number") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))

     grid.arrange(p1, p2, ncol=2, top="") 

   #+end_src
   
   #+CAPTION: Linear regression and non-uniform noise
   #+LABEL: fig:lm-1
   #+RESULTS:
   [[file:img/lm.png]]
   
   Linear regression has already been used successfully for
   auto-tuning problems\cite{Brewer:1995:HOV:209937.209946}. But they
   have been put aside for no real reasons to our knowledge. Using
   this method to study the impact of the parameters with using linear
   models to approximate the behavior of the search space coupled with
   efficient sampling strategies seemed very interesting to us.
   
   If linear regression have been efficient in brewer's
   work\cite{Brewer:1995:HOV:209937.209946} it is maybe because at
   this time the architecture of computers was less complicated than
   today. The figure\ref{fig:lm-1} shows clearly the limit of the
   linear regression on the simple case such as a Laplacian kernel on
   nowadays architectures. First, one of the assumptions of the linear
   regression is homoscedasticity of the data which is not often
   necessarily the case, and in our example we can see that the
   variability is not the same at each factor level.

   Heteroscedasticity is problematic because the least square is not
   the Best Linear Unbiased Estimator in this case and it biases the
   variance  and thus the coefficient of determination which makes it   
   more difficult to evaluate the accuracy of the model.

   If the error law is the same everywhere as in the left in
   figure\ref{fig:lm-1} we can still have the minimum values that
   follow the same evolution as the mean and we can still predict the
   minimum. The resulting model and approximation can still be correct
   and we can easily know what is the best size for the length of the
   vector. But we would need to make assumptions that about the 
   error and we do not know anything about the error. In the right in
   figure\ref{fig:lm-1}, the evolution of the mean and the evolution
   of the minimum is not correlated and the best value is mispredicted.  

   We conclude that in the case of heteroscedasticity and non-uniform
   error law, linear regression tracks the general tendency of impact
   of the parameters. But in our case in which we are interested about
   the minimum which is uncorrelated to the mean, the linear
   regression cannot lead to the global optimum and we need another
   estimator for the minimum. 

** The choise of quantile regression
   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data.rb ../../../data/2016_03_11/pilipili2/19_13_54/Data19_13_54_linear.yaml
   #+end_src

   #+begin_src R :results output graphics :file img/why_we_choose_quantile_reg.png :exports results :width 600 :height 400 :session
     library(ggplot2)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     ggplot(df) + 
         aes(x=x_component_number, y=time_per_pixel) +
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_smooth(method="lm", formula = y ~ x + I(1/x), aes(colour="linear regression")) +           
         stat_quantile(quantiles=0.05, formula = y ~ x + I(1/x), aes(colour="quantile regression")) +
         ggtitle("Impact of number of component on the x-axis") +
         labs(y="time per pixel in seconds", x="x component number") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))

   #+end_src

   #+CAPTION: Linear regression vs quantile regression
   #+LABEL: fig:qr-example
   #+RESULTS:
   [[file:img/why_we_choose_quantile_reg.png]]

   In our case 
* Experiments
* Future work
* Conclusion
#+LaTeX: \nocite{*}
#+LaTeX: \def\raggedright{}
\bibliographystyle{IEEEtran}
\bibliography{../../biblio.bib}


* Emacs Setup 							   :noexport:
  This document has local variables in its postembule, which should
  allow Org-mode to work seamlessly without any setup. If you're
  uncomfortable using such variables, you can safely ignore them at
  startup. Exporting may require that you copy them in your .emacs.

# Local Variables:
# eval:    (require 'org-install)
# eval:    (org-babel-do-load-languages 'org-babel-load-languages '( (sh . t) (R . t) (perl . t) (ditaa . t) ))
# eval:    (setq org-confirm-babel-evaluate nil)
# eval:    (unless (boundp 'org-latex-classes) (setq org-latex-classes nil))
# eval:    (add-to-list 'org-latex-classes '("memoir" "\\documentclass[smallextended]{memoir} \n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n  \\usepackage{graphicx}\n  \\usepackage{hyperref}"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("acm-proc-article-sp" "\\documentclass{acm_proc_article-sp}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (setq org-alphabetical-lists t)
# eval:    (setq org-src-fontify-natively t)
# eval:   (setq org-export-babel-evaluate nil)
# eval:   (setq ispell-local-dictionary "english")
# eval:   (eval (flyspell-mode t))
# eval:    (setq org-latex-listings 'minted)
# eval:    (setq org-latex-minted-options '(("bgcolor" "white") ("style" "tango") ("numbers" "left") ("numbersep" "5pt")))
# End:

#+TITLE: 
#+LANGUAGE: en
#+Author: 
#+TAGS: noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LaTeX_CLASS: memoir
#+LaTeX_CLASS_OPTIONS: [12pt, a4paper]
#+OPTIONS: H:5 title:nil author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil
#+BABEL: :session *R* :cache yes :results output graphics :exports both :tangle yes 

#+LATEX_HEADER:\usepackage[french,english]{babel}
#+LATEX_HEADER:\usepackage [vscale=0.76,includehead]{geometry}                % See geometry.pdf to learn the layout options. There are lots.
# #+LATEX_HEADER:\geometry{a4paper}                   % ... or a4paper or a5paper or ... 
# #+LATEX_HEADER:\geometry{landscape}                % Activate for for rotated page geometry
# #+LATEX_HEADER:\OnehalfSpacing
# #+LATEX_HEADER: \setSingleSpace{1.05}
# #+LATEX_HEADER:\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
#+LATEX_HEADER:\usepackage{amsmath}
#+LATEX_HEADER:\usepackage{fullpage}
#+LATEX_HEADER:\usepackage{mathptmx} % font = times
#+LATEX_HEADER:\usepackage{helvet} % font sf = helvetica
#+LATEX_HEADER:\usepackage[latin1]{inputenc}
#+LATEX_HEADER:\usepackage{relsize}
#+LATEX_HEADER:\usepackage{listings}

#+BEGIN_LaTeX
%Style des têtes de section, headings, chapitre
\headstyles{komalike}
\nouppercaseheads
\chapterstyle{dash}
\makeevenhead{headings}{\sffamily\thepage}{}{\sffamily\leftmark} 
\makeoddhead{headings}{\sffamily\rightmark}{}{\sffamily\thepage}
\makeoddfoot{plain}{}{}{} % Pages chapitre. 
\makeheadrule{headings}{\textwidth}{\normalrulethickness}
%\renewcommand{\leftmark}{\thechapter ---}
\renewcommand{\chaptername}{\relax}
\renewcommand{\chaptitlefont}{ \sffamily\bfseries \LARGE}
\renewcommand{\chapnumfont}{ \sffamily\bfseries \LARGE}
\setsecnumdepth{subsection}


% Title page formatting -- do not change!
\pretitle{\HUGE\sffamily \bfseries\begin{center}} 
\posttitle{\end{center}}
\preauthor{\LARGE  \sffamily \bfseries\begin{center}}
\postauthor{\par\end{center}}

\newcommand{\jury}[1]{% 
\gdef\juryB{#1}} 
\newcommand{\juryB}{} 
\newcommand{\session}[1]{% 
\gdef\sessionB{#1}} 
\newcommand{\sessionB}{} 
\newcommand{\option}[1]{% 
\gdef\optionB{#1}} 
\newcommand{\optionB}{} 

\renewcommand{\maketitlehookd}{% 
\vfill{}  \large\par\noindent  
\begin{center}\juryB \bigskip\sessionB\end{center}
\vspace{-1.5cm}}
\renewcommand{\maketitlehooka}{% 
\vspace{-1.5cm}\noindent\includegraphics[height=14ex]{logoINP.png}\hfill\raisebox{2ex}{\includegraphics[height=7ex]{logoUJF.jpg}}\\
\bigskip
\begin{center} \large
Master of Science in Informatics at Grenoble \\
Master Math\'ematiques Informatique - sp\'ecialit\'e Informatique \\ 
option \optionB  \end{center}\vfill}
% End of title page formatting

\option{$PDES$}
\title{ Semi-Automatic Performance Optimization of HPC Kernels }%\\\vspace{-1ex}\rule{10ex}{0.5pt} \\sub-title} 
\author{Steven QUINITO MASNADA}
\date{ June 22th } % Delete this line to display the current date
\jury{
Research project performed at $<$lab-name$>$ \\\medskip
Under the supervision of:\\
Arnaud LEGRAND, Frederic DESPREZ, Brice VIDREAU, CNRS\\\medskip
Defended before a jury composed of:\\
Prof Noel DEPALMA\\
Prof Martin HEUSSE\\
Dr Thomas ROPARS\\
Prof Olivier GRUBER\\
}
\session{$June$\hfill 2016}
#+END_LaTeX

#+BEGIN_LaTeX
\selectlanguage{english} % french si rapport en français
\frontmatter
\begin{titlingpage}
\maketitle
\end{titlingpage}

%\small
\setlength{\parskip}{-1pt plus 1pt}

\renewcommand{\abstracttextfont}{\normalfont}
\abstractintoc
\begin{abstract} 
Text 
\end{abstract}
\abstractintoc
\renewcommand\abstractname{R\'esum\'e}
\selectlanguage{english}% french si rapport en français

\cleardoublepage

\tableofcontents* % the asterisk means that the table of contents itself isn't put into the ToC
\normalsize

\mainmatter
\SingleSpace

#+END_LaTeX

# #+BEGIN_abstract
#   Blablabla
#   \newpage
# #+END_abstract

* Plan                                                             :noexport:
** Introduction [3/3]
*** Why is performance optimization difficult?
   - In HPC code optimization crucial to exploit very complex hardware.
     Cannot wait for the next generation to bring speedup because it
     does not (Frequency not higher but more cores and henanced ISA). 
     - many cores \to heavy parallelism \to need to program parallel
     - pipelining ILP \to 
     - vector support \to SIMD \to need to work with vector
     - cache hierarchies \to need to exploit data locality
     - GPUs! \to different way of programming (than CPU)
   - HPC plaforms have many \ne hardware \to code optimizations not portable.
     Porting application to another platform is time consumming and
     can be very tricky.
   - Many attempts in the last decade to automate the generation of
     optimized code
*** Code generation and opportunities
    - The compiler approach: loop unrolling, vectorization, automatic
      parallelization, loop nest transformation, etc. Yet, many
      opportunities are not exploited as it is too difficult to
      exploit them automatically. Sometimes, the source code has to be
      rewritten in a slightly different way to enable the compiler to
      be effective
    - Parametric optimization:
      - The source-to-source transformation (C to C, Fortran to Fortran,
        ...). Framework for transform code. Orio. Serge
        Guelton. Difficile mais limité à un seul langage, et
        exploitation d'accelérateurs différents difficile. Ça ne se
        permettra jamais de changer le mapping des données en mémoire
      - Meta-programming approach: allow the programmer to propose
        optimizations that the compiler would not be allowed to do
        (because of the language or because it would require information
        on the application that cannot be given to the compiler).
        # But it is also the case with source-to-source transformation
        # right?    
*** Optimizing: the auto-tuning approach
    - Many optimization options: compiler flags, source-to-source
      transformations, higher-level modifications (tile/block/vector
      size). Each combination represents an implementation.      
    - Auto-tuning: consider all this as a huge optimization problem
      and try to find the best combination. Many techniques (genetic
      algorithms, simulated annealing, tabu search, machine learning,
      ...) depending on the problem constraints. But mainly two
      problems:
      - the time needed for such optimization
      - knowing whether further optimizations could be expected or not
        (peak performance is generally useless and the optimization
        process is so complex that it's hard to know how it really
        performed) is difficult and même si tu sais qu'il devrait être
        possible de faire mieux, tu sais pas vraiment où, comment( cf
        of genetic algo on the full search space), ...
*** Our goal
    Many approaches in code generation/transformation. It's possible
    to start from high-level codes (e.g., pytran) but the most
    optimized codes are obtained from specific tools (FFT, BLAS,...).

    We decided to evaluate an intermediate approach by relying on
    BOAST, a metaprogramming... (Semi automatic approach \to gives back
    power to the user, framework ruby generating portable code in C,
    Fortran, OpenCL. DSL) and investigate various statistical
    techniques inspired from the design of expeirments field that
    emphasizes on reducing experiment cost.

    Investigate the design of a semi-automatic optimization framework,
    where the applicaiton developer has control and understands how
    the optimization works and guides it.
*** My contribution
    - Related work on auto-tuning
    - Proposal based on DoE
    - Evaluation
      - Comparison with state of the art
      - Analyze

    - (Complex methods used but no explanation on why they work)
    - Prevent biased measurement
    - Try a simple approach and try to understand it deeply
      - Getting knowledge from the problem to guide the user:
        - Take into account hypothesis \to use the knowledge of the user
          1. Sampling the space
          2. Model find 
             - Removing useless factors
             - Refine the model \to add quadratic terms, 1/x,
               interactions, etc...
          3. Fix parameters to prune the search space and add removed
             factors.
          4. Back to 1 until we are able to fix all parameters values   
          
        - Linear regression methods to model the search space \to
          finding good model based on hypothesis. Allow the user to
          check this hypothesis. And understand the problem.
          - Try OLS \to problem with regression of expectation
            heteroscedacity + non uniform noise
          - Solution \to quantile regression
            - Pb with rq \to error to compute std. err, etc...
            - Used iterated weighted least square 
              Pb to make inferences \to biased R-squared and std. error
              # Are std.err biased to?
              How to compute CI?
              
        - Modeling
          - Start generic \to go specific
            Over specification \to biased
          - Sampling is crucial \to Design of experiments \to reducing number of experiments
            What design of experiment to use?
            - Random
            - LHS
            - Screening
            - D-Optimal
            How to use them? Copying with constraint
            - Start without hypothesis on the model otherwise \to biais
            - Add point with hypothesis \to D-Opt
            
*** Structure of the report
** Context [3/3]
*** HPC/architectures     
    - Crucial for science and business
    - To get performance \to exploit hardware \to take characteristics into account
      - Many cores \to aims low idle time
        Thinking parallel
        Right number of threads \to because overhead in thread
        management.
        Less synchro as possible
      - GPUs \to suited to a certain type of computation \to can bring
        lots of performances.
      - vector support
        Data pipelining
        Share the same instruction on multiple data \to save decoding
        
      - cache levels
      - ILP \to break instruction dependencies
    - Architecture \ne from a HPC to another
      Specialized code \to not portable
*** Obtaining efficient code
**** Compilation
     Il fait ce qu'il peut mais pas de vision globale du code \to local
     optimization (intra procedure) \to because more easier no control
     flow
     - code re-ordering \to instruction scheduling find the best
       sequence for the pipeline \to reduce instruction conflict
       (dependencies between instructions) 
     - Register allocation
     - loop transformation \to parallelization and data locality \to 
       finding parallelism into loops \to loop nest transformation /
       unroll. 
     - Automatic parallelism \to multi-threaded, vectorized 
       Pb with shared/global variable, IO, indirect addressing, etc...
   
     Limited because stuck by semantic rules, not enough information
     at compile time, etc...
     
     Archi compliquée donc dur: Grigori Fursin.
     Sometime the platform the not well supported.
**** Source-to-source transformation (C vers C ou FORTRAN vers FORTRAN)
     - Relieve compiler \to deactivate optimization
       Gives to the compiler the desired optimization
     - Gives more expressiveness \to more information two performs
       transformation \to ensure that the semantic is correct 
     - Present the code correctly to allow the compiler to make his job.
     - orio, PIPS,  cloog 
       Generally annotation-based  
       How is it used
      - pluto (automatic parallelization)
      - pytran
      - auto-tuning on top of orio

     Pros and cons:
     
**** Meta-programming: BOAST
     Less constraint by semantic rules but can be error prone \to not
     correct transformation.
     BOAST: 
     - for advanced user
     - Ruby
     - Complete tool chain
       - DSL
       - Code generation
       - Compilation
       - Bench-marking
       - Kernel verification

*** Recap
    How to port performances.
** Problem analysis [0/1]
   - Huge search space \to need to explore only part of it \to
     optimization problem.
   - Interactions between parameters
   - Non-smooth and empirical objective function
   - Combination of discrete and continuous parameters
   - Constraint optimizations
     Represent unfeasible points.
        
** State of the art on Autotuning [2/4]
   - What is autotuning
     paramters \to represents different version/implementation

  # Maybe an overview of machine learning in general
  - Reuse knowledge of previous experience (generalization) \to machine
    learning. For different problem \to re-usability. 
    What is machine learning and why it is useful in auto-tuning.
    Generally exhaustive search costly training phase \to
    reducing impact. Classification \to which strategy to apply.
    - Small vs. Big
    - Milepost GCC \to learning characteristics of a program to
      predict what are the good combinations, optimization
      across programs. Predict good configuration using the
      distribution of good combination by taking the mode.
      Reuse knowledge across programs
    - Stefan Wild \to Learning combination across platform
      Worked for similar platforms. Search space pruning \to random
      search.
      Reuse knowledge across platforms
    - Opentuner \to which optimization technics for a given problem
      because the efficiency of a technics depends on the
      structure of the problem.
    - Incremental training \to Nitro using active learning
    - Collective tuning \to crowdtuning, Milepost
      Models stored in a common database and continuously updated.

  Optimization: exhaustive search is unfeasible.

  - "Direct search". The efficiency (ability to find the
    (near)-optimal solution and possibly in the fewest possible
    experiments) depends on the structure of the problem.
    - Main techniques:
      - Gradient descent: ferrari, a priori = local, geometry, convexity.
        - Issues: 
          - partly wrong hypothesis (geometry, convexity): simulated
            annealing, many local searches (genetic algorithms in some
            sense)
          - experimental estimation (empirical function)  :
            surrogates, etc. *local* approximation
            Usefull to remove the noise and facilitate the search
          - derivative estimation: Nelder Mead Simplex
        - \to many heuristics that combine all or part of the different
          previous approaches depending on how much the various
          hypothesis are wrong or not. Their efficiency highly depends
          on these hypothesis.
    - Some people have thus developed framework to characterize the
      optimization space.
      - ASK \to Emphasis on the sampling because important for the
        accuracy of the model \to complex sampling pipeline with
        different surrogate methods( bayesian regression,
        interpolation, etc... ). _Global modeling requires complex
        models and numerous experiments_.
    Illustration with a few tools:
    - Orio \to source to source annotation based autotuner 
      - random search, Nelder Mead Simplex and simulated annealing.
      - greeding algorithm for local search at the end of gobal.
    - OPAL \to Use direct search combinations of heuristics \to
      Mesh-adaptive direct-search \to pattern search.
      Global *and* local search \to work by iterative phase:
      - Sampling the space \to finding region of interest
      - Refining the solution
    - In some cases, the problem structure is known and one has an
      idea of where the optimal solution is but the structure of the
      space in this neighborhood is too complex. Some fall back to
      Exhaustive search \to Atlas Linear search, know where to search \to
      need to know the problem well.

  Primary Goals:
  - semi-automatic, almost interactive ? more global approach where
    the relevance of the hypothesis can be evaluated
  - optimize at low cost, need to prune the search space
  - from previous experience, generalization from an arch to another
    seems very difficult

  Somehow similar approach:
  - Getting knowledge on the fly \to regression, interpolation
    - Brewer \to linear regression for the modelization to predict
      objective function and root finding  or kind of greedy
      descent for the optimization.
      Find correct model automatically on platform CM-5, simulated
      version of Intel Paragon and network of station based on FORE ATM. 
      Not recent paper \to architecture have evolved. Is linear
      regression still ok?
** State of the art design of experiments [1/2]
   - Study phenomenon \to behavior of a system
     - Acting on many factor at a time instead of one
     - Get information on how the factors impact the system and
       interactions \to not possible with OFAT (one factor at a time) \to
       factorial design
     - Identify interaction without trying all range of values.
     - Define explanatory variable.
   - DoE:
     - OFAT
     -Factorial
       - Random
       - LHS
         For continuous space
         Provide Better coverage of the space
       - Fractional design
         Screening design \to Take the extreme values
       - Optimal design
         - D-Optimal
           Require to know the model
           Select points according to a model.
         - I-Optimal
         - A-Optimal
** Methodology [2/2]
*** Reproducible work
    - Lab book on github  
    - Literate programming 
    - org mode
*** Case study
****  Laplacian
      - OpenCL
      - Optimizations explanation
        - Vectorization \to vector length
        - Synthetize loading \to load overlap \to for memory bound?
        - Tilling \to y component number
        - Number of threads \to elements number
        - Size of temporary results \to temporary size
          Reducing pressure on registers? If high usage of registers?
          If not high usage of registers overhead of casting?
        - Size of a work group \to threads number
        - Shape of work group \to lws y
      - 23100 combinations
      - Minimization
      - Test 5 sizes of images \to mean
**** Experimental protocol  
    - Result validation against bruteforce
    - Comparison with random, gradiant search, and genetic algorithm
    - Bench min of 4 runs \to warm up effect
**** Search space characteristics
     - Qualitative observation in term of speed up
**** Comparison with random and genetic algo

** Envisioned general approach[1/1]
   # Maybe need more explanation 
   # What is the linear regression, how we use it, why, etc...
   Semi automatic, interactive \to gives more control, feed back to the
   user, guide him.
   Gives information about the search space characteristics \to shape \to
   define the search methods, where could be the best parts \to pruning
   BOAST \to for advanced users who are ready to rewrite their code in
   ruby and know what they are doing.
   Regression + sampling to get knowledge
   Show the structure of the problem parameters that have the most
   impact (global) one those have less impact (local)

   Interrogating correctly the search space \to sampling
   Build model of the objective function \to easier for optimize and extract information
   Use the knowledge of the expert:
   - Can test his hypothesis
   - Understand the search space and his problem
   - Understand what happens
    1. DoE
       - Sampling the space wisely
       - Use linear regression OLS:
         - remove factors from the model
         - model and optimize
    2. Loop back to 1 to refine the model
** Controlling measurement [1/1]
    - Time per pixel \to total time / number of pixel. Because we test
      different size of image.
    - min(x_1,...,x_10) ? how to protect against potential warm-up
      - Energy saving mode of current hardware(CPU and GPUs)
      - Mostly present just after the compilation of the kernel.
      - 4 runs \to take the minimum
    - randomizing to protect against bias, even for full search
      space. But run and image size not randomized.

    - Process
      - Code generation
      - Compilation
      - Bench-marking

** Results [3/3]
    Considering slowdown with regard to the best.
    Comparison:
    - GA \to not tuned \to would have take time to tune it
      Very efficient in general
    - Greedy
      Fails
    - Random
      Very simple and efficient also
    - LM
      - Uniform \to The one which get the most high performing version
        but sometime fails and gives very bad results.

    - Rq \to Another way of doing linear regression
      - Uniform \to Improved a little bit in general LM but less very
        high performing version 
   | Histogram of solutions | Cost |

** Analysis [0/14]
*** Characteristics of the search space [1/2]
   - Repartition of good combinations
   - Lots of local optimum \to local search failed
   - Heteroscedasticity \to noise due to interactions
*** Linear regression of expectation: why it cannot work and how it can be circumvented [1/3]
   OLS gives often good results but sometime can fail.
   Prediction of two different things:
   - Mean / Expected value
   - Quantile
**** Least Squared regression and non uniform noise  
    - Assumptions:
      - homoscedasticity (Gaussian noise) but pb we have heteroscedasticity
        - Why is it a problem?
          - Unbiased coefficient estimate but biased std error and thus
            R-squared \to more difficult know if a model is correct
          - But it is still ok if the error law is the same everywhere
      - But we don't know anything about the noise and normal
        distribution of the noise is assumed. We cannot do anything
        about that because in our case the noise come from complex
        interactions between parameters.
        Possible to reduce it by fixing values but it is not always
        possible to do that e.g. if for all the parameters the noise
        falls the same law. But we still have some difficult to find
        model due to the other parameters.        
    - Tracks general tendency of the impact of factors
    - 2 cases:
      - heteroscedasticity + same error law \to minimum can be predict
      - heteroscedasticity + different error law \to minimum and mean
        uncorrelated \to minimum can not be predict
**** Using quantile regression
     - Interested in extremal values \to minimum
       - 5th and 95th percentile \to good estimation for extreme values
     - Ways of computing quantile regression
       - empirical quantiles \to linear regression on a quantile
       - Least absolute values
       - Iterated weighted least squares 
         - But optimist R-squared
         - Don't know how to interpret the standard error
*** Model choice and refinement [0/2]
    - Hypothesis based on the kernel
      The expert knows his kernel and have hypothesis of how the
      optimization will influence the performances.
      - Explanation of the impact of the parameters \to justification of
        the model \to hypothesis
        - elements_number
        - y_component_number
        - etc...
    - Hypothesis testing:
      - Try \ne hypothesis
        - First start to eliminate factor that have no impact
        - Remove then from the model
        - Try to find interactions
      - Keep the more accurate and the simplest

    - Process dependent of the set of points \to cannot apply a model
      blindly even if it the correct model without considering the points.
    - Test parameters independently and remove useless ones. 
    - Iterative refinement \to try to find the interactions.
    - Determines the quality of the prediction
      - We cannot use R-squared \to biaised because of the iterative
        approach.
      - Visual checking \to yek! How can I do visualization on more than
        3D? I can not make regression for each factor because it's not
        the same than one regression including all the factors. But we
        could optimize each parameters independently.
      
*** Model Optimization [0/2]
   - Non-convex optimization 
     Constraint \to unfeasible points
     Barrier approach
   - Exhaustive search
*** Importance of the search space expression [0/1]              :deprecated:
    # Will see if I have more time to dig the subject
    - Easier modelization
    - Better capture of the search space features
*** Using as little points as possible [0/4]
    - Design of experiment
      - Random
      - Screenning design
        Not suitable for constrained search space \to lot of point cannot
        be reached because test those at the border. Constraints have
        to be expressed in the objective function
      - LHS
        Good starting point \to no hypothesis point are choosen
        uniformly but more wisely than a random sampling.
        Generally for continuous factors \to convert to discrete \to is it
        still wiser than random? 
      - D-optimal
        Can be used to find the model but use it careful \to no
        hypothesis at the begining otherwise it introduces some biais.
        it selects points that
        explain the model \to there many possible models, it depends
        which points are choosen.
        Usefull to make refinement \to when the model is already known.
    - Copying with constraints
** Future work [0/2]
   - Constraints
   - Find more suited design of experiments techniques
   - Validate approach on more complex kernel and different
     architectures
   - Automatization
** Conclusion [0/2]
   And finally I saved the world...

* TODO Introduction
** Why is performance optimization difficult?
  From genome sequencing to molecular dynamics, including climate or
  earthquake modeling, or aerodynamics, there is an ever increasing
  need for computer power. For this purpose, High Performance Computing (HPC) is
  the most effective solution. It has brought the science to another
  level and now it is a tool that has became essential for scientists like
  for example to simulate nuclear explosion or to analyze peta-octets of
  data. The expectations of scientists in term of performances are
  higher and higher as they need to run more and more heavy and complex
  computations. To take advantage of the power of a supercomputer it is
  essential to correctly optimize the applications. This is a very
  complicated task because today's HPCs are extremely complex
  machines. It is not possible to wait for the next generation of
  hardware to bring automatically a speedup as 
  it was the case up to the years 2000 because frequency cannot
  increase anymore and in contrary even tends to decrease. For this
  reason, we went from multi-cores to many-cores architectures and 
  for 2020 exascale platforms, supercomputers with millions of cores,
  are expected in order to reach the exaflops. Thus, developers have
  to take into account this massive parallelism when writing
  programs. Furthermore, they also have to take care about things such
  the dependencies of the instructions to fully occupy the pipeline. If
  there is any vector support the developer should adapt his code to work on
  vectors instead of single variables. In addition the architecture provides
  different cache hierarchy and it is crucial to exploit data locality
  to use them efficiently. 
  Finally to address the computer power need, GPUs have become very
  popular. Unfortunately, this add a little more complexity as they
  require a way of programming which is different from the CPUs. As a
  result, performance optimization is difficult to achieve. To build
  efficient HPC platforms, architects have to come up with unique
  combinations of a variety of hardware, which complicates the
  application optimization. Hence one end up with optimization working
  well on one supercomputer and bad on another one as the code is
  specific to one platform and porting applications is extremely time
  consuming and can also be very tricky.  
** Code generation and opportunities  
  In the last decade many attempt have been made to automate the
  generation of optimized code. Performing optimization is one of the
  primary functions of compiler. They are capable of detecting
  instructions that can be vectorized or reorganized to favor
  super-scalar execution. They 
  are also capable of many loop optimizations such loop unrolling to
  improve pipeline efficiency,
  nest transformation to improve data locality and expose parallelism,
  etc... It exists many other optimization opportunities but it is to
  difficult to exploit them automatically because the compiler does
  not have the necessary information at compile time. Moreover, it is
  sometimes necessary to rewrite the code in a slightly different way
  to enable the compiler to be effective. That is why frameworks such
  Orio\cite{Hartono:2009:AEP:1586640.1587666} 
  for source-to-source transformation have been developed. This
  approach generally use annotations to describe the transformations
  to apply. It
  allows to bring user's knowledge in the process of generation of an
  optimized code. The main advantage of such approach is that the
  original code is left unchanged, which is generally highly
  appreciated by the application developer. The drawbacks are that the
  it is restricted to one language because the input and output
  languages are the same therefore it is difficult to exploit different
  accelerators. Also it does not allow operations that would change the
  data structure layout such as transposing a matrix beforehand. The
  meta-programming approach goes further by giving more flexibility to
  the programmer as it provides a higher level of abstraction. It
  consists in using high level languages to descriptions the
  computation and the optimizations. This allow the programmer to
  propose optimizations that the compiler would not be allowed to
  do. But it requires to rewrite the application.   
** Optimizing: the auto-tuning approach
  Whatever the chosen approach, the application developer is left with
  numerous optimization options: there are the compiler flags, code
  generation parameters (e.g., the size of the a tile, block or
  vector). Each combination of parameters leads to unique binary code
  whose performance has to be evaluated. The auto-tuning approach considers
  all this as a huge optimization problem and tries to find the best
  combination of parameters. The search space can be huge, and an
  exhaustive search is thus prohibited. Hence many techniques have been
  used in the literature such as genetic algorithm, simulated
  annealing, tabu search or machine learning. But these  methods have
  several limitations. First, the number of combination 
  tested is often large, thus the time to perform the optimization can
  still be very long. In addition with such fully automated
  approaches, it is difficult to know whether further optimizations
  could be expected or not and how to get them. Because it is
  complicated to estimate the quality of an optimization. Comparing to
  the peak performance is generally meaningless and it is hard to know
  how the combination really performed because the best optimization
  is unknown. As a result the user is excluded from the tuning process
  by the lack of valuable feedback.
** Our Goal
   The idea we explored in this internship was to give some power back
   to the user by investigating the design of a semi-automatic
   optimization framework, where the application developer has control
   and understands how the optimizations works and guides it. For
   this, we relied on BOAST\cite{}, a metaprogramming framework in
   ruby that can generate portable code in C, Fortran and OpenCL. It
   provides a domain specific language to describe the kernel and the
   optimizations and embeds a complete tool chain to compile, run,
   benchmark and check the validity of a kernel. We investigate
   various statistical techniques inspired from the design of
   experiments that emphasizes on reducing experiment cost.
** My contribution
   My contribution during this internship was to prototype and
   evaluate an approach that takes into account a performance model
   hypothesized by in order to guide the tuning process.  

   Our approach consists in the following steps:
   1. Propose a model
   2. Explore the search space at very specific places that allow to
      evaluate the quality of the model. 
   3. Find the more accurate and simplest model by refinement and
      removing useless factors
   4. Determine the optimal parameters of the refined model and
      restrict the search space accordingly.
   5. Back to 1 until we are able to fix all the factors values.
   
   In a first time, we wanted to see if simple linear regression was
   suited to the code performance modeling problem. 
   # To model 
   # computer phenomena, linear models are generally enough to get
   # accurate prediction because the models are not too 
   # complex. 
   We tested this approach on a simple kernel that computes 
   the Laplacian of an image. 
   # We found that the linear regression is
   # able to be accurate enough while having simple models that traduce
   # how the different optimization parameters can acts. However we also
   # figured out regression of expectation is not suited with current
   # architectures as it was the case two decades ago\cite{}. Regression
   # of expectation suppose that our data are homoscedastics and follows
   # the same error law. There are no guaranty about it, thus there are
   # cases where the minimum does not follow the same evolution as the
   # mean. As we are interested at the minimum value the regression of
   # expectation cannot  be used to model the evolution of the minimum
   # when the data are heteroscedastics and do not follow the same error
   # law. Hence to circumvent this burden, quantile regression seemed
   # more suited, and we tried to use it in our initial approach. 
   # Quantile regression created some additional difficulties compared
   # to the standard linear regression. We use the iterated weighted
   # least squared to compute it. Even if the coefficient computed are
   # accurate, the main concern with this methods is that it is
   # difficult to make inferences because we have biased 
   # R-squared and standard error. This was mainly problematic for the
   # validation and refinement of the model.

   In a second time, we try find a way to reduce the number of points
   needed for checking and instanciating the model. To do so, sampling the
   search space correctly is crucial. That is why, we investigated
   techniques inspired from the design of experiments.  

   # One important point to find correct model is that the model and the
   # sampling should start with the least underlying hypothesis as
   # possible because over-specification could induce some biais. The
   # idea is to start with generic model and strategy such a LHS design
   # and when we have some certainty about the model, try more specific
   # sample by adding point with a D-Optimal design.

** Structure  of the report
   The second part of this report exposes context of this work. The
   third part describes the problem of the optimization auto-tuning
   problem. The fourt part presents the state of the in auto-tuning
   and design of experiments. The fifth part exposes the how this
   work was made. The sixth part explains the approach we used. 
   The seventh part explains how measurements was made. The eigth part
   shows the results we manage to have compared to other
   techniques. The nineth part provide a detailed analysis of the
   results. And finally the tenth part show what can be done to
   improve our process and results.
* TODO Context
** HPC architectures
  HPCs are complex machines and it is not straightforward to use them
  correctly. Indeed a not carefully tuned code is likely to
  have poor performances. Optimizing the code correctly by taking into
  account the characteristics of both the application and the machine
  can bring major speedup and increasing 
  the performance by a factor 10 is not rare. The current trend in HPCs is to
  have CPUs with an ever increasing amount of cores to
  reduce the frequency in order to reduce the power consumption and
  the heat. Thus to obtain performances it is mandatory to exploit
  correctly the parallelism of the platform. The computation has to be
  described in a parallel way. Translating automatically a sequential
  application into a parallel one generally brings poor
  performances. Hence, the developer has to define which are the parts that
  can be performed in parallel and how they are parallelized. The code
  has to be written in a way such that the work is *distributed among all*
  *the cores* available and keep them busy when I/Os occur to have the
  less possible cores idle. It is important to use the correct amount of
  *threads*. Too many threads often leads to more overhead due to the
  management of the threads. Too little and all the cores are not
  exploited correctly. Also the more the threads are independent from
  each other, the better, which means there should be as little
  synchronization as possible.  

  *Pipelining* is another kind of parallelism in which the treatment of
  instructions is split into a sequence of steps (fetch, decode,
  execute, etc...) and goes through a pipeline. Multiple instructions
  can be in the pipeline at the same time but only at different state
  of the processing, like in an assembly line. A correct scheduling of
  the instructions in the pipeline leads to a better occupancy of
  it. *Instruction Level Parallelism* is a mechanism that can
  change the order of the instructions to have a better overlapping of
  instructions in the pipeline. But for example conditions are a
  disaster because it can hinder this. In addition some
  CPUs have vector support. Such processors can manipulate not only
  scalar variables but also directly *vector* variables. The vector is loaded
  into a vector register and the same instruction is applied on the
  entire vector. This saves the cost of decoding the same instruction multiple
  times.

  Today, CPUs have become so fast that one of the main bottleneck is
  the memory. Thus accessing data in memory is much more expensive than
  performing computations. To deal with this problem the solution
  found is to use different *memory hierarchy*. The statement is that
  a data that is currently used is more likely to be re-used in a near
  future. Hence the idea is to keep that are the most frequently used
  data as close as possible to the CPUs, that is why CPUs embeds cache
  memories. In a processor there can be up to 4 level of cache
  (the registers, L1, L2, L3) and the closest to the
  CPU have the lowest latency possible but are also the smallest.
  Thus the pattern to access data has to be chosen carefully so that
  the most used data stay close to the computation units.

  Another recent characteristic of HPCs is the increasing use of
  *GPUs* because for computation that can be well parallelized which
  is the case generally the case with scientific computation, they
  are faster than CPUs. However GPUs do not work exactlty the same as
  CPUs and need to be programmed in a quite different manner, the
  cache and number of cores are different.
  # the architectures of the GPUs is
  # different from the one of the CPUs, the amount of cache memory is
  # limited and there are more compute units (for example the Nvidia
  # Tesla K40 has 2,880 cores). 
  # Thus optimizations that bring good
  # performances on CPUs may bring poor performances on GPUs such the
  # size of the vectors or the number of threads.

   #+BEGIN_LaTeX
   \begin{figure}[tbh]
   \centering
   \includegraphics[width=.8\linewidth]{./img/performance_platform_correlation.jpg}
   \caption{\label{fig:correlation}Performances correlation accross platforms}
   \end{figure}
   #+END_LaTeX

  As we saw, optimizing code for HPC applications can be very
  chalenging but porting applications accross platforms is even
  harder and highly time consuming as the optimization are low level
  and HPCs can be very different and complex. Optimization that gives
  good performances on one platform may not work so well on another
  \ref{fig:correlation}. As developers cannot spend months to port the
  application on another machine it is necessary to use tools that
  facilitate the porting and the optimization of scientific
  applications.  

** Obtaining efficient code
*** Compilation
    Many works have been made around compilers to optimize the code
    automatically. They are able to modify the order of the
    instructions to find better sequences to maximize the occupancy
    of the pipeline. In addition, automatic parallelism techniques
    are able to find sequential code that can be vectorized or
    multi-threaded. They can also perform loop transformation to
    reduce the overhead of loops, henance data locality and facilitate
    the parallelization using loop unroll / nest transformation
    techniques. But this require to still write the code with care to
    ease the job of the compiler. For instance automatic parallelism
    is difficult to apply when there are global or shared variable,
    indirect address are used, etc... Further more, compilers generally
    do not have global vision of the code and lack informations of
    compile time and thus perform only local optimization. In
    addition, they also can be limited by semantic rules. As a result,
    they are not able to evaluate which transformation to choose among
    all correct transformations and they just take the one that is
    semantically correct.
*** Source-to-source transformation
    Source-to-source transformation frameworks ease the task of both
    the developer and the compiler by taking a source code, working on
    Abstract Syntax Trees and applying transformation such automatic
    parallelization to generate a modified version of the original
    code. Unlike with compilers, the developer can specify how he
    wants the transformation to be done, for instance how many time
    the loop is unrolled. Then the framework ensures that the
    transformation is valid and generates a code that the compiler can
    easily work with. This relieve the compiler from the complicated
    tasks such the loop transform or the automatic parallelization and
    this gives the possibity the user guide the transformation by
    giving more information. The disadvantage of such tools it that
    they generally target one language and one compiler and can be
    still limited by semantic rules.  
*** Meta-programming: BOAST
    Meta-programming is a slightly different approach from
    source-to-source transformation in which the developer use high
    level language to make a description of his kernel and the
    possible optimizations (e.g. the size of a vector, the tiling,
    etc...). The advantage is that it is not linked to one output
    language or compiler. It also gives more control to the user as
    there is no checker that verify the correctness of the
    transformation, thus he can exactly specify how the transformation
    is performed. Hence, the developer has to know what he is doing
    and it can be error prone. In this work we used the
    meta-programming framework BOAST\cite{}. BOAST gives the ability
    to user to meta-program his kernels in ruby with a Description
    Specific Language (DSL), then BOAST can generate it in many target
    languages (C, Fortran, Cuda, OpenCL), compilate it and benchmark
    the resulting executable.  
** Summary
  In a word, optimizing HPC applications is tricky and porting even is
  more difficult but tools exist to assist the developer in this
  complicated task. However a major problem remains, generally the
  developer know what should be vectorized or what should be
  parallelized but he does not know what is the best size of the 
  vector or the best number of threads or what is the combination of
  compilation flags that brings the best speedup. This problem consist
  in tuning correctly the different optimization parameters of the
  applications. 

** Problem analysis
  The tuning of applications is a non-trivial problem, because the
  search space of the different combinations of parameters can be
  huge. For instance there are about 500 compilation flags for GCC and
  testing all the combinations to find the best one, even if this
  process is automated (auto-tuning). Thus it is formulated as a mathematical
  optimzation problem where the optimization function gives metrics
  of combination of parameters \vec{x}. 
  
  #+BEGIN_LaTeX
  \( \displaystyle\min_{x} {f(\vec{x}): \vec{x} \in \mathcal{D} \subset \!R^{n} } \)
  #+END_LaTeX  

  This function is empirical because the performances of a
  combination cannot be computed, measurements have to be done to
  evaluate the objective function at point x. It needs to generate the
  code variant, compile it and run it. Sometimes the problem can have
  constraints because some points are unfeasible, this means they
  cause the compilation to fail or the program to crash. In addition
  parameters can be discrete and continuous. 
 
* TODO State of the art
** Auto-tuning
   In auto-tuning one can find two major categories of
   approaches. Some has focus on the is of machine learning techniques
   Machine learning is used to build models over a large training set to make
   predictions. Thus, there is a will of generalization, the knowledge  
   is reuse from previous experiences. It is used to identify category
   of programs that have the same characteristics, and to determine
   what is the best action to apply for this category of programs.

   This approach has been proven successful by the project Milepost
   GCC from Grigori Fursin\cite{fursin:hal-00685276}, which is now part of GCC. He used
   machine learning to learn characteristics of programs and the
   distributions of combinations that gives the most speedup. The idea
   is that good performing combinations have high probability to bring
   good speedup for similar programs. This allowed to reuse knowledge
   across programs.
   
   Stefan Wild et al. focused porting of optimization between similar
   platforms\cite{RoyBalHovWil2015}. They study the correlation between platform and
   the performance of combinations parameters. They used machine
   learning to build performance model of platform and this model
   to approximate performance of another platform. The more the
   combinations performance are correlated between two platforms the
   more the accurate the predictions. They managed to find correlations
   between intel CPU, IBM Power but this approach fails with too
   dissimilar platforms (ARM in their case).

   As efficiency of a search strategy is dependent on the structure of
   the search, machine learning can be used to learn what search
   methods to use according to the characteristics of the search
   space. That is the approach taken by the auto-tuning framework
   Opentuner\cite{Ansel:2014:OEF:2628071.2628092}.  

   The main drawback with machine learning techniques is that they
   need to be trained on a large amount of instances to be effective
   enough. To mitigate this problem, some, such the framework
   Nitro\cite{Muralidharan:2014:NFA:2650283.2650550} uses active
   learning to distribute the training overhead.

   Another approach is to distribute the training overhead over the
   different users, it is called
   crowdtuning\cite{memon:hal-00944513}. Informations are collected in
   a shared database and machine learning is applied to continuously
   update the model. 
   
   Other have worked more around the optimization side to find more
   suited search techniques that are able to find the near-optimal
   solution by exploring the least points of the search space
   possible. Many techniques are applied to the auto-tuning
   problems. Some of them use the derivatives such gradient
   descent which is a kind of local search techniques. It exploits the
   locality of the search space and has particularity to converge
   quickly to a the optimal solution but it requires that the search
   space has a specific geometry and convexity of the objective
   function. But these hypothesis are not necessarily true\ref{fig:obj-func-ex}. The
   objective function may not be convex, hence with many local optimum
   and a local search search would be stuck in a local optimum. The
   problem is that local optimum can be far from the global
   optimum. That is why, to escape from this, global search are more
   suited such the simulated annealing, or genetic algorithm (kind of
   multiple local search).    

   #+begin_src R :results output graphics :file img/function_examples.png :exports results :width 600 :height 400 :session
     library(polynom)
     default <- par()
     par(mfrow = c(2, 2), oma = c(0, 0, 0, 0))
     plot(poly.calc(1:2), xlim=range(-10:10))
     plot(poly.calc(-1:5))
     plot(abs, xlim=range(-5,5))
     par(default)
   #+end_src

   #+CAPTION: Objective function characterics
   #+LABEL: fig:obj-func-ex
   #+RESULTS:
   [[file:img/function_examples.png]]

   Another concern is that, the objective function is an empirical
   function, hence it can be necessary to build a surrogate. This is
   usefull to remove the noise and as a result it facilitates the
   search. Also the derivative estimation may not be always possible and
   derivated-based searches cannot work, and the alternative is to use
   derivative-free based searches such as Nelder Mead Simplex.
   The previous search methods are used in
   Orio\cite{Hartono:2009:AEP:1586640.1587666}, a source to source 
   auto-tuner. It uses random search and simulated annealing as global
   search methods and Nelder Mead Simplex as local search. 

   The efficiency of the previous approach is highly dependent on how
   much the hypothesis about he search space are wrong and sometimes
   it is difficult know how it looks. For this reason some have worked
   on generic heuristics that combine all or part of the previous aspects
   such as pattern search\cite{Hooke:1961:DSS:321062.321069} which is
   a derivative-free based search that combines global search that
   explore the space in a finite set of direction to find  
   regions of interest and local search to examine regions of
   interest. This kind of methods allow to make less hypothesis and
   require less knowledge about the search space. This approach has
   been used in OPAL\cite{orban2011templating}, a meta-programming
   framework. It uses the mesh-adaptive
   direct-search\cite{Audet04meshadaptive}, it is an extention of the
   pattern search. It can explore in an infinte set of directions
   unlike pattern search and use derivative information when available
   to speedup the search.

   While some people developed framework to characterize the search
   space such as ASK\cite{deoliveiracastro:hal-00952307} in order to
   have a better understanding of it. This tool emphasis on the
   sampling because it is crucial for build an accurate model. It
   provides a complex sampling pipeline with different surrogate
   methods (Bayesian regression, interpolation, etc...)

   In some cases, the problem structure is well know and one has an
   idea of where is the optimal solution but the structure of the
   space in this neighborhood is too complex. The approach
   taken in Atlas \cite{Whaley:1998:ATL:509058.509096} is to focus
   only in one part of the search space to perform an exhaustive
   search. But this require know the problem well and where to search.

   In general auto-tuners exclude the user from the optimization
   process. It means that it is difficult for him to know if the
   result can be further improved, and has no clue about the quality
   of the solution. Our goal is to give more feedback and control
   through an semi-automatic and interactive approach to
   the user to guide him in the tuning the his application. Our
   approach is global and allow the user to evaluate the relevance of
   his hypothesis. We the feedback provided he is able to prune the
   search space to allow very low cost optimization.

   In the past a similar approach have been tempted by
   Brewer\cite{Brewer:1995:HOV:209937.209946} where linear regression
   of expectation have been used for the modelization to predict the
   objective function. It worked fine platform CM-5, simulated version
   of Intel Paragon and network of station based on FORE ATM, but
   these platforms are pretty old. To our knowledge this approach has
   not been used recently in the tuning of applications, we wanted to
   understand why and see if it is suited to the complexity of the
   current platforms.

** Design of experiments
   When there are lots of factors, covering the entire space of
   possible values is prohibitive. The goal experimental design is to
   build experiments in order to study the behavior of a system for a
   low experiments cost. For this reason many techniques has been
   developed to sample the space wisely.
 
   The efficiency of One-Factors-At-a-Time (OFAT) is the method of
   changing one factor at a time when the others are kept fixed. It is
   very limited when there are many factors, because it requires high
   number of experiments and it cannot find interactions between 
   factors. For these reasons factorial designs are generally more
   suited. They vary many parameters at the same time, hence
   interactions can be trapped, the estimate of the impact of the
   factors is more precise with a lower experiment cost.
  
   There are different kind of factorial designs. The first one is the
   full factorial design which consider the entire space. The simplest
   way of doing full factorial design is to chose points in the space 
   uniformly\ref{fig:doe-examples}. The drawback is that the points
   are not well distributed, there are part of space where there are
   lots of points and some other where there just few. The Latin
   Hyper-cube Sampling design provides a better coverage of the space
   by dividing the space into pieces of equal sizes and taking the
   same number of points at random in these areas. This method is made
   for continuous factors.  

   On other kind of factorial designs is the fractional factorial
   designs. Instead of considering the whole space it consider only a
   part of it. This part is chosen according to the statement that
   main effect and low order interactions (Sparsity of effect
   principle) are enough to explain the system. One of them is the
   screening design that consider only the lowest and highest values
   for factors.

   Optimal design is another category of factorial design. It samples
   the space such way that the variance is minimum, hence the
   estimation of the factors as the minimum bias. The points are taken
   according statistical model that means that means that the model
   must be already known. The advantage of optimal designs over
   non-optimal is that the need less experiment, as the sampling is
   localized. The D-Optimal design is one of them, it chooses the
   points such that the generalized variances of the least squares
   estimate of a model is minimized.
    
   The tuning of applications is in fact running multiple experiences in an
   automated or semi-automatic process. We thing that techniques from
   experimental design can help us to sample the space efficiently to
   achieve the optimization with low experimental cost.

   #+CAPTION: Space coverage by different DoE
   #+LABEL: fig:doe-examples
   #+begin_src R :results output graphics :file img/DoE_examples.png :exports results :width 600 :height 400 :session
     library(DoE.base)   
     library(DoE.wrapper)   
     library(ggplot2)

     library(grid)
     library(gridExtra)

     random <- data.frame(X1=runif(200,0,4),X2=runif(200,0,4))
     lhs <- lhs.design( type= "maximin" , nruns= 200 ,nfactors= 2 ,digits= NULL ,seed=20049 , factor.names=list(X1=c(0,4), X2=c(1,4) ) )
     Dopt <- Dopt.design(50, data=lhs, formula="~ X1 + X2 + I(1/X2)", nRepeat=5, randomize=TRUE)

     p1 <- qplot(data=random) +
         geom_point(aes(x=X1,y=X2)) +
         ggtitle("Random")

     p2 <- qplot(data=lhs) +
         geom_point(aes(x=X1,y=X2)) +
         ggtitle("lhs")

     p3 <- qplot(data=Dopt) +
         geom_point(aes(x=X1,y=X2)) +
         ggtitle("D-optimal")
         
     grid.arrange(p1, p2, p3, ncol=2, top=textGrob("")) 

   #+end_src

   #+RESULTS:
   [[file:img/DoE_examples.png]]

* DONE Methodology
** DONE Reproducible research
  Such experimental process mandate rigorous methodology.
  In order for this work to be usefull for someone else a laboratory
  book is available publicly on
  github\footnote{https://github.com/swhatelse/M2\_internship}. It 
  contains detailed about installation and configuration steps. It
  keeps tracks of every experiments including their description and
  analysis. Now it has more than 27K lines with more 14K lines of code
  and analysis. It is structured in a chronological way and thus
  follows the natural evolution of the work. This gives the possibility to
  easily understand what has been done at each step and why.
  Every pieces of codes I wrote is explained using literate programming, which
  is straightforward using the org-mode of emacs.
  The github repository also contains the complet set of scripts and
  data used for experiments giving the possibility to anyone to re-run
  the same experiments using the same data.

** DONE Case study
   # Maybe cite Brice paper for this part
   In order to elaborate our approach, we took a very simple example
   which is a kernel that computes the Laplacian of an image. We want
   the minimize the time to compute a pixel and there are multiple
   optimization that can be done to enhance the performance of this
   kernel. The parameters and their values we used to tune this
   applications are the following: 

     | Parameters         | Values                            |
     |--------------------+-----------------------------------|
     | /                  | <                                 |
     | vector length      | 1,2,4,8,16                        |
     | load overlap       | true,false                        |
     | temporary size     | 2,4                               |
     | elements number    | from 1 to 24                      |
     | y component number | from 1 to 6                       |
     | threads number     | 32,64,128,256,512,1024            |
     | local work size y  | 1,2,4,8,16,32,64,128,256,512,1024 |

   The first parameter vector length allow to specify the size of the
   vectors used to performs the computation. The Laplacian can be
   easily vectorized and on hardware that provides vector support 
   it allows to save some decoding phase as the same instruction is
   applied to the entire vector. As each architecture have different
   size of vectors, and some do not provide vector support we need to
   try the different values of vector size.

   # Not satisfying yet
   The second parameter is related to the vectorization. As vectors
   are manipulated, when loading, some data overlap. Thus it is
   possible to synthetize the load from other data and consequently
   reduce the number of loads. 

   The third parameter allows to specify the size of the variables used
   for storing intermediary results. Using smaller type can reduce the
   pressure on the registers but casting variable can also be
   harmfull. Hence the default size is int (4) and we can also use
   short (2). 

   The fourth parameter splits the image into pieces of the size of
   elements number. This specifies the of component a threads will
   process, that is the amount of work per thread, and as a
   consequence defines the number of threads used to perform the 
   computation. More threads can lead to better parallelism but also
   more overhead due to the bigger number of threads to manage. 
   
   # Not satisfying yet
   The fifth parameter is used to specify how the work for a thread is
   organized by specifying the tiling. It gives how the components of
   the image are distributed in the y-axis.

   Finally the two last parameters allow to tune two OpenCL/Cuda
   parameters and describe the distribution of the work at coarse
   grain. In OpenCL and Cuda, threads are grouped and scheduled 
   by blocks on a compute unit. Which means that threads are not
   scheduled individually but by blocks. Thus we use the parameter
   threads number to specify the size of a group. The parameter local
   work size y determines how the threads are organized in a block and
   represent the number of threads in the y-axis. These parameters
   have an impact on the scheduling, the data sharing and the
   occupancy of the compute units. Thus they can lead to better usage
   of the resources.

   All the combinations of these parameters would gives a search space
   of 190080 points. However some points are unfeasible. For instance,
   having more component numbers in the y-axis (y component number)
   than number of component (elements number) itself makes no
   sense. We also have constraint the size of the kernel because it is
   limited to the available resources on the device. Exceeding the
   resources cause the kernel to crash. That is why use constraints to
   reject all the that would produce a kernel to big or that is not
   correct. Finally it remains 23100 points in the search space.

   The experiments are run on one machine with GPU Nvidia K40 using the
   driver 340.32 and two CPUs Intel E5-2630.
* DONE Envisioned general approach
   When using fully automated tools, the user has no feedback about
   the optimization process and does not have a lot of control. How
   good is the optimized version of the code? How is it possible to
   improve it? What does the search space look like? What are the
   parameters that have a big impact (high-order parameters) and those
   which have a small impact (low-order parameters)? All this
   questions are necessary to understand the structure of the
   problem and provide valuable information for the expert to be able
   to prune the search space correctly and to choose the most suited
   search techniques. Thus we investigated the design of a
   semi-automated approach where the user tunes his application in an
   interactive way. All along the tuning process this method provides
   valuable information to user to guide him and exploit his
   knowledge. Of course, this assumes that the application developer
   understands well his kernel and knows the reason each code
   optimization he used.  

   As objective the function is empirical and is costly to evaluate, our
   approach consist in sampling the space with only few points to
   build a model in order to approximate it at low cost. We focused
   on linear regression because usually, it is enough to model
   accurately computer phenomenon. However a correct modeling goes
   to together with efficient sampling techniques. That is why we used
   techniques inspired from design of experiments where the goal is to
   maximize the amount of information and minimize the number of
   points.

   The figure\ref{fig:process} show the work-flow of our approach: 
   1. The user interrogate the search space for example to find what
      are the parameters that have the most impact and their
      interactions, check his hypothesis about the model, etc...
   2. The search space is sampled by taking into account the objective
      of the user. For instance if the user wants to have a first
      overview of the high-order parameters or if he wants to refine
      his model or if he needs to obtain more information about a precise
      part of the search space.
   3. Using linear regression a model is built based on the hypothesis
      provided by the user. It also determines what are the parameters
      that have the most impact. Parameters that have less impact are
      removed from the model and will be re-injected later when higher
      order parameters are fixed.
   4. The best value for the studied parameters are predicted from the
      model.
   5. The result of the regression and estimated best value for the
      parameters are return to the user. At this step, either he is
      satisfied by the result of the regression and he can prune the
      search space by fixing the parameters to the estimated values.
      Or he can ask to test another model, ask more points in a
      particular area to refine the model, etc...
   6. This process iterate until all parameters are fixed.

   In short the tuning is done through an iterative and instrumented
   process where the user refine is model according to the extracted
   information.
     
 
   #+BEGIN_LaTeX
   \begin{figure}[tbh]
   \centering
   \includegraphics[width=.8\linewidth]{./img/process.pdf}
   \caption{\label{fig:process}Workflow}
   \end{figure}
   #+END_LaTeX
* DONE Controlling measurement
   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data_detailed_v2.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file img/warm_up.pdf :exports none :width 8 :height 6 :session
     library(plyr)
     library(ggplot2)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     d2 <- df[df$lws_y == 2 & df$elements_number == 1 & df$threads_number == 32,]
     
     df2 = ddply(d2,.(run_index,vector_length,image_size_index), summarize, 
                      mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))
     
     
     ggplot(d2) +
          geom_line(aes(x=run_index, y=time_per_pixel, color=factor(load_overlap),linetype=factor(temporary_size))) + 
          geom_errorbar(data=df2,aes(x=run_index,y=mean, ymin=mean-err, ymax=mean+err)) +
          facet_grid(vector_length ~ image_size_index, scales="free_y", labeller=label_both) 
   #+end_src

   #+RESULTS:
   [[file:img/warm_up.pdf]]
   
   Current hardware has became more and more complex and provides
   features such that power saving, frequency scaling, etc... Thus it
   is possible to have measurements that are different from an
   experiment to another even if the set of inputs is the same exactly the
   same. For instance, frequency scaling mechanism could chose to scale
   down the frequency of the CPU because of the temperature inside the
   computer case has increased which would have an impact on the
   compute time. To have trusted measurements we are concerned about
   kind of problems because the metric in our case which is the time to
   compute a pixel, is sensitive to this. Thus we have to protected
   against variability between the same measurements and especially
   the warm-up effect. This phenomena can occurs on devices providing
   energy saving features. This kind of devices generally have a
   performance mode and an idle mode. As long as the device does not
   have a lot of work it stays in idle mode but at a some threshold it
   switches to the performances mode. Thus the device does not provides
   all its capabilities immediately, hence the warm-up effect.

   The measurement process is made as follows:
   1. Generation of the next a version of the code
   2. Compilation
   3. Bench-marking on several image sizes multiple times.
  
   As the code is executed on a GPU, the latter has no work to do
   during the code generation and compilation phases. For this reason
   we suspected that warm-up effect can occurs at this moment and also
   after an image is loaded. We tried to see if on the GPU Nvidia K40
   there this effect is present. We also tried to quantify it along
   with the variability we could have between the different runs of
   the same version of the code in order to protect against it. The
   figure\ref{warmup}, illustrates what we expected, there is a power
   saving mechanism on Nvidia K40 which turns the GPU into idle mode
   when the computational intensity is bellow a threshold. This effect
   occurs on the first size of image tested, which is just after the
   code generation and compilation phases. The more run are performed the
   better the performances. It also could have been the case when
   going from one image size to another, the GPU could have switched
   to idle mode while the loading of the image but is not the case the
   GPU does not have the time to switch to idle mode. So prevent to
   protected against warm-up effect we just need to make at least four
   runs on the first size of image and we keep the run the gives
   minimal time to compute one pixel. However we also did the same
   four all the size of images. An other concern is the variability
   between multiple execution of the same version of code but as we
   can see, it is only due to the warm-effect in the first image
   size. On the other size of images we have almost no variability.

   #+BEGIN_LaTeX
   \begin{figure}[htb]
   \centering
   \includegraphics[height=.5\textheight]{./img/warm_up.pdf}
   \caption{\label{fig:warmup}Warm-up effect}
   \end{figure}
   #+END_LaTeX
* DONE Results
***                                                                :noexport:
    #+begin_src R :results output graphics :file ./img/results_hist.pdf :exports none :width 6 :height 8 :session
      df_all_methods <- read.csv("../../../data/2016_04_08/pilipili2/18_08_24/all_search_2.csv", strip.white=T, header=T)  
        library(ggplot2)
        library(plyr)

        df_mean = ddply(df_all_methods,.(method), summarize, 
                        mean = mean(slowdown))

        df_median = ddply(df_all_methods,.(method), summarize, 
                          median = median(slowdown))

        df_err = ddply(df_all_methods,.(method), summarize,
                      mean = mean(slowdown), err = 2*sd(slowdown)/sqrt(length(slowdown)))

        ggplot(df_all_methods ) + 
            facet_grid(method~.) +
            theme_bw() +
            geom_histogram(aes(slowdown),binwidth=.05,color="white", fill="gray48") +
            geom_rect(data = df_err, aes(xmin=mean-err, xmax=mean+err, ymin=0, ymax=60, fill="red"), alpha=0.3) +
            geom_vline( aes(xintercept = median), df_median, color="darkgreen", linetype=2 ) +
            geom_vline( aes(xintercept = mean), df_mean, color="red", linetype=2 ) +
            labs(y="Frequency", x="Slowdown compared best combination of the entire search space") +
            scale_fill_discrete(name="",breaks=c("red"), labels=c("Mean error")) +
            ggtitle("") + 
            theme(legend.position="top") +
            coord_cartesian(xlim=c(.9,3), ylim=c(0,60))
    #+end_src

    #+RESULTS:
    [[file:./img/results_hist.pdf]]
*** 
    To evaluate our solution, we compared it against the following search
    methods that have already been used in auto-tuning:
    - RS: is the uniform random search that take points randomly in the
      search space with equal probabilities. 
    - GA: BOAST embeds an implementation of a genetic algorithm. We
      used a population size of 20, number of generations of 5 and
      mutation rate of 0.1. Among the different configuration tested
      it was the one that gives the best results, however it could be
      possible to obtain better results by tuning further the options.
    - LHS: it is not methods use to search, it is a sampling
      techniques which take point randomly but which also maximize the
      distance between the point to cover the full search
      space. However we want to see how a search based on it would
      perform. 
    - GR: we implemented a greedy search which is a derivative-free
      local search. From a starting point it explores all the possible
      directions at distance one and goes to the direction that gives
      the best improvement. This kind of algorithm is very efficient
      on convex objective function or if we already know where to
      search.
    
    There are many ways of performing linear regression. We evaluated
    two of them: 
    - LM: it uses the least square regression which gives an estimate
      of the mean.
    - RQ: it uses the quantile regression which gives an estimate of
      a given quantile.

    We measured each methods one hundred time with about 120 (0.5% of
    the search space) points for GA, RS, and LHS and we
    evaluated the slowdown achieved compared to the best solution
    available in the entire search space. For GR only 1 random point
    is chosen as starting point.

    We evaluated our approach using the random uniform sampling
    techniques to sample the search space. It starts with 50 random
    points and adds just enough points after pruning to perform the
    regression. The total number of points used lays between 84
    and 89. Our approach is intended to be semi-automatic but for
    evaluation purpose we automated the process. For this we decided
    to apply exactly the same strategy each time.  We fixed the
    parameters in the same order, thus pruning decisions are the same
    and we used exactly the same model without considering the
    structure of the random set and same things for the sampling
    decisions.

    Figure\ref{fig:search_comparison} shows the results of the
    different methods. With this search space, the local search GR is
    inefficient, half of the time we get a slowdown of higher than
    x2. It can be every far from the optimal solution, up to x66
    slower\ref{tab:comparison-table}. The LHS search does a lot more
    better with a slowdown that is never higher than 38.8%. 50% of the
    time we can get a slowdown lower than 17%.
    The uniform random search RS is very efficient here. Half of the time
    we get a slowdown that is less than 7.9% and we do not get a
    maximum slowdown of 34.1%.
    The genetic algorithm GA gives even better results with a slowdown
    which is less than 7.3% half of the time and a maximum slowdown of
    39%. 
    The technique using least square regression gives in general better
    results than the genetic algorithm with a higher rate of good
    performing versions, 50% of the time the slowdown is less than
    6.8%. However, sometimes it fails to find high performing versions
    and can be worst than GA with a maximum slowdown of x2.064.
    The quantile regression is the most effective techniques here with
    an even higher rate of high performing versions, half of the time
    it is very close the optimal solution with a slowdown that is
    lower than 1.2% with the worst versions that are not slower than
    17.3%.

    In brief, the regression of expectation gives generally better
    results than the implementation of the genetic algorithm we used
    but sometime can be worse. The quantile regression performed
    better than the genetic algorithm all the time with near optimal
    solutions half of the time.
     
   #+BEGIN_LaTeX
   \begin{figure}[htb]
   \centering
   \includegraphics[width=\linewidth]{./img/results_hist.pdf}
   \caption{\label{fig:search_comparison}Search comparison}
   \end{figure}
   #+END_LaTeX

    #+CAPTION: This table shows the minimum, maximum, mean and median slowdown including the mean number of points used by each method
    #+NAME:   tab:comparison-table
    | Methods |   Min |    Max |  Mean | Median | Mean Cost |
    |---------+-------+--------+-------+--------+-----------|
    | /       |    <> |     <> |    <> |     <> |           |
    | GR      | 1.006 | 66.010 | 7.964 |  2.161 |      2.33 |
    | LHS     | 1.009 |  1.388 | 1.155 |  1.172 |      99.6 |
    | RS      | 1.011 |  1.341 | 1.098 |  1.079 |       120 |
    | GA      |  1.00 |  1.390 | 1.098 |  1.073 |       120 |
    | LM      |  1.06 |  2.064 | 1.187 |  1.068 |     88.25 |
    | RQ      | 1.010 |  1.173 | 1.053 |  1.012 |     88.46 |

* Analysis
  This part gives a study of the search space and an explanation of
  the results of our approach. We also explain why the quantile
  regression is more suited than regression of expectation in
  optimization purpose. The automatisation of this approach is not the
  best way to use it and results can be further improved. Hence this
  part explains how results can be improved by using sampling strategy
  and choosing and refining the model according to the target problem.   
  Finally we dig into constrained optimization techniques to converge
  faster to the optimal solution using our model of the objective
  function. 
** DONE Characteristics of the search space
***                                                                :noexport:
   #+begin_src sh :results output :exports none
       ruby ../scripts/clean_data.rb ../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end.yaml
   #+end_src

   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_cleaned.yaml
   #+end_src

   #+RESULTS:
*** 
   #+begin_src R :results output graphics :file ./img/search_combination_rep_slowdown.png :exports results :width 800 :height 600 :session
     library(ggplot2)
     library(grid)
     library(gridExtra)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)

     slowdown <-  df$time_per_pixel / min(df$time_per_pixel)
     df$slowdown <- slowdown

     p2 <- qplot(data=df) +
         geom_histogram(aes(x=slowdown,y=..density.. * 0.05), binwidth=.05) +
         theme(axis.text.x = element_text(angle = 70, hjust = 1, face="bold", size=12)) +
         geom_vline(xintercept = median(slowdown), color="darkgreen", linetype=2) +
         geom_vline(xintercept = quantile(slowdown, prob=c(0.25,0.75)), color="blue", linetype=2) +
         geom_vline(xintercept = mean(slowdown), color="red", linetype=2) +
         ggtitle("Density of the combinations slowdown compared to the best") +
         labs(y="Density", x="Slowdown")

     p3 <- qplot(data=df) +
         geom_histogram(aes(x=slowdown,y=..density.. * 0.05), binwidth=.05) +
         theme(axis.text.x = element_text(angle = 70, hjust = 1, face="bold", size=12)) +
         geom_vline(xintercept = median(slowdown), color="darkgreen", linetype=2) +
         geom_vline(xintercept = quantile(slowdown, prob=c(0.25,0.75)), color="blue", linetype=2) +
         geom_vline(xintercept = mean(slowdown), color="red", linetype=2) +
         ggtitle("Density of the combinations slowdown compared to the best") +
         coord_cartesian(xlim=c(.9,17)) +
         labs(y="Density", x="Slowdown")

     grid.arrange(p2, p3,  ncol=2, top=textGrob("Repartition of the performance combination")) 
   #+end_src

   #+RESULTS:
   [[file:./img/search_combination_rep_slowdown.png]]

  By studying the characteristics of the search space we can
  understand the structure of the problem in order to be able to
  understand the results of the different search techniques. 
  The figure\ref{} and the table\ref{search-space-characteristics}
  show the distribution of the combinations over the 
  search space in term of slowdown. This search space contains a lot
  of good combinations, half of them have a slowdown that
  is less x6.1 which is x2.8 faster than the mean slowdown. However
  there are few bad ones with the worst at a slowdown of x382. Thus
  the probability of finding a good performing combination is high,
  this is the reason why randomized algorithms such as the RS, GA and
  LHS good results. Yet, this search space remains complicated,
  because as we saw previously our local search GR failed which means
  there are a lot of local optimum in which it is stuck and some are
  far from the optimal one. 

  #+CAPTION: This presents the slowdown characteristics of the search space
  #+NAME:   tab:search-space-characteristics
  | Min  | 1st Q. | Median | Mean   | 3rd Q. |     Max |
  |------+--------+--------+--------+--------+---------|
  | /    | <>     | <>     | <>     | <>     |         |
  | 1.00 | 2.599  | 6.116  | 17.276 | 17.177 | 382.168 |


   We can also notice in the figure\ref{} that the variability is not
   the same everywhere, hence our random variables are
   heteroscedastics. This  because the noise does not 
   follow the same law for the different value of the same
   parameters. This noise is due to complex interactions between
   parameters.  

   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data.rb ../../../data/2016_03_11/pilipili2/19_13_54/Data19_13_54_linear.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file ./img/heteroscedasticity.png :exports results :width 700 :height 400 :session
     library(ggplot2)
     library(grid)
     library(gridExtra)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)

      p1 <- qplot() + 
          geom_point( aes(x=df$vector_length, y=df$time_per_pixel), alpha=0.1 ) + 
          ggtitle("Impact of the vector length") +
          labs(y="time per pixel in seconds", x="vector length") +
          theme(axis.text=element_text(size=12),
                axis.title=element_text(size=14,face="bold"))

      p2 <- qplot() + 
          geom_point(aes(x=df$x_component_number, y=df$time_per_pixel),alpha=0.1) + 
          ggtitle("Impact of number of component on the x-axis") +
          labs(y="time per pixel in seconds", x="x component number") +
          theme(axis.text=element_text(size=12),
                axis.title=element_text(size=14,face="bold"))

     grid.arrange(p1,p2,  ncol=2, top="") 

   #+end_src

   #+RESULTS:
   [[file:./img/heteroscedasticity.png]]

** DONE Differences between regression of expectation and quantile regression
   The previous results showed that using linear regression gives
   often good results. But there is a significant difference between
   regression of expectation and quantile regression. It is due to the
   fact that they do not predict the same thing.
*** Linear regression of expectation: why it can be inefficient
   #+begin_src sh :results output :exports none
       ruby ../../../scripts/format_data.rb ../../../data/2016_03_11/pilipili2/19_13_54/Data19_13_54_linear.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file img/lm.png :exports results :width 800 :height 400 :session 
     library(ggplot2)
     library(plyr)
     library(gridExtra)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     err_x_comp = ddply(df,c("x_component_number"), summarize,
                        mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))


     err_v_len = ddply(df,c("vector_length"), summarize,
                       mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))

     p1 <- qplot(df$vector_length, df$time_per_pixel) + 
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_errorbar(data=err_v_len,aes(x=vector_length,y=mean, ymin=mean-err, ymax=mean+err),colour="red") +
         ggtitle("Impact of the vector length") +
         labs(y="time per pixel in seconds", x="vector length") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))

     p2 <- qplot(df$x_component_number, df$time_per_pixel) + 
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_errorbar(data=err_x_comp,aes(x=x_component_number,y=mean, ymin=mean-err, ymax=mean+err),colour="red") +
         ggtitle("Impact of number of component on the x-axis") +
         labs(y="time per pixel in seconds", x="x component number") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))

     grid.arrange(p1, p2, ncol=2, top="") 

   #+end_src
   
   #+CAPTION: Linear regression and non-uniform noise
   #+LABEL: fig:lm-1
   #+RESULTS:
   [[file:img/lm.png]]
   
   Linear regression of expectation has already been used successfully
   with auto-tuning problems by Brewer in the
   1990s\cite{Brewer:1995:HOV:209937.209946}. But this method have
   been put aside for no real reasons to our knowledge. Using this
   method to study the impact of the parameters with using linear
   models to approximate the behavior of the search space coupled with
   efficient sampling strategies seemed very appealing to us. 

   This techniques assumes that the noise is uniform and more
   specifically follows the Gaussian law, in this case we say that the
   variable are homoscedastics. However the figure\ref{fig:lm-1} shows
   that in the case of our Laplacian kernel, which is a very simple case,
   we have heteroscedasticity, which means that the noise is
   non-uniform because it is due to complex interaction between
   parameters. Heteroscedasticity is problematic because
   the least square is not the Best Linear Unbiased Estimator in this
   case and it biases the variance and thus the coefficient of
   determination which makes it more difficult to evaluate the
   accuracy of the model. In addition, we want to predict the minimum
   value of the objective function not the mean. With non-uniform
   noise the evaluation of the minimum value does not follows the
   evolution of the mean.
   
   The reason why linear regression of expectation have been
   efficient in Brewer's work\cite{Brewer:1995:HOV:209937.209946} is
   probably because at that time the architecture of computers was
   less complicated than today and the noise was uniform. If the error
   law is the same everywhere as in the left in figure\ref{fig:lm-1}
   we can still have the minimum values that follow the same evolution
   as the mean and we can still predict the minimum. The resulting
   model and approximation can still be correct and we can easily know
   what is the best size of vector. But we would still need to make
   assumptions that about the error and we do not know anything it. In
   the right in figure\ref{fig:lm-1}, the evolution of the mean and
   the evolution of the minimum is not correlated and the best value
   is mispredicted.     

   We conclude that in the case of heteroscedasticity and non-uniform
   error law, linear regression tracks the general tendency of impact
   of the parameters. But in our case in which we are interested about
   the minimum which is uncorrelated to the mean, the linear
   regression of expectation cannot lead to the global optimum and we
   need another estimator for the minimum. 

*** The choice of quantile regression
****                                                               :noexport:
   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data.rb ../../../data/2016_03_11/pilipili2/19_13_54/Data19_13_54_linear.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file img/why_we_choose_quantile_reg.pdf :exports results :width 6 :height 4 :session
     library(ggplot2)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     ggplot(df) + 
         aes(x=x_component_number, y=time_per_pixel) +
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_smooth(method="lm", formula = y ~ x + I(1/x), aes(colour="Least square regression")) +           
         stat_quantile(quantiles=0.05, formula = y ~ x + I(1/x), aes(colour="Quantile regression")) +
         ggtitle("Impact of number of component on the x-axis") +
         labs(y="Time per pixel in seconds", x="x component number") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))
   #+end_src

   #+RESULTS:
   [[file:img/why_we_choose_quantile_reg.pdf]]

**** 
    Quantile regression gives the estimate of quantile and has been
    proven successful in the ecologic field\cite{FEE:FEE200318412}
    where complex interactions between factors lead to non-uniform
    noise which is exactly the case of our Laplacian kernel. Moreover
    it has the ability to estimate multiple tendency from the minimum
    to the maximum. As we want to minimize the time to compute a pixel
    we need to estimate the minimum and regression on the 5th
    percentile is a generally a good estimation of 
    it\cite{books/daglib/0076234}. 
    Figure\ref{fig:qr-example} illustrates the comparison
    between regression of expectation and quantile regression. The
    regression of expectation estimates that the best performing
    version has a x_component_number of 4 which is not true. While
    the quantile regression of the 5th succeed in predicting that the
    best performing version has a x_component_number of 1. So the
    regression of expectation may not find the minimum while the
    quantile regression does if the model is correct.
    There are different ways of computing quantile regression. We tried
    the rq function from the quantreg package of R, it gives good
    estimation of the coefficient of the different parameters but for
    some reason it fails with our data to compute the standard error
    and the R-squared. Instead we use another techniques that use
    performs weighted least square to apply more weight to the desired
    quantile. And it iterates in order to converge to the quantile
    regression of the wanted quantile. Nevertheless, we do not yet
    master this technique completely, we managed to get the correct
    coefficients but we do not know how to determine correctly the
    minimum number of iterations to converge and how to determine
    correct the bounds for the weights. The other difficulty with this
    method is that we do not really know how to interpret the standard
    error, how to compute confidence intervals and the R-squared
    values are too optimistic. Hence it is a bit more complicated to
    make inferences about models. 

   #+BEGIN_LaTeX
   \begin{figure}[htb]
   \centering
   \includegraphics[width=.9\linewidth]{./img/why_we_choose_quantile_reg.pdf}
   \caption{\label{fig:qr-example}Linear regression vs quantile regression}
   \end{figure}
   #+END_LaTeX

** TODO Model choice, refinement and pruning decision
   Usually the expert understands every optimization of its kernel and
   has ideas about their potential effects. He needs not only check his
   hypothesis but also to have an accurate estimation of the impact of
   these optimization parameters.
   In the case of our Laplacian kernel the elements_number parameter
   is the number of component a thread compute and thus determines the
   number of threads used to perform the computation of the
   Laplacian. Using more threads allows to compute more component at
   in parallel. However it can also lead to a less efficient sharing
   of the cache resources as it will increase the number of memory
   loads. Also the higher the number of threads the more important the
   overhead due to their management. Hence the impact of the number of
   elements can be modeled as follow:
   #+BEGIN_LaTeX
   \[ \displaystyle elements number + \frac{1}{elements number} \]
   #+END_LaTeX  
  
   The parameter y_component_number

** TODO Using less point as possible: sampling strategy
** TODO Model optimization
***                                                                :noexport:
    #+begin_src sh :results output :exports both
      ruby ../scripts/format_data.rb ../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_cleaned.yaml
    #+end_src
    
    #+RESULTS:
    
    #+begin_src R :results output :session :exports both
      df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
    #+end_src
    
    #+RESULTS:
    
    #+begin_src R :results output graphics :file img/search_space_3D.pdf :exports both :width 6 :height 6 :session                                                                   
      f <- function(x) { x * (0.05 - 1 * (x < 0)) }
      g <- function(x) { f(x)/x^2 }
      h <- function(x) {pmin(g(x),1e15)}

      model <- time_per_pixel ~ elements_number + I(1/elements_number) + 
          y_component_number + I(1/y_component_number) + 
          elements_number:y_component_number

      fit <- lm(data=df,formula=model)
      for(i in 1:200){
          E <- residuals(fit)
          fit <- lm(data=df,formula=model,weight=g(E))
      }

      library(plot3D)
      f <- function(x){
          as.numeric(predict(fit,data.frame(elements_number=x[1], y_component_number=x[2]),interval="none"))
      }
      combinations <- matrix(NA,nrow=24,ncol=6)
      for(i in 1:24){
          for(j in 1:6){
              combinations[i,j] <- f(c(i,j))
          }
      }
      persp3D(xlab="elements_number", ylab="y_component_number", zlab="time per pixel in s", z=combinations, phi=20, theta=220, clab=c("time per pixel in s"),colkey = list(side=1,length = 0.5))
    #+end_src
    
    #+RESULTS:
    [[file:img/search_space_3D.pdf]]
    
    #+begin_src R :results output graphics :file img/search_space_3D_constrained.pdf :exports both :width 6 :height 6 :session
      g <- function(x){
          ifelse(x[1] <= x[2] * 4, f(x), NA)
      }

      resi = 5
      resj = 5
      offi = 3
      offj = 3
      combinations <- matrix(NA,nrow=24*resi,ncol=6*resj)
      for(i in 1:(24*resi)){
          for(j in 1:(6*resj)){
              combinations[i,j] <- g(c((i+offi)/resi,(j+offj)/resj))
          }
      }

      persp3D(x=1:(24*resi)/resi, y=1:(6*resj)/resj, xlab="elements_number", ylab="y_component_number", zlab="time per pixel in s", z=combinations, phi=20, theta=220, clab=c("time per pixel in s"), colkey = list(side=1,length = 0.5))
    #+end_src
    
    #+RESULTS:
    [[file:img/search_space_3D_constrained.pdf]]
    
    #+begin_src R :results output graphics :file img/search_space_3D_barrier.pdf :exports both :width 6 :height 6 :session
      h <- function(x){
          ifelse(x[1] <= x[2] * 4, f(x), (1 + .05*abs(x[1]-4*x[2]) ) * f(x))
      }

      combinations <- matrix(NA,nrow=24,ncol=6)
      for(i in 1:24){
          for(j in 1:6){
              combinations[i,j] <- h(c(i,j))
          }
      }

      persp3D(xlab="elements_number", ylab="y_component_number", zlab="time per pixel in s", z=combinations, phi=20, theta=190, clab=c("time per pixel in s"),colkey = list(side=1,length = 0.5))

    #+end_src
    
    #+RESULTS:
    [[file:img/search_space_3D_barrier.pdf]]

*** 
   #+BEGIN_LaTeX
   \begin{figure}[htb]
   \centering
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/search_space_3D.pdf}
   \caption{\label{fig:search_space_3d}Search space 3D}
   \end{minipage}
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/search_space_3D_constrained.pdf}
   \caption{\label{fig:search_space_3d}Constrained Search space}
   \end{minipage}
   \end{figure}
   #+END_LaTeX

   #+BEGIN_LaTeX
   \begin{figure}[htb]
   \centering
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/search_space_3D_barrier.pdf}
   \caption{\label{fig:search_space_3d}Barrier approach}
   \end{minipage}
   \end{figure}
   #+END_LaTeX
* TODO Future work
* Conclusion
#+LaTeX: \nocite{*}
#+LaTeX: \def\raggedright{}
\bibliographystyle{plain}
\bibliography{../../biblio.bib}


* Emacs Setup 							   :noexport:
  This document has local variables in its postembule, which should
  allow Org-mode to work seamlessly without any setup. If you're
  uncomfortable using such variables, you can safely ignore them at
  startup. Exporting may require that you copy them in your .emacs.

# Local Variables:
# eval:    (require 'org-install)
# eval:    (org-babel-do-load-languages 'org-babel-load-languages '( (sh . t) (R . t) (perl . t) (ditaa . t) ))
# eval:    (setq org-confirm-babel-evaluate nil)
# eval:    (unless (boundp 'org-latex-classes) (setq org-latex-classes nil))
# eval:    (add-to-list 'org-latex-classes '("memoir" "\\documentclass[smallextended]{memoir} \n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n  \\usepackage{graphicx}\n  \\usepackage{hyperref}" ("\\chapter{%s}" . "\\chapter*{%s}") ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("acm-proc-article-sp" "\\documentclass{acm_proc_article-sp}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (setq org-alphabetical-lists t)
# eval:    (setq org-src-fontify-natively t)
# eval:   (setq org-export-babel-evaluate nil)
# eval:   (setq ispell-local-dictionary "english")
# eval:   (eval (flyspell-mode t))
# eval:    (setq org-latex-listings 'minted)
# eval:    (setq org-latex-minted-options '(("bgcolor" "white") ("style" "tango") ("numbers" "left") ("numbersep" "5pt")))
# End:

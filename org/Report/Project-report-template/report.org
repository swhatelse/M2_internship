#+TITLE: Optimization of the Auto-Tuning of HPC Application Computing Kernels
#+Author:
#+LaTeX_CLASS: memoir
#+LaTeX_CLASS_OPTIONS: [12pt, a4paper]
#+OPTIONS: H:5 title:nil author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil

#+LATEX_HEADER:\usepackage[french,english]{babel}
#+LATEX_HEADER:\usepackage [vscale=0.76,includehead]{geometry}                % See geometry.pdf to learn the layout options. There are lots.
# #+LATEX_HEADER:\geometry{a4paper}                   % ... or a4paper or a5paper or ... 
# #+LATEX_HEADER:\geometry{landscape}                % Activate for for rotated page geometry
# #+LATEX_HEADER:\OnehalfSpacing
# #+LATEX_HEADER: \setSingleSpace{1.05}
# #+LATEX_HEADER:\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
#+LATEX_HEADER:\usepackage{amsmath}
#+LATEX_HEADER:\usepackage{fullpage}
#+LATEX_HEADER:\usepackage{mathptmx} % font = times
#+LATEX_HEADER:\usepackage{helvet} % font sf = helvetica
#+LATEX_HEADER:\usepackage[latin1]{inputenc}
#+LATEX_HEADER:\usepackage{relsize}

#+BEGIN_LaTeX
%Style des tÃªtes de section, headings, chapitre
\headstyles{komalike}
\nouppercaseheads
\chapterstyle{dash}
\makeevenhead{headings}{\sffamily\thepage}{}{\sffamily\leftmark} 
\makeoddhead{headings}{\sffamily\rightmark}{}{\sffamily\thepage}
\makeoddfoot{plain}{}{}{} % Pages chapitre. 
\makeheadrule{headings}{\textwidth}{\normalrulethickness}
%\renewcommand{\leftmark}{\thechapter ---}
\renewcommand{\chaptername}{\relax}
\renewcommand{\chaptitlefont}{ \sffamily\bfseries \LARGE}
\renewcommand{\chapnumfont}{ \sffamily\bfseries \LARGE}
\setsecnumdepth{subsection}


% Title page formatting -- do not change!
\pretitle{\HUGE\sffamily \bfseries\begin{center}} 
\posttitle{\end{center}}
\preauthor{\LARGE  \sffamily \bfseries\begin{center}}
\postauthor{\par\end{center}}

\newcommand{\jury}[1]{% 
\gdef\juryB{#1}} 
\newcommand{\juryB}{} 
\newcommand{\session}[1]{% 
\gdef\sessionB{#1}} 
\newcommand{\sessionB}{} 
\newcommand{\option}[1]{% 
\gdef\optionB{#1}} 
\newcommand{\optionB}{} 

\renewcommand{\maketitlehookd}{% 
\vfill{}  \large\par\noindent  
\begin{center}\juryB \bigskip\sessionB\end{center}
\vspace{-1.5cm}}
\renewcommand{\maketitlehooka}{% 
\vspace{-1.5cm}\noindent\includegraphics[height=14ex]{logoINP.png}\hfill\raisebox{2ex}{\includegraphics[height=7ex]{logoUJF.jpg}}\\
\bigskip
\begin{center} \large
Master of Science in Informatics at Grenoble \\
Master Math\'ematiques Informatique - sp\'ecialit\'e Informatique \\ 
option \optionB  \end{center}\vfill}
% End of title page formatting

\option{$<$option-name$>$}
%\title{ Project Title }%\\\vspace{-1ex}\rule{10ex}{0.5pt} \\sub-title} 
\author{Author Name}
\date{ $<$Defense Date$>$} % Delete this line to display the current date
\jury{
Research project performed at $<$lab-name$>$ \\\medskip
Under the supervision of:\\
$<$supervisor's first-name and last-name, supervisor's institution$>$\\\medskip
Defended before a jury composed of:\\
$[$Prof/Dr/Mrs/Mr$]$ $<$first-name last-name$>$\\
$[$Prof/Dr/Mrs/Mr$]$ $<$first-name last-name$>$\\
$[$Prof/Dr/Mrs/Mr$]$ $<$first-name last-name$>$\\
$[$Prof/Dr/Mrs/Mr$]$ $<$first-name last-name$>$\\
}
\session{$[$June/September$]$\hfill year}
#+END_LaTeX

#+BEGIN_abstract
  Blablabla
  \newpage
#+END_abstract

* Plan                                                             :noexport:
** Introduction
   - In HPC code optimization crucial to exploit hardware.
     Cannot wait for the next generation to bring speedup because it
     does not (Frequency not higher but more cores and henanced ISA). 
   - HPC plaforms \ne hardware \to code optimizations not portable.
     Porting application to another platform is time consumming and
     can be very tricky. Automatize the porting using tools \to
     autotuner.

   - BOAST framework ruby generating portable code in C, Fortran,
     OpenCL. DSL
** Problem analysis
   - Huge search space \to need to explore only part of it \to
     optimization problem.
   - Interactions between parameters
   - Non-smooth and empirical objective function
   - Combination of discrete and continuous parameters
     
** State of the art
   # - Atlas \to small search space or if we know where to search \to
   #   exhaustive search 
   # - Local search like gradiant search \to to know where to start
   #   Can be stuck at local minimum and be from the global optimum
   # - Random algorithms random search, genetic algorithm. 
   #   Efficient on complex problem with no geometry.
   #   Can escape from local optimum
   # - Mix of local and global search \to Generalized pattern search
   # - Using modelization get get knowledge about the search space and
   #   to predict behavior
   #   - Learning machine \to categorisation of similar problem to use
   #     same strategy, training overhead
   #   - Regression \to possible to use property of the function, such as
   #     derivative, convexity,etc...
*** Using information about the problem - Objective function
    - Derivative methods \to local strategy
      - If non convex \to multiple local minimum \to need to know where to
        start or randomized strategy e.g. simulated annealing
      - If derivation not possible (empirical function) estimate with regression
*** Using information about the problem - Other kind of knowledge   
    Problem too complex
    Heuristic based: genetic algorithm, random search, pattern search
    Also machine learning \to identifying category of problem and
    strategy that work well
*** Our goal
    - Complex methods used but no explanation on why they work
    - Try a simple approach and try to understand it deeply
    - Analytics methods & experiments design
    - Study of the search space on simple example

** Methods and material
  - Reproducible work
    - Lab book on github  
    - Literate programming 
  - Result validation against bruteforce
  - Comparison with random, gradiant search, and genetic algorithm
** Contribution
*** Case study
    # Maybe this should go in experiments
****  Laplacian
      - Optimizations explanation
        - Vectorization \to vector length
        - Synthetize loading \to load overlap
        - Tilling \to y component nulber
        - Number of threads \to elements number
        - Size of temporary results \to temporary size
        - Size of a work group \to threads number
        - Shape of work group \to lws y
**** Matrix product?
      - Optimizations explanation
*** Why linear regression is not suited
    - Tracks general tendency of the impact of factors
    - Heteroscedasticity
    - Non uniform noise
*** Use of quantile regression
    - Ways of computing quantile regression
    - 5th and 95th percentile \to good estimation for extreme values
    - But optimist R-squared
*** Model choice and refinement
    - Hypothesis based on the kernel
    - Iterative refinement
    - Determines the quality of the prediction
*** Importance of the search space expression
    - Easier modelization
    - Better capture of the search space features
*** Using less point as possible
    - Design of experiment
    - Copying with constraints
** Experiments
   - Bench min of 4 runs \to warm up effect
*** Laplacian
**** Search space characteristics
     - Qualitative observation in term of speed up
**** Comparison with random and genetic algo
** Conclusion

* Introduction
* Problem analysis
* State of the art
* Methods
* Contribution
** Case study
   #+BEGIN_SRC C
     void math(const int32_t width, const int32_t height, const uint8_t * psrc, uint8_t * pdst){
         int32_t i;
         int32_t j;
         int32_t c;
         int32_t tmp;
         int32_t w;
         w = (width) * (3);
         for (j = 1; j <= height - (2); j += 1) {
             for (i = 1; i <= width - (2); i += 1) {
                 for (c = 0; c <= 2; c += 1) {
                     tmp =  -(psrc[c + (3) * (i - (1) + (width) * (j - (1)))]) - (psrc[c + (3) * (i + (width) * (j - (1)))]) - (psrc[c + (3) * (i + 1 + (width) * (j - (1)))]) - (psrc[c + (3) * (i - (1) + (width) * (j))]) + (psrc[c + (3) * (i + (width) * (j))]) * (9) - (psrc[c + (3) * (i + 1 + (width) * (j))]) - (psrc[c + (3) * (i - (1) + (width) * (j + 1))]) - (psrc[c + (3) * (i + (width) * (j + 1))]) - (psrc[c + (3) * (i + 1 + (width) * (j + 1))]);
                     pdst[c + (3) * (i + (width) * (j))] = (tmp < 0 ? 0 : (tmp > 255 ? 255 : tmp));
                 }
             }
         }
     }
   #+END_SRC

   In order to elaborate our approach, we took a very simple example
   which is a kernel that computes the Laplacian of an image. There
   are multiple optimizations that can be done to henance the
   performance of this kernel. 
   The first optimization we can use is the vectorization, this allows
   to take advantage of hardware capable of executing one instruction
   on multiple data at a time and instead of computing one data, so
   multiple data are computed for the same cost. 
   To perform vectorization we need to load more data and
   some data overlap with each other, to reduce the number of load we
   can synthetize those data from other, this is the second
   optimization we can have.
   Another optimization to henance the performs of the kernel can be
   to use smaller type for intermediary results, reducing the pressure
   on the registers.
   We also can determine the number of threads use to performs the
   computation. More threads can lead to better parallelism. We do
   this by specifying the number of component a thread will work on.
* Experiments
* Conclusion

#+TITLE: Optimization of the Auto-Tuning of HPC Application Computing Kernels
#+LANGUAGE: en
#+Author:
#+TAGS: noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LaTeX_CLASS: memoir
#+LaTeX_CLASS_OPTIONS: [12pt, a4paper]
#+OPTIONS: H:5 title:nil author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil
#+BABEL: :session *R* :cache yes :results output graphics :exports both :tangle yes 

#+LATEX_HEADER:\usepackage[french,english]{babel}
#+LATEX_HEADER:\usepackage [vscale=0.76,includehead]{geometry}                % See geometry.pdf to learn the layout options. There are lots.
# #+LATEX_HEADER:\geometry{a4paper}                   % ... or a4paper or a5paper or ... 
# #+LATEX_HEADER:\geometry{landscape}                % Activate for for rotated page geometry
# #+LATEX_HEADER:\OnehalfSpacing
# #+LATEX_HEADER: \setSingleSpace{1.05}
# #+LATEX_HEADER:\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
#+LATEX_HEADER:\usepackage{amsmath}
#+LATEX_HEADER:\usepackage{fullpage}
#+LATEX_HEADER:\usepackage{mathptmx} % font = times
#+LATEX_HEADER:\usepackage{helvet} % font sf = helvetica
#+LATEX_HEADER:\usepackage[latin1]{inputenc}
#+LATEX_HEADER:\usepackage{relsize}
#+LATEX_HEADER:\usepackage{listings}

#+BEGIN_LaTeX
%Style des têtes de section, headings, chapitre
\headstyles{komalike}
\nouppercaseheads
\chapterstyle{dash}
\makeevenhead{headings}{\sffamily\thepage}{}{\sffamily\leftmark} 
\makeoddhead{headings}{\sffamily\rightmark}{}{\sffamily\thepage}
\makeoddfoot{plain}{}{}{} % Pages chapitre. 
\makeheadrule{headings}{\textwidth}{\normalrulethickness}
%\renewcommand{\leftmark}{\thechapter ---}
\renewcommand{\chaptername}{\relax}
\renewcommand{\chaptitlefont}{ \sffamily\bfseries \LARGE}
\renewcommand{\chapnumfont}{ \sffamily\bfseries \LARGE}
\setsecnumdepth{subsection}


% Title page formatting -- do not change!
\pretitle{\HUGE\sffamily \bfseries\begin{center}} 
\posttitle{\end{center}}
\preauthor{\LARGE  \sffamily \bfseries\begin{center}}
\postauthor{\par\end{center}}

\newcommand{\jury}[1]{% 
\gdef\juryB{#1}} 
\newcommand{\juryB}{} 
\newcommand{\session}[1]{% 
\gdef\sessionB{#1}} 
\newcommand{\sessionB}{} 
\newcommand{\option}[1]{% 
\gdef\optionB{#1}} 
\newcommand{\optionB}{} 

\renewcommand{\maketitlehookd}{% 
\vfill{}  \large\par\noindent  
\begin{center}\juryB \bigskip\sessionB\end{center}
\vspace{-1.5cm}}
\renewcommand{\maketitlehooka}{% 
\vspace{-1.5cm}\noindent\includegraphics[height=14ex]{logoINP.png}\hfill\raisebox{2ex}{\includegraphics[height=7ex]{logoUJF.jpg}}\\
\bigskip
\begin{center} \large
Master of Science in Informatics at Grenoble \\
Master Math\'ematiques Informatique - sp\'ecialit\'e Informatique \\ 
option \optionB  \end{center}\vfill}
% End of title page formatting

\option{$<$option-name$>$}
%\title{ Project Title }%\\\vspace{-1ex}\rule{10ex}{0.5pt} \\sub-title} 
\author{Author Name}
\date{ $<$Defense Date$>$} % Delete this line to display the current date
\jury{
Research project performed at $<$lab-name$>$ \\\medskip
Under the supervision of:\\
$<$supervisor's first-name and last-name, supervisor's institution$>$\\\medskip
Defended before a jury composed of:\\
$[$Prof/Dr/Mrs/Mr$]$ $<$first-name last-name$>$\\
$[$Prof/Dr/Mrs/Mr$]$ $<$first-name last-name$>$\\
$[$Prof/Dr/Mrs/Mr$]$ $<$first-name last-name$>$\\
$[$Prof/Dr/Mrs/Mr$]$ $<$first-name last-name$>$\\
}
\session{$[$June/September$]$\hfill year}
#+END_LaTeX

#+BEGIN_abstract
  Blablabla
  \newpage
#+END_abstract

* Plan                                                             
** Introduction
*** Why is performance optimization difficult
   - In HPC code optimization crucial to exploit very complex hardware.
     Cannot wait for the next generation to bring speedup because it
     does not (Frequency not higher but more cores and henanced ISA). 
     - many cores
     - cache hierarchies
     - vector support
     - pipelining ILP
     - GPUs!
   - HPC plaforms have many \ne hardware \to code optimizations not portable.
     Porting application to another platform is time consumming and
     can be very tricky.
   - Many attempts in the last decade to automate the generation of
     optimized code
*** Code generation and opportunities
    - The compiler approach: loop unrolling, vectorization, automatic
      parallelization, loop nest transformation, etc. Yet, many
      opportunities are not exploited as it is too difficult to
      exploit them automatically. Sometimes, the source code has to be
      rewritten in a slightly different way to enable the compiler to
      be effective
    - The source-to-source transformation (C to C, Fortran to Fortran,
      ...). Framework for transform code. Orio. Serge
      Guelton. Difficile mais limité à un seul langage, et
      exploitation d'accelérateurs différents difficile. Ça ne se
      permettra jamais de changer le mapping des données en mémoire
    - Meta-programming approach: allow the programmer to propose
      optimizations that the compiler would not be allowed to do
      (because of the language or because it would require information
      on the application that cannot be given to the compiler).
*** Optimizing: the auto-tuning approach
    - Many optimization options: compiler flags, source-to-source
      transformations, higher-level modifications (tile/block/vector size).
    - Auto-tuning: consider all this as a huge optimization problem
      and try to find the best combination. Many techniques (genetic
      algorithms, simulated annealing, tabu search, machine learning,
      ...) depending on the problem constraints. But mainly two
      problems:
      - the time needed for such optimization
      - knowing whether further optimizations could be expected or not
        (peak performance is generally useless and the optimization
        process is so complex that it's hard to know how it really
        performed) is difficult and même si tu sais qu'il devrait être
        possible de faire mieux, tu sais pas vraiment où, comment( cf
        of genetic algo on the full search space), ...
*** Our goal
    Investigate the design of a semi-automatic optimization framework,
    where the applicaiton developer has control and understands how
    the optimization works and guides it.

    We relied on BOAST, a metaprogramming... (Semi automatic approach
     \to gives back power to the user, framework ruby generating
     portable code in C, Fortran, OpenCL. DSL)  and investigate various
     statisical techniques inspired from the design of expeirments
     field that emphasizes on reducing experiment cost.
*** My contribution

    - Complex methods used but no explanation on why they work
    - Try a simple approach and try to understand it deeply
    - Analytics methods & experiments design
    - Study of the search space on simple example
      

*** Structure of the report
** Problem analysis
   - Huge search space \to need to explore only part of it \to
     optimization problem.
   - Interactions between parameters
   - Non-smooth and empirical objective function
   - Combination of discrete and continuous parameters
   - Constraint optimizations
     Represent unfeasible points.
     
** State of the art
   # - Atlas \to small search space or if we know where to search \to
   #   exhaustive search 
   # - Local search like gradiant search \to to know where to start
   #   Can be stuck at local minimum and be from the global optimum
   # - Random algorithms random search, genetic algorithm. 
   #   Efficient on complex problem with no geometry.
   #   Can escape from local optimum
   # - Mix of local and global search \to Generalized pattern search
   # - Using modelization get get knowledge about the search space and
   #   to predict behavior
   #   - Learning machine \to categorisation of similar problem to use
   #     same strategy, training overhead
   #   - Regression \to possible to use property of the function, such as
   #     derivative, convexity,etc...

   - 3 complementary approaches:
     - "Direct search" \to heuristics random, algo gen, etc...
       - Exhaustive search \to Atlas Linear search, know where to search
         \to need to know the problem well.
       - Orio \to source to source annotation based atutotuner 
         - random search, Nelder Mead Simplex and simulated annealing.
         - greeding algorithm for local search at the of gobal.
       - OPAL \to Use direct search combinations of heuristics \to
         Mesh-adaptive direct-search \to pattern search.
         Global *and* local search \to work by iterative phase:
         - Sampling the space \to finding region of interest
         - Refining the solution

     - Reuse knowledge of previous experience \to machine learning
       - classification \to which strategy to apply
         - Opentuner \to which optimization technics for a given problem
           because the efficiency of a technics depends on the
           structure of the problem.
         - Milepost GCC \to learning characteristics of a program to
           predict what are the good combinations, optimization
           across programs. Predict good configuration using the
           distribution of good combination by taking the mode.
           Reuse knowledge across programs
         - Stefan Wild \to Learning combination across platform
           Worked for similar platforms. Search space pruning \to random
           search.
           Reuse knowledge across platforms
       - Generally exhaustive search costly training phase \to reducing impact
         - Incremental training \to Nitro using active learning
         - Collective tuning \to crowdtuning, Milepost
           Models stored in a common database and continuously updated.

     - Getting knowledge on the fly \to regression, interpolation
       - Brewer \to linear regression for the modelization to predict
         objective function and root finding  or kind of greedy
         descent for the optimization.
         Find correct model automatically
         Not recent paper \to architecture have evolved. Is linear
         regression still ok?
       - ASK \to Emphasis on the sampling because important for the
         accuracy of the model \to complex sampling pipeline
         with different surrogate methods( bayesian regression,
         interpolation, etc... ) 
        
 
# *** Using information about the problem - Objective function
#     - Derivative methods \to local strategy
#       - If non convex \to multiple local minimum \to need to know where to
#         start or randomized strategy e.g. simulated annealing
#       - If derivation not possible (empirical function) estimate with regression
# *** Using information about the problem - Other kind of knowledge   
#     Problem too complex
#     Heuristic based: genetic algorithm, random search, pattern search
#     Also machine learning \to identifying category of problem and
#     strategy that work well

** Methods and material
  - Reproducible work
    - Lab book on github  
    - Literate programming 
    - org mode
  - Result validation against bruteforce
  - Comparison with random, gradiant search, and genetic algorithm
** Contribution
*** Case study
    # Maybe this should go in experiments
****  Laplacian
      - Optimizations explanation
        - Vectorization \to vector length
        - Synthetize loading \to load overlap
        - Tilling \to y component number
        - Number of threads \to elements number
        - Size of temporary results \to temporary size
          Reducing pressure on registers? If high usage of registers?
          If not high usage of registers overhead of casting?
        - Size of a work group \to threads number
        - Shape of work group \to lws y
      - 23100 combinations
      - Minimization
**** Matrix product?
      - Optimizations explanation
*** The reason why linear regression is not suited
    - Assumptions:
      - homoscedasticity but pb we have heteroscedasticity
        - Why is it a problem?
          - Unbiased coefficient estimate but biased std error and thus
            R-squared \to more difficult know if a model is correct
          - But it is still ok if the error law is the same everywhere
      - But we don't know anything about the noise and normal
        distribution of the noise is assumed. We cannot do anything
        about that because in our case the noise come from complex
        interactions between parameters.
        Possible to reduce it by fixing values but it is not always
        possible to do that e.g. if for all the parameters the noise
        falls the same law. But we still have some difficult to find
        model due to the other parameters.        
    - Tracks general tendency of the impact of factors
    - 2 cases:
      - heteroscedasticity + same error law \to minimum can be predict
      - heteroscedasticity + different error law \to minimum and mean
        uncorrelated \to minimum can not be predict
*** The choise of quantile regression
    - Interested in extremal values \to minimum
    - Ways of computing quantile regression
      - empirical quantiles \to linear regression on a quantile
      - Least absolute values
      - Iterated weighted least squares 
        - But optimist R-squared
        - Don't know how to interpret the standard error
    - 5th and 95th percentile \to good estimation for extreme values
*** Model choice and refinement
    - Hypothesis based on the kernel
      The expert knows his kernel and have hypothesis of how the
      optimization will influence the performances.
    - Test parameters independently and remove useless ones. 
    - Iterative refinement \to try to find the interactions.
    - Determines the quality of the prediction using the R-squared.
*** Importance of the search space expression
    - Easier modelization
    - Better capture of the search space features
*** Using less point as possible
    - Design of experiment
      - Random
      - Screenning design
      - LHS
      - D-optimal
    - Copying with constraints
** Experiments
   - Bench min of 4 runs \to warm up effect
*** Laplacian
**** Search space characteristics
     - Qualitative observation in term of speed up
**** Comparison with random and genetic algo
** Future work
   - Find more suited design of experiments technics
   - Validate approach on more complex kernel and different
     architectures
   - Automatization
** Conclusion

* Introduction
* Problem analysis
* State of the art
* Methods
  In order for this work to be usefull for someone else a laboratory
  book is available publicly on
  github\footnote{https://github.com/swhatelse/M2\_internship}. It
  contains detailed about installation and configuration steps. It
  keeps tracks of every experiments including their description and
  analysis. It is structure in a chronological way and thus follows
  the natural evolution of the work. This gives the possibility to
  easily understand what have been done at each step and why.
  Every pieces of codes is explained using literate programming which
  is very straight forward using the org-mode of emacs.
  The github repository also contains the complet set of scripts and
  data used for experiments giving the possibility to anyone to re-run
  the same experiments using the same data.

  The experiments are run on one machine with GPU Nvidia K40 using the
  driver 340.32 and two CPUs Intel E5-2630.
* Contribution
** Case study
   # Not sure it is necessary:
   # #+BEGIN_LaTeX
   # \lstset{language=C}
   # \begin{lstlisting}
   #   void math(const int32_t width, const int32_t height, const uint8_t * psrc, uint8_t * pdst){
   #       int32_t i;
   #       int32_t j;
   #       int32_t c;
   #       int32_t tmp;
   #       int32_t w;
   #       w = (width) * (3);
   #       for (j = 1; j <= height - (2); j += 1) {
   #           for (i = 1; i <= width - (2); i += 1) {
   #               for (c = 0; c <= 2; c += 1) {
   #                   tmp =  - (psrc[c + (3) * (i - (1) + (width) * (j - (1)))]) 
   #                          - (psrc[c + (3) * (i + (width) * (j - (1)))]) 
   #                          - (psrc[c + (3) * (i + 1 + (width) * (j - (1)))]) 
   #                          - (psrc[c + (3) * (i - (1) + (width) * (j))]) 
   #                          + (psrc[c + (3) * (i + (width) * (j))]) * (9) 
   #                          - (psrc[c + (3) * (i + 1 + (width) * (j))]) 
   #                          - (psrc[c + (3) * (i - (1) + (width) * (j + 1))]) 
   #                          - (psrc[c + (3) * (i + (width) * (j + 1))]) 
   #                          - (psrc[c + (3) * (i + 1 + (width) * (j + 1))]);
   #                   pdst[c + (3) * (i + (width) * (j))] = (tmp < 0 ? 0 : (tmp > 255 ? 255 : tmp));
   #               }
   #           }
   #       }
   #   }
   # \end{lstlisting}
   # #+END_LaTeX
   
   # Maybe cite Brice paper for this part
   
   In order to elaborate our approach, we took a very simple example
   which is a kernel that computes the Laplacian of an image. There
   are multiple optimization that can be done to enhance the
   performance of this kernel. 

   The first optimization we can use is the vectorization, this allows
   to take advantage of hardware capable of executing one instruction
   on multiple data at a time and instead of computing one data, so
   multiple data are computed for the same cost. Thus we can specify
   the length of the vector and we must find what is the correct
   length of the vector. 

   To perform vectorization we need to load more data and some data
   overlap with each other, to reduce the number of load we can
   synthetize those data from other, this is the second optimization
   we can have. 

   Another optimization to henance the performs of the kernel can be
   to use smaller type for intermediary results, reducing the pressure
   on the registers.

   We also can determine the number of threads use to performs the
   computation. More threads can lead to better parallelism but also
   more threads overhead. We do this by specifying the number of
   component a thread will work on. We need know what is the correct
   size of the job for a thread.
   
   After specifying the quantity of work per thread we can specify how
   this work is organized by specifying the tilling. It gives how the
   components are distributed in the y axis.

   There are also two parameters that are important for any
   kernel. First we have the number of threads in work group and then
   the organization of the threads in the work group. These parameters
   defines the work distribution at coarse grain and have an impact on
   the threads scheduling, data sharing. This leads to better usage of
   the resources and it worth to tune it carefully.

   All theses optimizations give us search space of 23100 combinations
   to minimize the time to compute one pixel.
** The reason why linear regression is not suited
   #+begin_src sh :results output :exports none
       ruby ../../../scripts/format_data.rb ../../../data/2016_03_11/pilipili2/19_13_54/Data19_13_54_linear.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file img/lm.png :exports results :width 800 :height 400 :session 
     library(ggplot2)
     library(plyr)
     library(gridExtra)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     err_x_comp = ddply(df,c("x_component_number"), summarize,
                        mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))


     err_v_len = ddply(df,c("vector_length"), summarize,
                       mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))

     p1 <- qplot(df$vector_length, df$time_per_pixel) + 
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_errorbar(data=err_v_len,aes(x=vector_length,y=mean, ymin=mean-err, ymax=mean+err),colour="red") +
         ggtitle("Impact of the vector length") +
         labs(y="time per pixel in seconds", x="vector length") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))

     p2 <- qplot(df$x_component_number, df$time_per_pixel) + 
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_errorbar(data=err_x_comp,aes(x=x_component_number,y=mean, ymin=mean-err, ymax=mean+err),colour="red") +
         ggtitle("Impact of number of component on the x-axis") +
         labs(y="time per pixel in seconds", x="x component number") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))

     grid.arrange(p1, p2, ncol=2, top="") 

   #+end_src
   
   #+CAPTION: Linear regression and non-uniform noise
   #+LABEL: fig:lm-1
   #+RESULTS:
   [[file:img/lm.png]]
   
   Linear regression has already been used successfully for
   auto-tuning problems\cite{Brewer:1995:HOV:209937.209946}. But they
   have been put aside for no real reasons to our knowledge. Using
   this method to study the impact of the parameters with using linear
   models to approximate the behavior of the search space coupled with
   efficient sampling strategies seemed very interesting to us.
   
   If linear regression have been efficient in brewer's
   work\cite{Brewer:1995:HOV:209937.209946} it is maybe because at
   this time the architecture of computers was less complicated than
   today. The figure\ref{fig:lm-1} shows clearly the limit of the
   linear regression on the simple case such as a Laplacian kernel on
   nowadays architectures. First, one of the assumptions of the linear
   regression is homoscedasticity of the data which is not often
   necessarily the case, and in our example we can see that the
   variability is not the same at each factor level.

   Heteroscedasticity is problematic because the least square is not
   the Best Linear Unbiased Estimator in this case and it biases the
   variance  and thus the coefficient of determination which makes it   
   more difficult to evaluate the accuracy of the model.

   If the error law is the same everywhere as in the left in
   figure\ref{fig:lm-1} we can still have the minimum values that
   follow the same evolution as the mean and we can still predict the
   minimum. The resulting model and approximation can still be correct
   and we can easily know what is the best size for the length of the
   vector. But we would need to make assumptions that about the 
   error and we do not know anything about the error. In the right in
   figure\ref{fig:lm-1}, the evolution of the mean and the evolution
   of the minimum is not correlated and the best value is mispredicted.  

   We conclude that in the case of heteroscedasticity and non-uniform
   error law, linear regression tracks the general tendency of impact
   of the parameters. But in our case in which we are interested about
   the minimum which is uncorrelated to the mean, the linear
   regression cannot lead to the global optimum and we need another
   estimator for the minimum. 

** The choise of quantile regression
   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data.rb ../../../data/2016_03_11/pilipili2/19_13_54/Data19_13_54_linear.yaml
   #+end_src

   #+begin_src R :results output graphics :file img/why_we_choose_quantile_reg.png :exports results :width 600 :height 400 :session
     library(ggplot2)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     ggplot(df) + 
         aes(x=x_component_number, y=time_per_pixel) +
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_smooth(method="lm", formula = y ~ x + I(1/x), aes(colour="linear regression")) +           
         stat_quantile(quantiles=0.05, formula = y ~ x + I(1/x), aes(colour="quantile regression")) +
         ggtitle("Impact of number of component on the x-axis") +
         labs(y="time per pixel in seconds", x="x component number") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))

   #+end_src

   #+CAPTION: Linear regression vs quantile regression
   #+LABEL: fig:qr-example
   #+RESULTS:
   [[file:img/why_we_choose_quantile_reg.png]]

   In our case 
* Experiments
* Future work
* Conclusion
#+LaTeX: \nocite{*}
#+LaTeX: \def\raggedright{}
\bibliographystyle{IEEEtran}
\bibliography{../../biblio.bib}


* Emacs Setup 							   :noexport:
  This document has local variables in its postembule, which should
  allow Org-mode to work seamlessly without any setup. If you're
  uncomfortable using such variables, you can safely ignore them at
  startup. Exporting may require that you copy them in your .emacs.

# Local Variables:
# eval:    (require 'org-install)
# eval:    (org-babel-do-load-languages 'org-babel-load-languages '( (sh . t) (R . t) (perl . t) (ditaa . t) ))
# eval:    (setq org-confirm-babel-evaluate nil)
# eval:    (unless (boundp 'org-latex-classes) (setq org-latex-classes nil))
# eval:    (add-to-list 'org-latex-classes '("memoir" "\\documentclass[smallextended]{memoir} \n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n  \\usepackage{graphicx}\n  \\usepackage{hyperref}"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("acm-proc-article-sp" "\\documentclass{acm_proc_article-sp}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (setq org-alphabetical-lists t)
# eval:    (setq org-src-fontify-natively t)
# eval:   (setq org-export-babel-evaluate nil)
# eval:   (setq ispell-local-dictionary "english")
# eval:   (eval (flyspell-mode t))
# eval:    (setq org-latex-listings 'minted)
# eval:    (setq org-latex-minted-options '(("bgcolor" "white") ("style" "tango") ("numbers" "left") ("numbersep" "5pt")))
# End:

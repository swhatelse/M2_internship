#+TITLE: 
#+LANGUAGE: en
#+Author: 
#+TAGS: noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LaTeX_CLASS: memoir
#+LaTeX_CLASS_OPTIONS: [12pt, a4paper]
#+OPTIONS: H:5 title:nil author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil
#+BABEL: :session *R* :cache yes :results output graphics :exports both :tangle yes 

#+LATEX_HEADER:\usepackage[french,english]{babel}
#+LATEX_HEADER:\usepackage [vscale=0.76,includehead]{geometry}                % See geometry.pdf to learn the layout options. There are lots.
# #+LATEX_HEADER:\geometry{a4paper}                   % ... or a4paper or a5paper or ... 
# #+LATEX_HEADER:\geometry{landscape}                % Activate for for rotated page geometry
# #+LATEX_HEADER:\OnehalfSpacing
# #+LATEX_HEADER: \setSingleSpace{1.05}
# #+LATEX_HEADER:\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
#+LATEX_HEADER:\usepackage{amsmath}
#+LATEX_HEADER:\usepackage{fullpage}
#+LATEX_HEADER:\usepackage{mathptmx} % font = times
#+LATEX_HEADER:\usepackage{helvet} % font sf = helvetica
#+LATEX_HEADER:\usepackage[latin1]{inputenc}
#+LATEX_HEADER:\usepackage{relsize}
#+LATEX_HEADER:\usepackage{listings}

#+BEGIN_LaTeX
%Style des têtes de section, headings, chapitre
\headstyles{komalike}
\nouppercaseheads
\chapterstyle{dash}
\makeevenhead{headings}{\sffamily\thepage}{}{\sffamily\leftmark} 
\makeoddhead{headings}{\sffamily\rightmark}{}{\sffamily\thepage}
\makeoddfoot{plain}{}{}{} % Pages chapitre. 
\makeheadrule{headings}{\textwidth}{\normalrulethickness}
%\renewcommand{\leftmark}{\thechapter ---}
\renewcommand{\chaptername}{\relax}
\renewcommand{\chaptitlefont}{ \sffamily\bfseries \LARGE}
\renewcommand{\chapnumfont}{ \sffamily\bfseries \LARGE}
\setsecnumdepth{subsection}


% Title page formatting -- do not change!
\pretitle{\HUGE\sffamily \bfseries\begin{center}} 
\posttitle{\end{center}}
\preauthor{\LARGE  \sffamily \bfseries\begin{center}}
\postauthor{\par\end{center}}

\newcommand{\jury}[1]{% 
\gdef\juryB{#1}} 
\newcommand{\juryB}{} 
\newcommand{\session}[1]{% 
\gdef\sessionB{#1}} 
\newcommand{\sessionB}{} 
\newcommand{\option}[1]{% 
\gdef\optionB{#1}} 
\newcommand{\optionB}{} 

\renewcommand{\maketitlehookd}{% 
\vfill{}  \large\par\noindent  
\begin{center}\juryB \bigskip\sessionB\end{center}
\vspace{-1.5cm}}
\renewcommand{\maketitlehooka}{% 
\vspace{-1.5cm}\noindent\includegraphics[height=14ex]{logoINP.png}\hfill\raisebox{2ex}{\includegraphics[height=7ex]{logoUJF.jpg}}\\
\bigskip
\begin{center} \large
Master of Science in Informatics at Grenoble \\
Master Math\'ematiques Informatique - sp\'ecialit\'e Informatique \\ 
option \optionB  \end{center}\vfill}
% End of title page formatting

\option{$PDES$}
\title{ Semi-Automatic Performance Optimization of HPC Kernels }%\\\vspace{-1ex}\rule{10ex}{0.5pt} \\sub-title} 
\author{Steven QUINITO MASNADA}
\date{ June 22th } % Delete this line to display the current date
\jury{
Research project performed at $<$lab-name$>$ \\\medskip
Under the supervision of:\\
Arnaud LEGRAND, Frederic DESPREZ, Brice VIDREAU, CNRS\\\medskip
Defended before a jury composed of:\\
Prof Noel DEPALMA\\
Prof Martin HEUSSE\\
Dr Thomas ROPARS\\
Prof Olivier GRUBER\\
}
\session{$June$\hfill 2016}
#+END_LaTeX

#+BEGIN_LaTeX
\selectlanguage{english} % french si rapport en français
\frontmatter
\begin{titlingpage}
\maketitle
\end{titlingpage}

%\small
\setlength{\parskip}{-1pt plus 1pt}

\renewcommand{\abstracttextfont}{\normalfont}
\abstractintoc
\begin{abstract} 
Text 
\end{abstract}
\abstractintoc
\renewcommand\abstractname{R\'esum\'e}
\selectlanguage{english}% french si rapport en français

\cleardoublepage

\tableofcontents* % the asterisk means that the table of contents itself isn't put into the ToC
\normalsize

\mainmatter
\SingleSpace

#+END_LaTeX

# #+BEGIN_abstract
#   Blablabla
#   \newpage
# #+END_abstract

* Plan 
** Introduction [3/3]
*** Why is performance optimization difficult?
   - In HPC code optimization crucial to exploit very complex hardware.
     Cannot wait for the next generation to bring speedup because it
     does not (Frequency not higher but more cores and henanced ISA). 
     - many cores \to heavy parallelism \to need to program parallel
     - pipelining ILP \to 
     - vector support \to SIMD \to need to work with vector
     - cache hierarchies \to need to exploit data locality
     - GPUs! \to different way of programming (than CPU)
   - HPC plaforms have many \ne hardware \to code optimizations not portable.
     Porting application to another platform is time consumming and
     can be very tricky.
   - Many attempts in the last decade to automate the generation of
     optimized code
*** Code generation and opportunities
    - The compiler approach: loop unrolling, vectorization, automatic
      parallelization, loop nest transformation, etc. Yet, many
      opportunities are not exploited as it is too difficult to
      exploit them automatically. Sometimes, the source code has to be
      rewritten in a slightly different way to enable the compiler to
      be effective
    - Parametric optimization:
      - The source-to-source transformation (C to C, Fortran to Fortran,
        ...). Framework for transform code. Orio. Serge
        Guelton. Difficile mais limité à un seul langage, et
        exploitation d'accelérateurs différents difficile. Ça ne se
        permettra jamais de changer le mapping des données en mémoire
      - Meta-programming approach: allow the programmer to propose
        optimizations that the compiler would not be allowed to do
        (because of the language or because it would require information
        on the application that cannot be given to the compiler).
        # But it is also the case with source-to-source transformation
        # right?    
*** Optimizing: the auto-tuning approach
    - Many optimization options: compiler flags, source-to-source
      transformations, higher-level modifications (tile/block/vector
      size). Each combination represents an implementation.      
    - Auto-tuning: consider all this as a huge optimization problem
      and try to find the best combination. Many techniques (genetic
      algorithms, simulated annealing, tabu search, machine learning,
      ...) depending on the problem constraints. But mainly two
      problems:
      - the time needed for such optimization
      - knowing whether further optimizations could be expected or not
        (peak performance is generally useless and the optimization
        process is so complex that it's hard to know how it really
        performed) is difficult and même si tu sais qu'il devrait être
        possible de faire mieux, tu sais pas vraiment où, comment( cf
        of genetic algo on the full search space), ...
*** Our goal
    Many approaches in code generation/transformation. It's possible
    to start from high-level codes (e.g., pytran) but the most
    optimized codes are obtained from specific tools (FFT, BLAS,...).

    We decided to evaluate an intermediate approach by relying on
    BOAST, a metaprogramming... (Semi automatic approach \to gives back
    power to the user, framework ruby generating portable code in C,
    Fortran, OpenCL. DSL) and investigate various statistical
    techniques inspired from the design of expeirments field that
    emphasizes on reducing experiment cost.

    Investigate the design of a semi-automatic optimization framework,
    where the applicaiton developer has control and understands how
    the optimization works and guides it.
*** My contribution
    - Related work on auto-tuning
    - Proposal based on DoE
    - Evaluation
      - Comparison with state of the art
      - Analyze

    - (Complex methods used but no explanation on why they work)
    - Prevent biased measurement
    - Try a simple approach and try to understand it deeply
      - Getting knowledge from the problem to guide the user:
        - Take into account hypothesis \to use the knowledge of the user
          1. Sampling the space
          2. Model find 
             - Removing useless factors
             - Refine the model \to add quadratic terms, 1/x,
               interactions, etc...
          3. Fix parameters to prune the search space and add removed
             factors.
          4. Back to 1 until we are able to fix all parameters values   
          
        - Linear regression methods to model the search space \to
          finding good model based on hypothesis. Allow the user to
          check this hypothesis. And understand the problem.
          - Try OLS \to problem with regression of expectation
            heteroscedacity + non uniform noise
          - Solution \to quantile regression
            - Pb with rq \to error to compute std. err, etc...
            - Used iterated weighted least square 
              Pb to make inferences \to biased R-squared and std. error
              # Are std.err biased to?
              How to compute CI?
              
        - Modeling
          - Start generic \to go specific
            Over specification \to biased
          - Sampling is crucial \to Design of experiments \to reducing number of experiments
            What design of experiment to use?
            - Random
            - LHS
            - Screening
            - D-Optimal
            How to use them? Copying with constraint
            - Start without hypothesis on the model otherwise \to biais
            - Add point with hypothesis \to D-Opt
            
*** Structure of the report
** Context [0/2]
*** HPC/architectures     
    - Crucial for science and business
    - To get performance \to exploit hardware \to take characteristics into account
      - Many cores \to aims low idle time
        Thinking parallel
        Right number of threads \to because overhead in thread
        management.
        Less synchro as possible
      - GPUs \to suited to a certain type of computation \to can bring
        lots of performances.
      - vector support
        Data pipelining
        Share the same instruction on multiple data \to save decoding
        
      - cache levels
      - ILP \to break instruction dependencies
    - Architecture \ne from a HPC to another
      Specialized code \to not portable
*** Obtaining efficient code
**** Compilation
     Il fait ce qu'il peut mais pas de vision globale du code \to local
     optimization (intra procedure) \to because more easier no control
     flow
     - code re-ordering \to instruction scheduling find the best
       sequence for the pipeline \to reduce instruction conflict
       (dependencies between instructions) 
     - Register allocation
     - loop transformation \to parallelization and data locality \to 
       finding parallelism into loops \to loop nest transformation /
       unroll. 
     - Automatic parallelism \to multi-threaded, vectorized 
       Pb with shared/global variable, IO, indirect addressing, etc...
   
     Limited because stuck by semantic rules, not enough information
     at compile time, etc...
     
     Archi compliquée donc dur: Grigori Fursin.
     Sometime the platform the not well supported.
**** Source-to-source transformation (C vers C ou FORTRAN vers FORTRAN)
     - Relieve compiler \to deactivate optimization
       Gives to the compiler the desired optimization
     - Gives more expressiveness \to more information two performs
       transformation \to ensure that the semantic is correct 
     - Present the code correctly to allow the compiler to make his job.
     - orio, PIPS,  cloog 
       Generally annotation-based  
       How is it used
      - pluto (automatic parallelization)
      - pytran
      - auto-tuning on top of orio

     Pros and cons:
     
**** Meta-programming: BOAST
     Less constraint by semantic rules but can be error prone \to not
     correct transformation.
*** Recap
    How to port performances.
** Problem analysis [0/2]
   - Huge search space \to need to explore only part of it \to
     optimization problem.
   - Interactions between parameters
   - Non-smooth and empirical objective function
   - Combination of discrete and continuous parameters
   - Constraint optimizations
     Represent unfeasible points.
        
** State of the art on Autotuning [2/4]
   - What is autotuning
     paramters \to represents different version/implementation

  # Maybe an overview of machine learning in general
  - Reuse knowledge of previous experience (generalization) \to machine
    learning. For different problem \to re-usability. 
    What is machine learning and why it is useful in auto-tuning.
    Generally exhaustive search costly training phase \to
    reducing impact. Classification \to which strategy to apply.
    - Small vs. Big
    - Milepost GCC \to learning characteristics of a program to
      predict what are the good combinations, optimization
      across programs. Predict good configuration using the
      distribution of good combination by taking the mode.
      Reuse knowledge across programs
    - Stefan Wild \to Learning combination across platform
      Worked for similar platforms. Search space pruning \to random
      search.
      Reuse knowledge across platforms
    - Opentuner \to which optimization technics for a given problem
      because the efficiency of a technics depends on the
      structure of the problem.
    - Incremental training \to Nitro using active learning
    - Collective tuning \to crowdtuning, Milepost
      Models stored in a common database and continuously updated.

  Optimization: exhaustive search is unfeasible.

  - "Direct search". The efficiency (ability to find the
    (near)-optimal solution and possibly in the fewest possible
    experiments) depends on the structure of the problem.
    - Main techniques:
      - Gradient descent: ferrari, a priori = local, geometry, convexity.
        - Issues: 
          - partly wrong hypothesis (geometry, convexity): simulated
            annealing, many local searches (genetic algorithms in some
            sense)
          - experimental estimation (empirical function)  :
            surrogates, etc. *local* approximation
            Usefull to remove the noise and facilitate the search
          - derivative estimation: Nelder Mead Simplex
        - \to many heuristics that combine all or part of the different
          previous approaches depending on how much the various
          hypothesis are wrong or not. Their efficiency highly depends
          on these hypothesis.
    - Some people have thus developed framework to characterize the
      optimization space.
      - ASK \to Emphasis on the sampling because important for the
        accuracy of the model \to complex sampling pipeline with
        different surrogate methods( bayesian regression,
        interpolation, etc... ). _Global modeling requires complex
        models and numerous experiments_.
    Illustration with a few tools:
    - Orio \to source to source annotation based autotuner 
      - random search, Nelder Mead Simplex and simulated annealing.
      - greeding algorithm for local search at the end of gobal.
    - OPAL \to Use direct search combinations of heuristics \to
      Mesh-adaptive direct-search \to pattern search.
      Global *and* local search \to work by iterative phase:
      - Sampling the space \to finding region of interest
      - Refining the solution
    - In some cases, the problem structure is known and one has an
      idea of where the optimal solution is but the structure of the
      space in this neighborhood is too complex. Some fall back to
      Exhaustive search \to Atlas Linear search, know where to search \to
      need to know the problem well.

  Primary Goals:
  - semi-automatic, almost interactive ? more global approach where
    the relevance of the hypothesis can be evaluated
  - optimize at low cost, need to prune the search space
  - from previous experience, generalization from an arch to another
    seems very difficult

  Somehow similar approach:
  - Getting knowledge on the fly \to regression, interpolation
    - Brewer \to linear regression for the modelization to predict
      objective function and root finding  or kind of greedy
      descent for the optimization.
      Find correct model automatically on platform CM-5, simulated
      version of Intel Paragon and network of station based on FORE ATM. 
      Not recent paper \to architecture have evolved. Is linear
      regression still ok?
** State of the art design of experiments [1/2]
   - Study phenomenon \to behavior of a system
     - Acting on many factor at a time instead of one
     - Get information on how the factors impact the system and
       interactions \to not possible with OFAT (one factor at a time) \to
       factorial design
     - Identify interaction without trying all range of values.
     - Define explanatory variable.
   - DoE:
     - OFAT
     -Factorial
       - Random
       - LHS
         For continuous space
         Provide Better coverage of the space
       - Fractional design
         Screening design \to Take the extreme values
       - Optimal design
         - D-Optimal
           Require to know the model
           Select points according to a model.
         - I-Optimal
         - A-Optimal
** Methods and material [0/2]
*** Reproducible work
    - Lab book on github  
    - Literate programming 
    - org mode
*** Case study
****  Laplacian
      - OpenCL
      - Optimizations explanation
        - Vectorization \to vector length
        - Synthetize loading \to load overlap \to for memory bound?
        - Tilling \to y component number
        - Number of threads \to elements number
        - Size of temporary results \to temporary size
          Reducing pressure on registers? If high usage of registers?
          If not high usage of registers overhead of casting?
        - Size of a work group \to threads number
        - Shape of work group \to lws y
      - 23100 combinations
      - Minimization
      - Test 5 sizes of images \to mean
**** Experimental protocol  
    - Result validation against bruteforce
    - Comparison with random, gradiant search, and genetic algorithm
    - Bench min of 4 runs \to warm up effect
**** Search space characteristics
     - Qualitative observation in term of speed up
**** Comparison with random and genetic algo

** Contribution [0/22]                                             :noexport:
** Envisioned general approach[0/1]
    1. DoE
       - Sampling the space wisely
       - Use linear regression OLS:
         - remove factors from the model
         - model and optimize
    2. Loop back to 1 to refine the model
** Controlling measurement [0/1]
    - Time per pixel \to total time / number of pixel. Because we test
      different size of image.
    - min(x_1,...,x_10) ? how to protect against potential warm-up
      - Energy saving mode of current hardware(CPU and GPUs)
      - Mostly present just after the compilation of the kernel.
      - 4 runs \to take the minimum
    - randomizing to protect against bias, even for full search
      space. But run and image size not randomized.
** Results [0/2]
    Considering speedup with regard to the best.
    Comparison:
    - GA \to not tuned \to would have take time to tune it
    - Greedy
    - Random
    - LM
      - Full \to evaluate the quality of the model \to maybe in the
        analysis to explain why we don't have the best
      - Uniform
      - LHS
      - D-Opt
    - Rq \to Another way of doing linear regression
      - Full \to evaluate the quality of the model \to maybe in the
        analysis to explain why we don't have the best
      - Uniform
      - LHS
      - D-Opt
   | Histogram of solutions | Cost |

** Analysis [0/14]
*** Characteristics of the search space [0/2]
   - Repartition of good combinations
   - Lot of local optimum \to local search failed
   - Heteroscedasticity
*** Linear regression of expectation: why it cannot work and how it can be circumvented [1/3]
**** Least Squared regression and non uniform noise  
    - Assumptions:
      - homoscedasticity but pb we have heteroscedasticity
        - Why is it a problem?
          - Unbiased coefficient estimate but biased std error and thus
            R-squared \to more difficult know if a model is correct
          - But it is still ok if the error law is the same everywhere
      - But we don't know anything about the noise and normal
        distribution of the noise is assumed. We cannot do anything
        about that because in our case the noise come from complex
        interactions between parameters.
        Possible to reduce it by fixing values but it is not always
        possible to do that e.g. if for all the parameters the noise
        falls the same law. But we still have some difficult to find
        model due to the other parameters.        
    - Tracks general tendency of the impact of factors
    - 2 cases:
      - heteroscedasticity + same error law \to minimum can be predict
      - heteroscedasticity + different error law \to minimum and mean
        uncorrelated \to minimum can not be predict
**** Using quantile regression
     - Interested in extremal values \to minimum
       - 5th and 95th percentile \to good estimation for extreme values
     - Ways of computing quantile regression
       - empirical quantiles \to linear regression on a quantile
       - Least absolute values
       - Iterated weighted least squares 
         - But optimist R-squared
         - Don't know how to interpret the standard error
*** Model choice and refinement [0/2]
    - Hypothesis based on the kernel
      The expert knows his kernel and have hypothesis of how the
      optimization will influence the performances.
      - Explanation of the impact of the parameters \to justification of
        the model \to hypothesis
        - elements_number
        - y_component_number
        - etc...
    - Hypothesis testing:
      - Try \ne hypothesis
        - First start to eliminate factor that have no impact
        - Remove then from the model
        - Try to find interactions
      - Keep the more accurate and the simplest

    - Test parameters independently and remove useless ones. 
    - Iterative refinement \to try to find the interactions.
    - Determines the quality of the prediction
      - We cannot use R-squared \to biaised because of the iterative
        approach.
      - Visual checking \to yek! How can I do visualization on more than
        3D? I can not make regression for each factor because it's not
        the same than one regression including all the factors. But we
        could optimize each parameters independently.
      
*** Model Optimization [0/2]
   - Non-convex optimization 
     Constraint \to unfeasible points
     Barrier approach
   - Exhaustive search
*** Importance of the search space expression [0/1]              :deprecated:
    # Will see if I have more time to dig the subject
    - Easier modelization
    - Better capture of the search space features
*** Using as little points as possible [0/4]
    - Design of experiment
      - Random
      - Screenning design
        Not suitable for constrained search space \to lot of point cannot
        be reached because test those at the border. Constraints have
        to be expressed in the objective function
      - LHS
        Good starting point \to no hypothesis point are choosen
        uniformly but more wisely than a random sampling.
        Generally for continuous factors \to convert to discrete \to is it
        still wiser than random? 
      - D-optimal
        Can be used to find the model but use it careful \to no
        hypothesis at the begining otherwise it introduces some biais.
        it selects points that
        explain the model \to there many possible models, it depends
        which points are choosen.
        Usefull to make refinement \to when the model is already known.
    - Copying with constraints
** Future work [0/2]
   - Constraints
   - Find more suited design of experiments techniques
   - Validate approach on more complex kernel and different
     architectures
   - Automatization
** Conclusion [0/2]
   And finally I saved the world...

* TODO Introduction
** Why is performance optimization difficult?
  From genome sequencing to molecular dynamics, including climate or
  earthquake modeling, or aerodynamics, there is an ever increasing
  need for computer power. For this purpose, High Performance Computing (HPC) is
  the most effective solution. It has brought the science to another
  level and now it is a tool that has became essential for scientists like
  for example to simulate nuclear explosion or to analyze peta-octets of
  data. The expectations of scientists in term of performances are
  higher and higher as they need to run more and more heavy and complex
  computations. To take advantage of the power of a supercomputer it is
  essential to correctly optimize the applications. This is a very
  complicated task because today's HPCs are extremely complex
  machines. It is not possible to wait for the next generation of
  hardware to bring automatically a speedup as 
  it was the case up to the years 2000 because frequency cannot
  increase anymore and in contrary even tends to decrease. For this
  reason, we went from multi-cores to many-cores architectures and 
  for 2020 exascale platforms, supercomputers with millions of cores,
  are expected in order to reach the exaflops. Thus, developers have
  to take into account this massive parallelism when writing
  programs. Furthermore, they also have to take care about things such
  the dependencies of the instructions to fully occupy the pipeline. If
  there is any vector support the developer should adapt his code to work on
  vectors instead of single variables. In addition the architecture provides
  different cache hierarchy and it is crucial to exploit data locality
  to use them efficiently. 
  Finally to address the computer power need, GPUs have become very
  popular. Unfortunately, this add a little more complexity as they
  require a way of programming which is different from the CPUs. As a
  result, performance optimization is difficult to achieve. To build
  efficient HPC platforms, architects have to come up with unique
  combinations of a variety of hardware, which complicates the
  application optimization. Hence one end up with optimization working
  well on one supercomputer and bad on another one as the code is
  specific to one platform and porting applications is extremely time
  consuming and can also be very tricky.  
** Code generation and opportunities  
  In the last decade many attempt have been made to automate the
  generation of optimized code. Performing optimization is one of the
  primary functions of compiler. They are capable of detecting
  instructions that can be vectorized or parallelized. They 
  are also capable of many loop optimizations such  loop unrolling,
  nest transformation, etc... Yet it exists many
  other opportunities to perform optimizations but it is to difficult
  to exploit them automatically because the compiler does not have the
  necessary information at compile time. Moreover, it is sometimes necessary
  to rewrite the code in a slightly different way to enable the
  compiler to be effective. That is why frameworks such
  Orio\cite{Hartono:2009:AEP:1586640.1587666} 
  for source-to-source transformation have been developed. This
  approach generally use annotations to describe the optimizations. It
  allows to bring user's knowledge in the process of generation of an
  optimized code. The drawbacks are that the it is restricted to one
  language because the input and output languages are the same and it
  is difficult to exploit different accelerators. Also it does not
  allow operations that change the memory mapping such transposing a
  matrix. The meta-programming approach goes further by giving more
  flexibility to the programmer as it provides a higher level of
  abstraction. It consists in using high level languages to
  descriptions the computation and the optimizations. This allow the
  programmer to propose optimizations that the compiler would not be
  allowed to do. But it requires to rewrite the application.  
** Optimizing: the auto-tuning approach
  The problem is, usually there are many optimization options, there
  are the compiler flags, code generation parameters (e.g. the size of
  the a tile, block or vector). Each combinations of parameters is a
  generated implementation of a program and the auto-tuning consider
  all this as a huge optimization problem and try to find the best
  combination of parameters. The search space can be huge, and the
  exhaustive search is prohibitive. Hence many techniques have been
  used such genetic algorithm, simulated annealing, tabu search,
  machine learning. But these kind of methods have some
  limitations. First the number of combination tested is not optimal,
  thus the time to perform the optimization can still be very long. In
  addition to this, it is difficult to know whether further
  optimizations could be expected or not and how to get them. Because
  it is complicated to estimate the quality of an
  optimization. Comparing to the peak performance is generally 
  meaningless and it is hard to know how the combination really
  performed because the best optimization is unknown. As a result the
  user is exclude from the tuning process by the lack of 
  feed back and any valuable information.
** Our Goal
   The idea is to give some power back to the user by investigating
   the design of semi-automatic optimization framework, where the
   application developer has control and understands how the 
   optimizations works and guides it. For this, we relied on
   BOAST\cite{}, a metaprogramming framework in ruby that can
   generate portable code in C, Fortran and OpenCL. It provides a
   domain specific language to describe the kernel and the
   optimizations and embeds a complete chain of tools to compile, run,
   benchmark and check the validity of a kernel. We investigate
   various statistical techniques inspired from the design of
   experiments that emphasizes on reducing experiment cost.
** My contribution
   My contribution during this internship was to try an approach 
   that take into account the hypothesis the developer is doing to
   make a model of the impact of the parameters in order to guide the
   user in the tuning process. More precisely we investigated if
   linear regression and design experiments could bring accurate
   information using the least point as possible.  

   Our approach consists in the following steps:
   1. Explore the search space at very specific place
   2. Find the more accurate and simplest model by refinement and
      removing useless factors
   3. Fixing parameters to prune the search space and add removed factors
   4. Back to 1 until we are able to fix all the factors values.
   
   In the first time, we wanted to see if the linear regression was
   suited to modeling the problem of code optimization and if we could
   achieve to have a model that is simple and close to the reality. 
   # To model 
   # computer phenomena, linear models are generally enough to get
   # accurate prediction because the models are not too 
   # complex. 
   We tested this approach on a simple kernel that compute 
   the Laplacian of an image. 
   # We found that the linear regression is
   # able to be accurate enough while having simple models that traduce
   # how the different optimization parameters can acts. However we also
   # figured out regression of expectation is not suited with current
   # architectures as it was the case two decades ago\cite{}. Regression
   # of expectation suppose that our data are homoscedastics and follows
   # the same error law. There are no guaranty about it, thus there are
   # cases where the minimum does not follow the same evolution as the
   # mean. As we are interested at the minimum value the regression of
   # expectation cannot  be used to model the evolution of the minimum
   # when the data are heteroscedastics and do not follow the same error
   # law. Hence to circumvent this burden, quantile regression seemed
   # more suited, and we tried to use it in our initial approach. 
   # Quantile regression created some additional difficulties compared
   # to the standard linear regression. We use the iterated weighted
   # least squared to compute it. Even if the coefficient computed are
   # accurate, the main concern with this methods is that it is
   # difficult to make inferences because we have biased 
   # R-squared and standard error. This was mainly problematic for the
   # validation and refinement of the model.

   In the second time, we try find a way reduce the number of points
   need for the model and yet being accurate. To do so, sampling the
   search space correctly is crucial, that is why, we investigated to
   find how we can use efficiently techniques inspired from design of
   experiments.  

   # One important point to find correct model is that the model and the
   # sampling should start with the least underlying hypothesis as
   # possible because over-specification could induce some biais. The
   # idea is to start with generic model and strategy such a LHS design
   # and when we have some certainty about the model, try more specific
   # sample by adding point with a D-Optimal design.

** Structure  of the report
   The second part of this report exposes context of this work. The
   third part describes the problem of the optimization auto-tuning
   problem. The fourt part presents the state of the in auto-tuning
   and design of experiments. The fifth part exposes the how this
   work was made. The sixth part explains the approach we used. 
   The seventh part explains how measurements was made. The eigth part
   shows the results we manage to have compared to other
   techniques. The nineth part provide a detailed analysis of the
   results. And finally the tenth part show what can be done to
   improve our process and results.
* TODO Context
** HPC architectures
  HPCs are complex machines and it is not straightforward to use them
  correctly. Indeed with a not carefully tuned code it is likely to
  have poor performances. Optimizing the code correctly by taking into
  account the characteristic can bring major speedup and increasing
  the performance x10 is not rare. The current trend in HPCs is to
  have CPUs with an ever increasing amount of cores and a tendency to
  reduce the frequency in order to reduce the power consumption and
  the heat. Thus to get performances it is mandatory to exploit
  correctly the parallelism of the platform. The computation has to be
  described in a parallel way. Traducing directly a sequential
  application into a parallel one generally bring poor
  performances. Hence, the developer has to define which are the parts that
  can be performed in parallel and how they are parallelized. The code
  has to be written in a way such the work is distributed among all
  the cores available and keep them busy when I/Os occur to have the
  less possible cores idle. It is important to use the correct amount
  threads. Too many threads can bring more overhead due to the
  management of the threads. Too little and all the cores are not
  exploited correctly. Also the more the threads are independent from
  each other, the better, which means there should be less
  synchronization as possible.  

  Pipelining is another kind of parallelism in which the treatment of
  instructions is split into a sequence of steps (fetch, decode,
  execute, etc...) and goes through a pipeline. Multiple instructions
  can be in the pipeline at the same time but only at different state
  of the processing, like in an assembly line. A correct scheduling of
  the instructions in the pipeline leads to a better occupancy of
  it. Instruction Level Parallelism is a mechanism that can change the
  order of the instructions to have a better overlapping of the
  instructions in the pipeline. In addition some CPUs have vector
  support. Such processors can manipulate not only scalar variables
  but also vector variables. The vector is loaded into a vector
  register and the same instruction is applied on the entire. This the
  save cost of decoding the same instruction multiple times. 

  Today, CPUs have become so fast that one of the main bottleneck is
  the memory. Thus accessing data in memory is much more expensive than
  performing computations. To deal with this problem the solution
  found is to use different hierarchy of memory. The statement is that
  a data that is currently used is more likely to be re-used in a near
  future. Hence the idea is to keep that are the most frequently used
  data as close as possible to the CPUs, that is why CPUs embeds cache
  memories. In a processors there can be up to 4 level of cache
  (the registers, L1, L2, L3) and the fastest are the closest to the
  CPU to have the lowest latency possible but they also are the smallest.
  Thus the pattern to data access have to be chosen carefully so that
  the most used stay close to the computation units.

  Another recent characteristic of HPCs is the increasing use of
  GPUs because for computation that can be well parallelized like it
  is the case generally the with case with scientific computation they
  are faster than GPUs. However GPUs do not work exactlty the same as
  CPUs and need to be programmed in a slighly different manner, the
  cache and number of cores are different.
  # the architectures of the GPUs is
  # different from the one of the CPUs, the amount of cache memory is
  # limited and there are more compute units (for example the Nvidia
  # Tesla K40 has 2,880 cores). 
  # Thus optimizations that bring good
  # performances on CPUs may bring poor performances on GPUs such the
  # size of the vectors or the number of threads.

   #+BEGIN_LaTeX
   \begin{figure}[tbh]
   \centering
   \includegraphics[width=.8\linewidth]{./img/performance_platform_correlation.jpg}
   \caption{\label{fig:correlation}Performances correlation accross platforms}
   \end{figure}
   #+END_LaTeX

  As we saw, optimizing code for HPC applications can be very
  chalenging but porting applications accross platforms is even
  harder and highly time consuming as the optimization are low level
  and HPCs can be very different and complex. Optimization that gives
  good performances on one platform may not work so well on another
  \ref{fig:correlation}. As developers cannot spend months to port the
  application on another machine it is necessary to use tools that
  facilitate the porting and the optimization of scientific
  applications.  

** Obtaining efficient code
*** Compilation
    Many works have been made around compilers to optimize the code
    automatically. They are able to modify the order of the
    instructions to find better sequences to maximize the occupancy
    of the pipeline. In addition, automatic parallelism techniques
    are able to find sequential code that can be vectorized or
    multi-threaded. They can also perform loop transformation to
    reduce the overhead of loops, henance data locality and facilitate
    the parallelization using loop unroll / nest transformation
    techniques. But this require to still write the code with care to
    ease the job of the compiler. For instance automatic parallelism
    is difficult to apply when there are global or shared variable,
    indirect address are used, etc... Further more, compilers generally
    do not have global vision of the code and lack informations of
    compile time and thus perform only local optimization. In
    addition, they also can be limited by semantic rules. As a result,
    they are not able to evaluate which transformation to choose among
    all correct transformations and they just take the one that is
    semantically correct.
*** Source-to-source transformation
    Source-to-source transformation frameworks ease the task of both
    the developer and the compiler by taking a source code, working on
    Abstract Syntax Trees and applying transformation such automatic
    parallelization to generate a modified version of the original
    code. Unlike with compilers, the developer can specify how he
    wants the transformation to be done, for instance how many time
    the loop is unrolled. Then the framework ensures that the
    transformation is valid and generates a code that the compiler can
    easily work with. This relieve the compiler from the complicated
    tasks such the loop transform or the automatic parallelization and
    this gives the possibity the user guide the transformation by
    giving more information. The disadvantage of such tools it that
    they generally target one language and one compiler and can be
    still limited by semantic rules.  
*** Meta-programming: BOAST
    Meta-programming is a slightly different approach from
    source-to-source transformation in which the developer use high
    level language to make a description of his kernel and the
    possible optimizations (e.g. the size of a vector, the tiling,
    etc...). The advantage is that it is not linked to one output
    language or compiler. It also gives more control to the user as
    there is no checker that verify the correctness of the
    transformation, thus he can exactly specify how the transformation
    is performed. Hence, the developer has to know what he is doing
    and it can be error prone. In this work we used the
    meta-programming framework BOAST\cite{}. BOAST gives the ability
    to user to meta-program his kernels in ruby with a Description
    Specific Language (DSL), then BOAST can generate it in many target
    languages (C, Fortran, Cuda, OpenCL), compilate it and benchmark
    the resulting executable.  
** Summary
  In a word, optimizing HPC applications is tricky and porting even is
  more difficult but tools exist to assist the developer in this
  complicated task. However a major problem remains, generally the
  developer know what should be vectorized or what should be
  parallelized but he does not know what is the best size of the 
  vector or the best number of threads or what is the combination of
  compilation flags that brings the best speedup. This problem consist
  in tuning correctly the different optimization parameters of the
  applications. 

* TODO Problem analysis
  The tuning of applications is a non-trivial problem, because the
  search space of the different combinations of parameters can be
  huge. For instance there are about 500 compilation flags for GCC and
  testing all the combinations to find the best one, even if this
  process is automated (auto-tuning). Thus it is formulated as a mathematical
  optimzation problem where the optimization function gives metrics
  of combination of parameters \vec{x}. 
  
  #+BEGIN_LaTeX
  \( \displaystyle\min_{x} {f(\vec{x}): \vec{x} \in \mathcal{D} \subset \!R^{n} } \)
  #+END_LaTeX  

  This function is empirical because the performances of a
  combination cannot be computed, measurements have to be done to
  evaluate the objective function at point x. It needs to generate the
  code variant, compile it and run it. Sometimes the problem can have
  constraints because some points are unfeasible, this means they
  cause the compilation to fail or the program to crash. In addition
  parameters can be discrete and continuous. 
 
* TODO State of the art
** Auto-tuning
   In auto-tuning one can find two major categories of
   approaches. Some has focus on the is of machine learning techniques
   Machine learning is used to build models over a large training set to make
   predictions. Thus, there is a will of generalization, the knowledge  
   is reuse from previous experiences. It is used to identify category
   of programs that have the same characteristics, and to determine
   what is the best action to apply for this category of programs.

   This approach has been proven successful by the project Milepost
   GCC from Grigori Fursin\cite{fursin:hal-00685276}, which is now part of GCC. He used
   machine learning to learn characteristics of programs and the
   distributions of combinations that gives the most speedup. The idea
   is that good performing combinations have high probability to bring
   good speedup for similar programs. This allowed to reuse knowledge
   across programs.
   
   Stefan Wild et al. focused porting of optimization between similar
   platforms\cite{RoyBalHovWil2015}. They study the correlation between platform and
   the performance of combinations parameters. They used machine
   learning to build performance model of platform and this model
   to approximate performance of another platform. The more the
   combinations performance are correlated between two platforms the
   more the accurate the predictions. They managed to find correlations
   between intel CPU, IBM Power but this approach fails with too
   dissimilar platforms (ARM in their case).

   As efficiency of a search strategy is dependent on the structure of
   the search, machine learning can be used to learn what search
   methods to use according to the characteristics of the search
   space. That is the approach taken by the auto-tuning framework
   Opentuner\cite{Ansel:2014:OEF:2628071.2628092}.  

   The main drawback with machine learning techniques is that they
   need to be trained on a large amount of instances to be effective
   enough. To mitigate this problem, some, such the framework
   Nitro\cite{Muralidharan:2014:NFA:2650283.2650550} uses active
   learning to distribute the training overhead.

   Another approach is to distribute the training overhead over the
   different users, it is called
   crowdtuning\cite{memon:hal-00944513}. Informations are collected in
   a shared database and machine learning is applied to continuously
   update the model. 
   
   Other have worked more around the optimization side to find more
   suited search techniques that are able to find the near-optimal
   solution by exploring the least points of the search space
   possible. Many techniques are applied to the auto-tuning
   problems. Some of them use the derivatives such gradient
   descent which is a kind of local search techniques. It exploits the
   locality of the search space and has particularity to converge
   quickly to a the optimal solution but it requires that the search
   space has a specific geometry and convexity of the objective
   function. But these hypothesis are not necessarily true\ref{fig:obj-func-ex}. The
   objective function may not be convex, hence with many local optimum
   and a local search search would be stuck in a local optimum. The
   problem is that local optimum can be far from the global
   optimum. That is why, to escape from this, global search are more
   suited such the simulated annealing, or genetic algorithm (kind of
   multiple local search).    

   #+begin_src R :results output graphics :file img/function_examples.png :exports results :width 600 :height 400 :session
     library(polynom)
     default <- par()
     par(mfrow = c(2, 2), oma = c(0, 0, 0, 0))
     plot(poly.calc(1:2), xlim=range(-10:10))
     plot(poly.calc(-1:5))
     plot(abs, xlim=range(-5,5))
     par(default)
   #+end_src

   #+CAPTION: Objective function characterics
   #+LABEL: fig:obj-func-ex
   #+RESULTS:
   [[file:img/function_examples.png]]

   Another concern is that, the objective function is an empirical
   function, hence it can be necessary to build a surrogate. This is
   usefull to remove the noise and as a result it facilitates the
   search. Also the derivative estimation may not be always possible and
   derivated-based searches cannot work, and the alternative is to use
   derivative-free based searches such as Nelder Mead Simplex.
   The previous search methods are used in
   Orio\cite{Hartono:2009:AEP:1586640.1587666}, a source to source 
   auto-tuner. It uses random search and simulated annealing as global
   search methods and Nelder Mead Simplex as local search. 

   The efficiency of the previous approach is highly dependent on how
   much the hypothesis about he search space are wrong and sometimes
   it is difficult know how it looks. For this reason some have worked
   on generic heuristics that combine all or part of the previous aspects
   such as pattern search\cite{Hooke:1961:DSS:321062.321069} which is
   a derivative-free based search that combines global search that
   explore the space in a finite set of direction to find  
   regions of interest and local search to examine regions of
   interest. This kind of methods allow to make less hypothesis and
   require less knowledge about the search space. This approach has
   been used in OPAL\cite{orban2011templating}, a meta-programming
   framework. It uses the mesh-adaptive
   direct-search\cite{Audet04meshadaptive}, it is an extention of the
   pattern search. It can explore in an infinte set of directions
   unlike pattern search and use derivative information when available
   to speedup the search.

   While some people developed framework to characterize the search
   space such as ASK\cite{deoliveiracastro:hal-00952307} in order to
   have a better understanding of it. This tool emphasis on the
   sampling because it is crucial for build an accurate model. It
   provides a complex sampling pipeline with different surrogate
   methods (Bayesian regression, interpolation, etc...)

   In some cases, the problem structure is well know and one has an
   idea of where is the optimal solution but the structure of the
   space in this neighborhood is too complex. The approach
   taken in Atlas \cite{Whaley:1998:ATL:509058.509096} is to focus
   only in one part of the search space to perform an exhaustive
   search. But this require know the problem well and where to search.

   In general auto-tuners exclude the user from the optimization
   process. It means that it is difficult for him to know if the
   result can be further improved, and has no clue about the quality
   of the solution. Our goal is to give more feedback and control
   through an semi-automatic and interactive approach to
   the user to guide him in the tuning the his application. Our
   approach is global and allow the user to evaluate the relevance of
   his hypothesis. We the feedback provided he is able to prune the
   search space to allow very low cost optimization.

   In the past a similar approach have been tempted by
   Brewer\cite{Brewer:1995:HOV:209937.209946} where linear regression
   of expectation have been used for the modelization to predict the
   objective function. It worked fine platform CM-5, simulated version
   of Intel Paragon and network of station based on FORE ATM, but
   these platforms are pretty old. To our knowledge this approach has
   not been used recently in the tuning of applications, we wanted to
   understand why and see if it is suited to the complexity of the
   current platforms.

** Design of experiments
   When there are lots of factors, covering the entire space of
   possible values is prohibitive. The goal experimental design is to
   build experiments in order to study the behavior of a system for a
   low experiments cost. For this reason many techniques has been
   developed to sample the space wisely.
 
   The efficiency of One-Factors-At-a-Time (OFAT) is the method of
   changing one factor at a time when the others are kept fixed. It is
   very limited when there are many factors, because it requires high
   number of experiments and it cannot find interactions between 
   factors. For these reasons factorial designs are generally more
   suited. They vary many parameters at the same time, hence
   interactions can be trapped, the estimate of the impact of the
   factors is more precise with a lower experiment cost.
  
   There are different kind of factorial designs. The first one is the
   full factorial design which consider the entire space. The simplest
   way of doing full factorial design is to chose points in the space 
   uniformly\ref{fig:doe-examples}. The drawback is that the points
   are not well distributed, there are part of space where there are
   lots of points and some other where there just few. The Latin
   Hyper-cube Sampling design provides a better coverage of the space
   by dividing the space into pieces of equal sizes and taking the
   same number of points at random in these areas. This method is made
   for continuous factors.  

   On other kind of factorial designs is the fractional factorial
   designs. Instead of considering the whole space it consider only a
   part of it. This part is chosen according to the statement that
   main effect and low order interactions (Sparsity of effect
   principle) are enough to explain the system. One of them is the
   screening design that consider only the lowest and highest values
   for factors.

   Optimal design is another category of factorial design. It samples
   the space such way that the variance is minimum, hence the
   estimation of the factors as the minimum bias. The points are taken
   according statistical model that means that means that the model
   must be already known. The advantage of optimal designs over
   non-optimal is that the need less experiment, as the sampling is
   localized. The D-Optimal design is one of them, it chooses the
   points such that the generalized variances of the least squares
   estimate of a model is minimized.
    
   The tuning of applications is in fact running multiple experiences in an
   automated or semi-automatic process. We thing that techniques from
   experimental design can help us to sample the space efficiently to
   achieve the optimization with low experimental cost.

   #+CAPTION: Space coverage by different DoE
   #+LABEL: fig:doe-examples
   #+begin_src R :results output graphics :file img/DoE_examples.png :exports results :width 600 :height 400 :session
     library(DoE.base)   
     library(DoE.wrapper)   
     library(ggplot2)

     library(grid)
     library(gridExtra)

     random <- data.frame(X1=runif(200,0,4),X2=runif(200,0,4))
     lhs <- lhs.design( type= "maximin" , nruns= 200 ,nfactors= 2 ,digits= NULL ,seed=20049 , factor.names=list(X1=c(0,4), X2=c(1,4) ) )
     Dopt <- Dopt.design(50, data=lhs, formula="~ X1 + X2 + I(1/X2)", nRepeat=5, randomize=TRUE)

     p1 <- qplot(data=random) +
         geom_point(aes(x=X1,y=X2)) +
         ggtitle("Random")

     p2 <- qplot(data=lhs) +
         geom_point(aes(x=X1,y=X2)) +
         ggtitle("lhs")

     p3 <- qplot(data=Dopt) +
         geom_point(aes(x=X1,y=X2)) +
         ggtitle("lhs")
         
     grid.arrange(p1, p2, p3, ncol=2, top=textGrob("")) 

   #+end_src

   #+RESULTS:
   [[file:img/DoE_examples.png]]

* DONE Methodology
** DONE Reproducible research
  In order for this work to be usefull for someone else a laboratory
  book is available publicly on
  github\footnote{https://github.com/swhatelse/M2\_internship}. It
  contains detailed about installation and configuration steps. It
  keeps tracks of every experiments including their description and
  analysis. It is structured in a chronological way and thus follows
  the natural evolution of the work. This gives the possibility to
  easily understand what have been done at each step and why.
  Every pieces of codes is explained using literate programming which
  is very straight forward using the org-mode of emacs.
  The github repository also contains the complet set of scripts and
  data used for experiments giving the possibility to anyone to re-run
  the same experiments using the same data.

** DONE Case study
   # Maybe cite Brice paper for this part
   In order to elaborate our approach, we took a very simple example
   which is a kernel that computes the Laplacian of an image. We want
   the minimize the time to compute a pixel and there are multiple
   optimization that can be done to enhance the performance of this
   kernel. The parameters and their values we used to tune this
   applications are the following: 

     | Parameters         | Values                            |
     |--------------------+-----------------------------------|
     | vector length      | 1,2,4,8,16                        |
     | load overlap       | true,false                        |
     | temporary size     | 2,4                               |
     | elements number    | from 1 to 24                      |
     | y component number | from 1 to 6                       |
     | threads number     | 32,64,128,256,512,1024            |
     | local work size y  | 1,2,4,8,16,32,64,128,256,512,1024 |

   The first parameter vector length allow to specify the size of the
   vectors used to performs the computation. The Laplacian can be
   easily vectorized and on hardware that provides vector support 
   it allows to save some decoding phase as the same instruction is
   applied to the entire vector. As each architecture have different
   size of vectors, and some do not provide vector support we need to
   try the different values of vector size.

   # Not satisfying yet
   The second parameter is related to the vectorization. As vectors
   are manipulated, when loading, some data overlap. Thus it is
   possible to synthetize the load from other data and consequently
   reduce the number of loads. 

   The third parameter allows to specify the size of the variables used
   for storing intermediary results. Using smaller type can reduce the
   pressure on the registers but casting variable can also be
   harmfull. Hence the default size is int (4) and we can also use
   short (2). 

   The fourth parameter splits the image into pieces of the size of
   elements number. This specifies the of component a threads will
   process, that is the amount of work per thread, and as a
   consequence defines the number of threads used to perform the 
   computation. More threads can lead to better parallelism but also
   more overhead due to the bigger number of threads to manage. 
   
   # Not satisfying yet
   The fifth parameter is used to specify how the work for a thread is
   organized by specifying the tiling. It gives how the components of
   the image are distributed in the y-axis.

   Finally the two last parameters allow to tune two OpenCL/Cuda
   parameters and describe the distribution of the work at coarse
   grain. In OpenCL and Cuda, threads are grouped and scheduled 
   by blocks on a compute unit. Which means that threads are not
   scheduled individually but by blocks. Thus we use the parameter
   threads number to specify the size of a group. The parameter local
   work size y determines how the threads are organized in a block and
   represent the number of threads in the y-axis. These parameters
   have an impact on the scheduling, the data sharing and the
   occupancy of the compute units. Thus they can lead to better usage
   of the resources.

   All the combinations of these parameters would gives a search space
   of 190080 points. However some points are unfeasible. For instance,
   having more component numbers in the y-axis (y component number)
   than number of component (elements number) itself makes no
   sense. We also have constraint the size of the kernel because it is
   limited to the available resources on the device. Exceeding the
   resources cause the kernel to crash. That is why use constraints to
   reject all the that would produce a kernel to big or that is not
   correct. Finally it remains 23100 points in the search space.

   The experiments are run on one machine with GPU Nvidia K40 using the
   driver 340.32 and two CPUs Intel E5-2630.
* Envisioned general approach
   #+BEGIN_LaTeX
   \begin{figure}[tbh]
   \centering
   \includegraphics[width=.8\linewidth]{./img/process.pdf}
   \caption{\label{fig:1}Workflow}
   \end{figure}
   #+END_LaTeX
* Controlling measurement
   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data_detailed_v2.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file img/warm_up.png :exports results :width 800 :height 600 :session
     library(plyr)
     library(ggplot2)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     d2 <- df[df$lws_y == 2 & df$elements_number == 1 & df$threads_number == 32,]
     
     df2 = ddply(d2,.(run_index,vector_length,image_size_index), summarize, 
                      mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))
     
     
     ggplot(d2) +
         # geom_jitter(aes(x=factor(run_index), y=time_per_pixel, color=factor(load_overlap), shape=factor(temporary_size))) + 
         geom_point(aes(x=factor(run_index), y=time_per_pixel)) + 
         geom_errorbar(data=df2,aes(x=factor(run_index),y=mean, ymin=mean-err, ymax=mean+err)) +
         facet_grid(vector_length ~ image_size_index, scales="free_y", labeller=label_both) 
   #+end_src

   #+RESULTS:
   [[file:img/warm_up.png]]

* Results
***                                                                :noexport:
**** Random search
   #+begin_src sh :results output :exports none
       ruby ../../../scripts/format_data.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_cleaned.yaml
   #+end_src  

   #+RESULTS:

   #+begin_src R :results output :session :exports none
     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     speedup <- c()
     for (i in 1:100){
         s <- df[sample(1:nrow(df), size = 100, replace = FALSE),]
         speedup[i] <-  min(df$time_per_pixel) / min(s$time_per_pixel)
     }
     random <- data.frame(speedup)
   #+end_src

   #+RESULTS:

**** Gradient search
***** Implementation   
   #+begin_src R :results output :session :exports none
        kernel_size <- function(point) {
            vector_number <- ceiling((point$elements_number / point$y_component_number) / point$vector_length)
            
            tempload <- (1 - point$load_overlap) * (vector_number * point$vector_length) / point$vector_length * point$vector_length
            temp <-  point$load_overlap * 3 * vector_number * (point$y_component_number+2) * point$vector_length
            res <- vector_number * point$y_component_number * point$vector_length
            tempc <- 3 * vector_number * (point$y_component_number + 2) * point$temporary_size * point$vector_length
            out_vec = (1 - point$load_overlap) * tempc
            resc <- vector_number * point$y_component_number * point$temporary_size * point$vector_length

            tot <- (tempload + temp + res + tempc + out_vec + resc) * point$threads_number
        }

        check_constraint <- function(point){
            res <- if(point$load_overlap %in% 0:1 &
                      point$lws_y <= point$threads_number &
                      point$elements_number %% point$y_component_number == 0 &
                      point$elements_number %/% point$y_component_number <= 4 &
                      kernel_size(point) < kernel_size(data.frame(elements_number=6, y_component_number=6, vector_length=8, temporary_size=2, load_overlap=0, threads_number=1024))
                      ) T else F
        }

        point_equal <- function(p1,p2){
            res <- if(p1$elements_number == p2$elements_number &
                      p1$y_component_number == p2$y_component_number &
                      p1$vector_length == p2$vector_length &
                      p1$temporary_size == p2$temporary_size &
                      p1$load_overlap == p2$load_overlap &
                      p1$threads_number == p2$threads_number) T else F
        }

        gradient_descent <- function(point, limit=100){
            elements_number    <- c(1,0,0,0,0,0,0,-1,0,0,0,0,0,0)
            y_component_number <- c(0,1,0,0,0,0,0,0,-1,0,0,0,0,0)
            vector_length      <- c(0,0,1,0,0,0,0,0,0,-1,0,0,0,0)
            temporary_size     <- c(0,0,0,1,0,0,0,0,0,0,-1,0,0,0)
            load_overlap       <- c(0,0,0,0,1,0,0,0,0,0,0,-1,0,0)
            threads_number     <- c(0,0,0,0,0,1,0,0,0,0,0,0,-1,0)
            lws_y              <- c(0,0,0,0,0,0,1,0,0,0,0,0,0,-1)

            factors <- list(elements_number = as.numeric(levels(as.factor(df$elements_number))), 
                                  y_component_number = as.numeric(levels(as.factor(df$y_component_number))), 
                                  vector_length = as.numeric(levels(as.factor(df$vector_length))), 
                                  temporary_size = as.numeric(levels(as.factor(df$temporary_size))), 
                                  threads_number= as.numeric(levels(as.factor(df$threads_number))), 
                                  lws_y= as.numeric(levels(as.factor(df$lws_y)))) 

            directions <- data.frame(elements_number, y_component_number, vector_length, temporary_size, load_overlap, threads_number, lws_y)
            count <- 0

            repeat{
                old_point <- point
                candidates <- data.frame()

                for(i in 1:nrow(directions)){
                    idx_elements_number = match(point$elements_number, factors$elements_number) + directions[i,]$elements_number
                    idx_y_component_number = match(point$y_component_number, factors$y_component_number) + directions[i,]$y_component_number
                    idx_vector_length = match(point$vector_length, factors$vector_length) + directions[i,]$vector_length
                    idx_temporary_size = match(point$temporary_size, factors$temporary_size) + directions[i,]$temporary_size
                    idx_threads_number = match(point$threads_number, factors$threads_number) + directions[i,]$threads_number
                    idx_lws_y = match(point$lws_y, factors$lws_y) + directions[i,]$lws_y

                    if(!(idx_elements_number %in% 1:length(levels(as.factor(df$elements_number))))) next
                    if(!(idx_y_component_number %in% 1:length(levels(as.factor(df$y_component_number))))) next
                    if(!(idx_vector_length %in% 1:length(levels(as.factor(df$vector_length))))) next
                    if(!(idx_temporary_size %in% 1:length(levels(as.factor(df$temporary_size))))) next
                    if(!(idx_threads_number %in% 1:length(levels(as.factor(df$threads_number))))) next
                    if(!(idx_lws_y %in% 1:length(levels(as.factor(df$lws_y))))) next

                    p <- data.frame(elements_number = factors$elements_number[idx_elements_number],
                                    y_component_number = factors$y_component_number[idx_y_component_number],
                                    vector_length = factors$vector_length[idx_vector_length],
                                    temporary_size = factors$temporary_size[idx_temporary_size],
                                    load_overlap = if(point$load_overlap == "true") 1 + directions[i,]$load_overlap else 0 + directions[i,]$load_overlap,
                                    threads_number = factors$threads_number[idx_threads_number],
                                    lws_y = factors$lws_y[idx_lws_y]
                                    )

                    if(check_constraint(p) == T){
                        p <- df[df$elements_number == p$elements_number & 
                                df$y_component_number == p$y_component_number & 
                                df$vector_length == p$vector_length &
                                df$temporary_size == p$temporary_size &
                                df$load_overlap == (if (p$load_overlap == 0) "false" else "true") &
                                #df$load_overlap == "true" &
                                df$threads_number == p$threads_number &
                                df$lws_y == p$lws_y,]
                        candidates <- rbind(p, candidates)
                    }
                }
                
                if(candidates[candidates$time_per_pixel == min(candidates$time_per_pixel),]$time_per_pixel < point$time_per_pixel){
                    point <- candidates[candidates$time_per_pixel == min(candidates$time_per_pixel),]
                }

                count <- count + 1
                if(count >= 100 | point_equal(old_point,point) == T){
                    break
                }
            }
            
            point
        }

        row_to_coordinate <- function(row){
            drops <- c("time_per_pixel", "vector_recompute")
            row[, !(names(row) %in% drops)]
         }
   #+end_src

   #+RESULTS:

***** Run search
      #+begin_src sh :results output :exports none
        ruby ../../../scripts/format_data.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_cleaned.yaml
      #+end_src  

      #+RESULTS:

      #+begin_src R :results output :session :exports none
        options(width=150)
        grad <- data.frame()
        grad_start <- data.frame()
        for(i in 1:100){
            p <- df[sample(1:nrow(df), size = 1, replace = FALSE),]
            grad_start <- rbind(grad_start,p)  
            grad <- rbind(grad,gradient_descent(p,1e5))
        }

        speedup <- c()
        for (i in 1:nrow(grad)){
            speedup[i] <-  min(df$time_per_pixel) / grad[i,]$time_per_pixel
        }
        grad <- data.frame(speedup)
      #+end_src

      #+RESULTS:

**** Genetic Algorithm
   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_gen_g10_p10_m01_e1_tt.yaml 
   #+end_src

   #+RESULTS:

   #+begin_src R :results output :session :exports none
     ga_res <- read.csv("/tmp/test.csv", strip.white=T, header=T)

     library(ggplot2)
     speedup <- c()
     for (i in 1:nrow(ga_res)){
         speedup[i] <-  min(df$time_per_pixel) / ga_res[i,]$time_per_pixel
     }
     ga <- data.frame(speedup)
   #+end_src

   #+RESULTS:

**** Linear regression of expectation
***** Random
***** LHS
***** D-Optimal
**** Quantile regression
***** Random
***** LHS
***** D-Optimal
*** 
   #+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports results :width 800 :height 400 :session
     library(ggplot2)
     library(grid)
     library(gridExtra)

     p1 <- qplot(data=random) +
         geom_histogram(aes(x=speedup,y=..density.. * 0.05), binwidth=0.05, color="white") +
         theme(axis.text.x = element_text(angle = 70, hjust = 1, face="bold", size=12)) +
         geom_vline(xintercept = median(random$speedup), color="green", linetype=2) +
         geom_vline(xintercept = mean(random$speedup), color="red", linetype=2) +
         geom_vline(xintercept = quantile(random$speedup, prob=c(0.25,0.75)), color="blue", linetype=2) +
         ggtitle("Random search") +
         labs(y="Density", x="Speedup")

     p2 <- qplot(data=grad) +
         geom_histogram(aes(x=speedup,y=..density.. * 0.05), binwidth=.05, color="white") +
         theme(axis.text.x = element_text(angle = 70, hjust = 1, face="bold", size=12)) +
         geom_vline(xintercept = median(grad$speedup), color="darkgreen", linetype=2) +
         geom_vline(xintercept = quantile(grad$speedup, prob=c(0.25,0.75)), color="blue", linetype=2) +
         geom_vline(xintercept = mean(grad$speedup), color="red", linetype=2) +
         ggtitle("Greedy") +
         labs(y="Density", x="Speedup")

     p3 <- qplot(data=ga) +
         geom_histogram(aes(x=speedup,y=..density.. * 0.05), binwidth=.05, color="white") +
         theme(axis.text.x = element_text(angle = 70, hjust = 1, face="bold", size=12)) +
         geom_vline(xintercept = median(ga$speedup), color="darkgreen", linetype=2) +
         geom_vline(xintercept = quantile(ga$speedup, prob=c(0.25,0.75)), color="blue", linetype=2) +
         geom_vline(xintercept = mean(ga$speedup), color="red", linetype=2) +
         ggtitle("Genetic algorithm") +
         labs(y="Density", x="Speedup")
         
       grid.arrange(p1, p2, p3, ncol=3, top=textGrob("Speedup of the tuning process compared to the best")) 
   #+end_src

   #+RESULTS:
   [[file:/tmp/babel-3905hlV/figure39052nn.png]]

* Analysis
** Characteristics of the search space
   #+begin_src sh :results output :exports none
       ruby ../scripts/clean_data.rb ../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end.yaml
   #+end_src

   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_cleaned.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file ./img/search_combination_rep_speedup.png :exports results :width 800 :height 600 :session
     library(ggplot2)
     library(grid)
     library(gridExtra)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     speedup <- c()
     for(i in 1:nrow(df)){
         speedup[i] <- min(df$time_per_pixel) / df[i,]$time_per_pixel      
     }
     df$speedup <- speedup

     p1 <- qplot(data=df) +
         geom_histogram(aes(x=time_per_pixel,y=..density.. * 1e-9), binwidth=1e-9, color="white") +
         theme(axis.text.x = element_text(angle = 70, hjust = 1, face="bold", size=12)) +
         geom_vline(xintercept = median(df$time_per_pixel), color="green", linetype=2) +
         geom_vline(xintercept = mean(df$time_per_pixel), color="red", linetype=2) +
         geom_vline(xintercept = quantile(df$time_per_pixel, prob=c(0.25,0.75)), color="blue", linetype=2) +
         ggtitle("Density of the of performances combinations") +
         labs(y="Density", x="Time per pixel")

     p2 <- qplot(data=df) +
         geom_histogram(aes(x=speedup,y=..density.. * 0.05), binwidth=.05, color="white") +
         theme(axis.text.x = element_text(angle = 70, hjust = 1, face="bold", size=12)) +
         geom_vline(xintercept = median(speedup), color="darkgreen", linetype=2) +
         geom_vline(xintercept = quantile(speedup, prob=c(0.25,0.75)), color="blue", linetype=2) +
         geom_vline(xintercept = mean(speedup), color="red", linetype=2) +
         ggtitle("Density of the combinations speedup compared to the best") +
         labs(y="Density", x="Speedup")
         
       grid.arrange(p1, p2,  ncol=2, top=textGrob("Repartition of the performance combination")) 
   #+end_src

   #+RESULTS:
   [[file:./img/search_combination_rep_speedup.png]]

   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data.rb ../../../data/2016_03_11/pilipili2/19_13_54/Data19_13_54_linear.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file ./img/heteroscedasticity.png :exports results :width 700 :height 400 :session
     library(ggplot2)
     library(grid)
     library(gridExtra)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)

      p1 <- qplot() + 
          geom_point( aes(x=df$vector_length, y=df$time_per_pixel), alpha=0.1 ) + 
          ggtitle("Impact of the vector length") +
          labs(y="time per pixel in seconds", x="vector length") +
          theme(axis.text=element_text(size=12),
                axis.title=element_text(size=14,face="bold"))

      p2 <- qplot() + 
          geom_point(aes(x=df$x_component_number, y=df$time_per_pixel),alpha=0.1) + 
          ggtitle("Impact of number of component on the x-axis") +
          labs(y="time per pixel in seconds", x="x component number") +
          theme(axis.text=element_text(size=12),
                axis.title=element_text(size=14,face="bold"))

     grid.arrange(p1,p2,  ncol=2, top="") 

   #+end_src

   #+RESULTS:
   [[file:./img/heteroscedasticity.png]]

** Linear regression of expectation: why it cannot work and how it can be circumvented
   #+begin_src sh :results output :exports none
       ruby ../../../scripts/format_data.rb ../../../data/2016_03_11/pilipili2/19_13_54/Data19_13_54_linear.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file img/lm.png :exports results :width 800 :height 400 :session 
     library(ggplot2)
     library(plyr)
     library(gridExtra)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     err_x_comp = ddply(df,c("x_component_number"), summarize,
                        mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))


     err_v_len = ddply(df,c("vector_length"), summarize,
                       mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))

     p1 <- qplot(df$vector_length, df$time_per_pixel) + 
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_errorbar(data=err_v_len,aes(x=vector_length,y=mean, ymin=mean-err, ymax=mean+err),colour="red") +
         ggtitle("Impact of the vector length") +
         labs(y="time per pixel in seconds", x="vector length") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))

     p2 <- qplot(df$x_component_number, df$time_per_pixel) + 
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_errorbar(data=err_x_comp,aes(x=x_component_number,y=mean, ymin=mean-err, ymax=mean+err),colour="red") +
         ggtitle("Impact of number of component on the x-axis") +
         labs(y="time per pixel in seconds", x="x component number") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))

     grid.arrange(p1, p2, ncol=2, top="") 

   #+end_src
   
   #+CAPTION: Linear regression and non-uniform noise
   #+LABEL: fig:lm-1
   #+RESULTS:
   [[file:img/lm.png]]
   
   Linear regression has already been used successfully for
   auto-tuning problems\cite{Brewer:1995:HOV:209937.209946}. But they
   have been put aside for no real reasons to our knowledge. Using
   this method to study the impact of the parameters with using linear
   models to approximate the behavior of the search space coupled with
   efficient sampling strategies seemed very interesting to us.
   
   If linear regression have been efficient in brewer's
   work\cite{Brewer:1995:HOV:209937.209946} it is maybe because at
   this time the architecture of computers was less complicated than
   today. The figure\ref{fig:lm-1} shows clearly the limit of the
   linear regression on the simple case such as a Laplacian kernel on
   nowadays architectures. First, one of the assumptions of the linear
   regression is homoscedasticity of the data which is not often
   necessarily the case, and in our example we can see that the
   variability is not the same at each factor level.

   Heteroscedasticity is problematic because the least square is not
   the Best Linear Unbiased Estimator in this case and it biases the
   variance  and thus the coefficient of determination which makes it   
   more difficult to evaluate the accuracy of the model.

   If the error law is the same everywhere as in the left in
   figure\ref{fig:lm-1} we can still have the minimum values that
   follow the same evolution as the mean and we can still predict the
   minimum. The resulting model and approximation can still be correct
   and we can easily know what is the best size for the length of the
   vector. But we would need to make assumptions that about the 
   error and we do not know anything about the error. In the right in
   figure\ref{fig:lm-1}, the evolution of the mean and the evolution
   of the minimum is not correlated and the best value is mispredicted.  

   We conclude that in the case of heteroscedasticity and non-uniform
   error law, linear regression tracks the general tendency of impact
   of the parameters. But in our case in which we are interested about
   the minimum which is uncorrelated to the mean, the linear
   regression cannot lead to the global optimum and we need another
   estimator for the minimum. 

*** The choice of quantile regression
   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data.rb ../../../data/2016_03_11/pilipili2/19_13_54/Data19_13_54_linear.yaml
   #+end_src

   #+begin_src R :results output graphics :file img/why_we_choose_quantile_reg.png :exports results :width 600 :height 400 :session
     library(ggplot2)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     ggplot(df) + 
         aes(x=x_component_number, y=time_per_pixel) +
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_smooth(method="lm", formula = y ~ x + I(1/x), aes(colour="linear regression")) +           
         stat_quantile(quantiles=0.05, formula = y ~ x + I(1/x), aes(colour="quantile regression")) +
         ggtitle("Impact of number of component on the x-axis") +
         labs(y="time per pixel in seconds", x="x component number") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))

   #+end_src

   #+CAPTION: Linear regression vs quantile regression
   #+LABEL: fig:qr-example
   #+RESULTS:
   [[file:img/why_we_choose_quantile_reg.png]]

   In our case
** Model choice and refinement
** Model optimization
** Using less point as possible: sampling strategy
* Future work
* Conclusion
#+LaTeX: \nocite{*}
#+LaTeX: \def\raggedright{}
\bibliographystyle{plain}
\bibliography{../../biblio.bib}


* Emacs Setup 							   :noexport:
  This document has local variables in its postembule, which should
  allow Org-mode to work seamlessly without any setup. If you're
  uncomfortable using such variables, you can safely ignore them at
  startup. Exporting may require that you copy them in your .emacs.

# Local Variables:
# eval:    (require 'org-install)
# eval:    (org-babel-do-load-languages 'org-babel-load-languages '( (sh . t) (R . t) (perl . t) (ditaa . t) ))
# eval:    (setq org-confirm-babel-evaluate nil)
# eval:    (unless (boundp 'org-latex-classes) (setq org-latex-classes nil))
# eval:    (add-to-list 'org-latex-classes '("memoir" "\\documentclass[smallextended]{memoir} \n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n  \\usepackage{graphicx}\n  \\usepackage{hyperref}" ("\\chapter{%s}" . "\\chapter*{%s}") ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("acm-proc-article-sp" "\\documentclass{acm_proc_article-sp}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (setq org-alphabetical-lists t)
# eval:    (setq org-src-fontify-natively t)
# eval:   (setq org-export-babel-evaluate nil)
# eval:   (setq ispell-local-dictionary "english")
# eval:   (eval (flyspell-mode t))
# eval:    (setq org-latex-listings 'minted)
# eval:    (setq org-latex-minted-options '(("bgcolor" "white") ("style" "tango") ("numbers" "left") ("numbersep" "5pt")))
# End:

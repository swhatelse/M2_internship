#+TITLE: 
#+LANGUAGE: en
#+Author: 
#+TAGS: noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LaTeX_CLASS: memoir
#+LaTeX_CLASS_OPTIONS: [12pt, a4paper]
#+OPTIONS: H:5 title:nil author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil
#+BABEL: :session *R* :cache yes :results output graphics :exports both :tangle yes 

#+LATEX_HEADER:\usepackage[french,english]{babel}
#+LATEX_HEADER:\usepackage [vscale=0.76,includehead]{geometry}                % See geometry.pdf to learn the layout options. There are lots.
# #+LATEX_HEADER:\geometry{a4paper}                   % ... or a4paper or a5paper or ... 
# #+LATEX_HEADER:\geometry{landscape}                % Activate for for rotated page geometry
# #+LATEX_HEADER:\OnehalfSpacing
# #+LATEX_HEADER: \setSingleSpace{1.05}
# #+LATEX_HEADER:\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
#+LATEX_HEADER:\usepackage{amsmath}
#+LATEX_HEADER:\usepackage{fullpage}
#+LATEX_HEADER:\usepackage{mathptmx} % font = times
#+LATEX_HEADER:\usepackage{helvet} % font sf = helvetica
#+LATEX_HEADER:\usepackage[latin1]{inputenc}
#+LATEX_HEADER:\usepackage{relsize}
#+LATEX_HEADER:\usepackage{listings}

#+BEGIN_LaTeX
%Style des têtes de section, headings, chapitre
\headstyles{komalike}
\nouppercaseheads
\chapterstyle{dash}
\makeevenhead{headings}{\sffamily\thepage}{}{\sffamily\leftmark} 
\makeoddhead{headings}{\sffamily\rightmark}{}{\sffamily\thepage}
\makeoddfoot{plain}{}{}{} % Pages chapitre. 
\makeheadrule{headings}{\textwidth}{\normalrulethickness}
%\renewcommand{\leftmark}{\thechapter ---}
\renewcommand{\chaptername}{\relax}
\renewcommand{\chaptitlefont}{ \sffamily\bfseries \LARGE}
\renewcommand{\chapnumfont}{ \sffamily\bfseries \LARGE}
\setsecnumdepth{subsection}


% Title page formatting -- do not change!
\pretitle{\HUGE\sffamily \bfseries\begin{center}} 
\posttitle{\end{center}}
\preauthor{\LARGE  \sffamily \bfseries\begin{center}}
\postauthor{\par\end{center}}

\newcommand{\jury}[1]{% 
\gdef\juryB{#1}} 
\newcommand{\juryB}{} 
\newcommand{\session}[1]{% 
\gdef\sessionB{#1}} 
\newcommand{\sessionB}{} 
\newcommand{\option}[1]{% 
\gdef\optionB{#1}} 
\newcommand{\optionB}{} 

\renewcommand{\maketitlehookd}{% 
\vfill{}  \large\par\noindent  
\begin{center}\juryB \bigskip\sessionB\end{center}
\vspace{-1.5cm}}
\renewcommand{\maketitlehooka}{% 
\vspace{-1.5cm}\noindent\includegraphics[height=14ex]{logoINP.png}\hfill\raisebox{2ex}{\includegraphics[height=7ex]{logoUJF.jpg}}\\
\bigskip
\begin{center} \large
Master of Science in Informatics at Grenoble \\
Master Math\'ematiques Informatique - sp\'ecialit\'e Informatique \\ 
option \optionB  \end{center}\vfill}
% End of title page formatting

\option{$PDES$}
\title{ Semi-Automatic Performance Optimization of HPC Kernels }%\\\vspace{-1ex}\rule{10ex}{0.5pt} \\sub-title} 
\author{Steven QUINITO MASNADA}
\date{ June 22th } % Delete this line to display the current date
\jury{
Research project performed at $<$lab-name$>$ \\\medskip
Under the supervision of:\\
Arnaud LEGRAND, Frederic DESPREZ, Brice VIDREAU, CNRS\\\medskip
Defended before a jury composed of:\\
Prof Noel DEPALMA\\
Prof Martin HEUSSE\\
Dr Thomas ROPARS\\
Prof Olivier GRUBER\\
Dr Henri-Pierre CHARLES\\
}
\session{$June$\hfill 2016}
#+END_LaTeX

#+BEGIN_LaTeX
\selectlanguage{english} % french si rapport en français
\frontmatter
\begin{titlingpage}
\maketitle
\end{titlingpage}

%\small
\setlength{\parskip}{-1pt plus 1pt}

\renewcommand{\abstracttextfont}{\normalfont}
\abstractintoc
\begin{abstract} 
Text 
\end{abstract}
\abstractintoc
\renewcommand\abstractname{R\'esum\'e}
\selectlanguage{english}% french si rapport en français

\cleardoublepage

\tableofcontents* % the asterisk means that the table of contents itself isn't put into the ToC
\normalsize

\mainmatter
\SingleSpace

#+END_LaTeX

# #+BEGIN_abstract
#   Blablabla
#   \newpage
# #+END_abstract

* Plan                                                             :noexport:
** Introduction [3/3]
*** Why is performance optimization difficult?
   - In HPC code optimization crucial to exploit very complex hardware.
     Cannot wait for the next generation to bring speedup because it
     does not (Frequency not higher but more cores and henanced ISA). 
     - many cores \to heavy parallelism \to need to program parallel
     - pipelining ILP \to 
     - vector support \to SIMD \to need to work with vector
     - cache hierarchies \to need to exploit data locality
     - GPUs! \to different way of programming (than CPU)
   - HPC plaforms have many \ne hardware \to code optimizations not portable.
     Porting application to another platform is time consumming and
     can be very tricky.
   - Many attempts in the last decade to automate the generation of
     optimized code
*** Code generation and opportunities
    - The compiler approach: loop unrolling, vectorization, automatic
      parallelization, loop nest transformation, etc. Yet, many
      opportunities are not exploited as it is too difficult to
      exploit them automatically. Sometimes, the source code has to be
      rewritten in a slightly different way to enable the compiler to
      be effective
    - Parametric optimization:
      - The source-to-source transformation (C to C, Fortran to Fortran,
        ...). Framework for transform code. Orio. Serge
        Guelton. Difficile mais limité à un seul langage, et
        exploitation d'accelérateurs différents difficile. Ça ne se
        permettra jamais de changer le mapping des données en mémoire
      - Meta-programming approach: allow the programmer to propose
        optimizations that the compiler would not be allowed to do
        (because of the language or because it would require information
        on the application that cannot be given to the compiler).
        # But it is also the case with source-to-source transformation
        # right?    
*** Optimizing: the auto-tuning approach
    - Many optimization options: compiler flags, source-to-source
      transformations, higher-level modifications (tile/block/vector
      size). Each combination represents an implementation.      
    - Auto-tuning: consider all this as a huge optimization problem
      and try to find the best combination. Many techniques (genetic
      algorithms, simulated annealing, tabu search, machine learning,
      ...) depending on the problem constraints. But mainly two
      problems:
      - the time needed for such optimization
      - knowing whether further optimizations could be expected or not
        (peak performance is generally useless and the optimization
        process is so complex that it's hard to know how it really
        performed) is difficult and même si tu sais qu'il devrait être
        possible de faire mieux, tu sais pas vraiment où, comment( cf
        of genetic algo on the full search space), ...
*** Our goal
    Many approaches in code generation/transformation. It's possible
    to start from high-level codes (e.g., pytran) but the most
    optimized codes are obtained from specific tools (FFT, BLAS,...).

    We decided to evaluate an intermediate approach by relying on
    BOAST, a metaprogramming... (Semi automatic approach \to gives back
    power to the user, framework ruby generating portable code in C,
    Fortran, OpenCL. DSL) and investigate various statistical
    techniques inspired from the design of expeirments field that
    emphasizes on reducing experiment cost.

    Investigate the design of a semi-automatic optimization framework,
    where the applicaiton developer has control and understands how
    the optimization works and guides it.
*** My contribution
    - Related work on auto-tuning
    - Proposal based on DoE
    - Evaluation
      - Comparison with state of the art
      - Analyze

    - (Complex methods used but no explanation on why they work)
    - Prevent biased measurement
    - Try a simple approach and try to understand it deeply
      - Getting knowledge from the problem to guide the user:
        - Take into account hypothesis \to use the knowledge of the user
          1. Sampling the space
          2. Model find 
             - Removing useless factors
             - Refine the model \to add quadratic terms, 1/x,
               interactions, etc...
          3. Fix parameters to prune the search space and add removed
             factors.
          4. Back to 1 until we are able to fix all parameters values   
          
        - Linear regression methods to model the search space \to
          finding good model based on hypothesis. Allow the user to
          check this hypothesis. And understand the problem.
          - Try OLS \to problem with regression of expectation
            heteroscedacity + non uniform noise
          - Solution \to quantile regression
            - Pb with rq \to error to compute std. err, etc...
            - Used iterated weighted least square 
              Pb to make inferences \to biased R-squared and std. error
              # Are std.err biased to?
              How to compute CI?
              
        - Modeling
          - Start generic \to go specific
            Over specification \to biased
          - Sampling is crucial \to Design of experiments \to reducing number of experiments
            What design of experiment to use?
            - Random
            - LHS
            - Screening
            - D-Optimal
            How to use them? Copying with constraint
            - Start without hypothesis on the model otherwise \to biais
            - Add point with hypothesis \to D-Opt
            
*** Structure of the report
** Context [3/3]
*** HPC/architectures     
    - Crucial for science and business
    - To get performance \to exploit hardware \to take characteristics into account
      - Many cores \to aims low idle time
        Thinking parallel
        Right number of threads \to because overhead in thread
        management.
        Less synchro as possible
      - GPUs \to suited to a certain type of computation \to can bring
        lots of performances.
      - vector support
        Data pipelining
        Share the same instruction on multiple data \to save decoding
        
      - cache levels
      - ILP \to break instruction dependencies
    - Architecture \ne from a HPC to another
      Specialized code \to not portable
*** Obtaining efficient code
**** Compilation
     Il fait ce qu'il peut mais pas de vision globale du code \to local
     optimization (intra procedure) \to because more easier no control
     flow
     - code re-ordering \to instruction scheduling find the best
       sequence for the pipeline \to reduce instruction conflict
       (dependencies between instructions) 
     - Register allocation
     - loop transformation \to parallelization and data locality \to 
       finding parallelism into loops \to loop nest transformation /
       unroll. 
     - Automatic parallelism \to multi-threaded, vectorized 
       Pb with shared/global variable, IO, indirect addressing, etc...
   
     Limited because stuck by semantic rules, not enough information
     at compile time, etc...
     
     Archi compliquée donc dur: Grigori Fursin.
     Sometime the platform the not well supported.
**** Source-to-source transformation (C vers C ou FORTRAN vers FORTRAN)
     - Relieve compiler \to deactivate optimization
       Gives to the compiler the desired optimization
     - Gives more expressiveness \to more information two performs
       transformation \to ensure that the semantic is correct 
     - Present the code correctly to allow the compiler to make his job.
     - orio, PIPS,  cloog 
       Generally annotation-based  
       How is it used
      - pluto (automatic parallelization)
      - pytran
      - auto-tuning on top of orio

     Pros and cons:
     
**** Meta-programming: BOAST
     Less constraint by semantic rules but can be error prone \to not
     correct transformation.
     BOAST: 
     - for advanced user
     - Ruby
     - Complete tool chain
       - DSL
       - Code generation
       - Compilation
       - Bench-marking
       - Kernel verification

*** Recap
    How to port performances.
** Problem analysis [0/1]
   - Huge search space \to need to explore only part of it \to
     optimization problem.
   - Interactions between parameters
   - Non-smooth and empirical objective function
   - Combination of discrete and continuous parameters
   - Constraint optimizations
     Represent unfeasible points.
        
** State of the art on Autotuning [2/4]
   - What is autotuning
     paramters \to represents different version/implementation

  # Maybe an overview of machine learning in general
  - Reuse knowledge of previous experience (generalization) \to machine
    learning. For different problem \to re-usability. 
    What is machine learning and why it is useful in auto-tuning.
    Generally exhaustive search costly training phase \to
    reducing impact. Classification \to which strategy to apply.
    - Small vs. Big
    - Milepost GCC \to learning characteristics of a program to
      predict what are the good combinations, optimization
      across programs. Predict good configuration using the
      distribution of good combination by taking the mode.
      Reuse knowledge across programs
    - Stefan Wild \to Learning combination across platform
      Worked for similar platforms. Search space pruning \to random
      search.
      Reuse knowledge across platforms
    - Opentuner \to which optimization technics for a given problem
      because the efficiency of a technics depends on the
      structure of the problem.
    - Incremental training \to Nitro using active learning
    - Collective tuning \to crowdtuning, Milepost
      Models stored in a common database and continuously updated.

  Optimization: exhaustive search is unfeasible.

  - "Direct search". The efficiency (ability to find the
    (near)-optimal solution and possibly in the fewest possible
    experiments) depends on the structure of the problem.
    - Main techniques:
      - Gradient descent: ferrari, a priori = local, geometry, convexity.
        - Issues: 
          - partly wrong hypothesis (geometry, convexity): simulated
            annealing, many local searches (genetic algorithms in some
            sense)
          - experimental estimation (empirical function)  :
            surrogates, etc. *local* approximation
            Usefull to remove the noise and facilitate the search
          - derivative estimation: Nelder Mead Simplex
        - \to many heuristics that combine all or part of the different
          previous approaches depending on how much the various
          hypothesis are wrong or not. Their efficiency highly depends
          on these hypothesis.
    - Some people have thus developed framework to characterize the
      optimization space.
      - ASK \to Emphasis on the sampling because important for the
        accuracy of the model \to complex sampling pipeline with
        different surrogate methods( bayesian regression,
        interpolation, etc... ). _Global modeling requires complex
        models and numerous experiments_.
    Illustration with a few tools:
    - Orio \to source to source annotation based autotuner 
      - random search, Nelder Mead Simplex and simulated annealing.
      - greeding algorithm for local search at the end of gobal.
    - OPAL \to Use direct search combinations of heuristics \to
      Mesh-adaptive direct-search \to pattern search.
      Global *and* local search \to work by iterative phase:
      - Sampling the space \to finding region of interest
      - Refining the solution
    - In some cases, the problem structure is known and one has an
      idea of where the optimal solution is but the structure of the
      space in this neighborhood is too complex. Some fall back to
      Exhaustive search \to Atlas Linear search, know where to search \to
      need to know the problem well.

  Primary Goals:
  - semi-automatic, almost interactive ? more global approach where
    the relevance of the hypothesis can be evaluated
  - optimize at low cost, need to prune the search space
  - from previous experience, generalization from an arch to another
    seems very difficult

  Somehow similar approach:
  - Getting knowledge on the fly \to regression, interpolation
    - Brewer \to linear regression for the modelization to predict
      objective function and root finding  or kind of greedy
      descent for the optimization.
      Find correct model automatically on platform CM-5, simulated
      version of Intel Paragon and network of station based on FORE ATM. 
      Not recent paper \to architecture have evolved. Is linear
      regression still ok?
** State of the art design of experiments [1/2]
   - Study phenomenon \to behavior of a system
     - Acting on many factor at a time instead of one
     - Get information on how the factors impact the system and
       interactions \to not possible with OFAT (one factor at a time) \to
       factorial design
     - Identify interaction without trying all range of values.
     - Define explanatory variable.
   - DoE:
     - OFAT
     -Factorial
       - Random
       - LHS
         For continuous space
         Provide Better coverage of the space
       - Fractional design
         Screening design \to Take the extreme values
       - Optimal design
         - D-Optimal
           Require to know the model
           Select points according to a model.
         - I-Optimal
         - A-Optimal
** Methodology [3/3]
*** Reproducible work
    - Lab book on github  
    - Literate programming 
    - org mode
*** Case study
****  Laplacian
      - OpenCL
      - Optimizations explanation
        - Vectorization \to vector length
        - Synthetize loading \to load overlap \to for memory bound?
        - Tilling \to y component number
        - Number of threads \to elements number
        - Size of temporary results \to temporary size
          Reducing pressure on registers? If high usage of registers?
          If not high usage of registers overhead of casting?
        - Size of a work group \to threads number
        - Shape of work group \to lws y
      - 23100 combinations
      - Minimization
      - Test 5 sizes of images \to mean
**** Experimental protocol  
    - Result validation against bruteforce
    - Comparison with random, gradiant search, and genetic algorithm
    - Bench min of 4 runs \to warm up effect
**** Search space characteristics
     - Qualitative observation in term of speed up
**** Comparison with random and genetic algo
*** Controlling measurement 
    - Time per pixel \to total time / number of pixel. Because we test
      different size of image.
    - min(x_1,...,x_10) ? how to protect against potential warm-up
      - Energy saving mode of current hardware(CPU and GPUs)
      - Mostly present just after the compilation of the kernel.
      - 4 runs \to take the minimum
    - randomizing to protect against bias, even for full search
      space. But run and image size not randomized.

    - Process
      - Code generation
      - Compilation
      - Bench-marking

** Envisioned general approach[1/1]
   # Maybe need more explanation 
   # What is the linear regression, how we use it, why, etc...
   Semi automatic, interactive \to gives more control, feed back to the
   user, guide him.
   Gives information about the search space characteristics \to shape \to
   define the search methods, where could be the best parts \to pruning
   BOAST \to for advanced users who are ready to rewrite their code in
   ruby and know what they are doing.
   Regression + sampling to get knowledge
   Show the structure of the problem parameters that have the most
   impact (global) one those have less impact (local)

   Interrogating correctly the search space \to sampling
   Build model of the objective function \to easier for optimize and extract information
   Use the knowledge of the expert:
   - Can test his hypothesis
   - Understand the search space and his problem
   - Understand what happens
    1. DoE
       - Sampling the space wisely
       - Use linear regression OLS:
         - remove factors from the model
         - model and optimize
    2. Loop back to 1 to refine the model

** Results [3/3]
    Considering slowdown with regard to the best.
    Comparison:
    - GA \to not tuned \to would have take time to tune it
      Very efficient in general
    - Greedy
      Fails
    - Random
      Very simple and efficient also
    - LM
      - Uniform \to The one which get the most high performing version
        but sometime fails and gives very bad results.

    - Rq \to Another way of doing linear regression
      - Uniform \to Improved a little bit in general LM but less very
        high performing version 
   | Histogram of solutions | Cost |

** Analysis [0/14]
*** Characteristics of the search space [1/2]
   - Discrete and constrained
   - Repartition of good combinations
   - Lots of local optimum \to local search failed
   - Heteroscedasticity \to noise due to interactions
*** Linear regression of expectation: why it cannot work and how it can be circumvented [2/3]
   OLS gives often good results but sometime can fail.
   Prediction of two different things:
   - Mean / Expected value
   - Quantile
**** Least Squared regression and non uniform noise  
    - Assumptions:
      - homoscedasticity (Gaussian noise) but pb we have heteroscedasticity
        - Why is it a problem?
          - Unbiased coefficient estimate but biased std error and thus
            R-squared \to more difficult know if a model is correct
          - But it is still ok if the error law is the same everywhere
      - But we don't know anything about the noise and normal
        distribution of the noise is assumed. We cannot do anything
        about that because in our case the noise come from complex
        interactions between parameters.
        Possible to reduce it by fixing values but it is not always
        possible to do that e.g. if for all the parameters the noise
        falls the same law. But we still have some difficult to find
        model due to the other parameters.        
    - Tracks general tendency of the impact of factors
    - 2 cases:
      - heteroscedasticity + same error law \to minimum can be predict
      - heteroscedasticity + different error law \to minimum and mean
        uncorrelated \to minimum can not be predict
**** Using quantile regression
     - Interested in extremal values \to minimum
       - 5th and 95th percentile \to good estimation for extreme values
     - Ways of computing quantile regression
       - empirical quantiles \to linear regression on a quantile
       - Least absolute values
       - Iterated weighted least squares 
         - But optimist R-squared
         - Don't know how to interpret the standard error
*** Explanation of LM success and "failures" [3/3]
    - Failure :
      Happened 2 times
      Due to the automatic strategy. Misprediction of vector
      length. Tried instantiate not accurate model \to lot of predicted
      good point are bad indeed.
      This can be detect by the user it, and can act considering this
      by either remove the factors or asking more points.
    - Success:
      Instantiation of correct model with only relevant factors with
      low error \to low uncertainty of the estimate. The more factors
      are fixed the lower the noise \to converge to the same solution
      most of the time.
    - Vector_length either 1 or 16 because simple linear
      model. Crucial to fix correctly this parameters correctly
    - Never reach best case because of the lack of acuraccy of the model  
*** Model choice and refinement [0/2]                            :deprecated:
    - Hypothesis based on the kernel
      The expert knows his kernel and have hypothesis of how the
      optimization will influence the performances.
      - Explanation of the impact of the parameters \to justification of
        the model \to hypothesis
        - elements_number
        - y_component_number
        - etc...
    - Hypothesis testing:
      - Try \ne hypothesis
        - First start to eliminate factor that have no impact
        - Remove then from the model
        - Try to find interactions
      - Keep the more accurate and the simplest

    - Process dependent of the set of points \to cannot apply a model
      blindly even if it the correct model without considering the points.
    - Test parameters independently and remove useless ones. 
    - Iterative refinement \to try to find the interactions.
    - Determines the quality of the prediction
      - We cannot use R-squared \to biaised because of the iterative
        approach.
      - Visual checking \to yek! How can I do visualization on more than
        3D? I can not make regression for each factor because it's not
        the same than one regression including all the factors. But we
        could optimize each parameters independently.
*** Using as little points as possible [0/4]                     :deprecated:
    - Design of experiment
      - Random
      - Screenning design
        Not suitable for constrained search space \to lot of point cannot
        be reached because test those at the border. Constraints have
        to be expressed in the objective function
      - LHS
        Good starting point \to no hypothesis point are choosen
        uniformly but more wisely than a random sampling.
        Generally for continuous factors \to convert to discrete \to is it
        still wiser than random? 
      - D-optimal
        Can be used to find the model but use it careful \to no
        hypothesis at the begining otherwise it introduces some biais.
        it selects points that
        explain the model \to there many possible models, it depends
        which points are choosen.
        Usefull to make refinement \to when the model is already known.
    - Strategy
      - Start sampling with less hypothesis as possible \to to avoid biais
      - Points budget \to distributing correctly the budget of point
        between each step is crucial
    - Copying with constraints
*** Importance of the search space expression [0/1]              :deprecated:
    # Will see if I have more time to dig the subject
    - Easier modelization
    - Better capture of the search space features
      
** Technical difficulties
*** Model Optimization [0/2]
   After modelization we need to perform search on the estimated
   objective function. 
   - Model continuous but we work on discrete pb
   However \to Non-convex optimization  
     Constraint \to unfeasible points
     Optimization quickly stuck
     - Barrier approach \to guide the search to feasible regions
     - Simulated annealing \to need to tune it correctly
     - Gradient descent
   - Exhaustive search \to ensure that we get the best response and
     allowed us to evaluate the model.
*** LHS
    - Pb with constraints lots of points rejected
    - Continuous to discrete
    - Not better than random is our case.
*** RQ
    Rq didn't works reimplement it \to iterated weighted least square.
    How many iteration to converge? Too optimistic R-squared maybe
    because of the weight and too much iteration. Don't know how to
    interpret std err and compute confidence interval. 
    Pb to make inference about the model and compare them.
** Future work [0/2]
   - Constraints \to barrier approach
   - Find more suited design of experiments techniques
   - Validate approach on more complex kernel and different
     architectures
     
** Conclusion [0/2]
   And finally I saved the world...

* DONE Introduction
** Why is performance optimization difficult?
  From genome sequencing to molecular dynamics, including climate or
  earthquake modeling, or aerodynamics, there is an ever increasing
  need for computer power. For this purpose, High Performance Computing (HPC) is
  the most effective solution. It has brought the science to another
  level and now it is a tool that has became essential for scientists like
  for example to simulate nuclear explosion or to analyze peta-octets of
  data. The expectations of scientists in term of performances are
  higher and higher as they need to run more and more heavy and complex
  computations. To take advantage of the power of a supercomputer it is
  essential to correctly optimize the applications. This is a very
  complicated task because today's HPCs are extremely complex
  machines. It is not possible to wait for the next generation of
  hardware to bring automatically a speedup as 
  it was the case up to the years 2000 because frequency cannot
  increase anymore and in contrary even tends to decrease. For this
  reason, we went from multi-cores to many-cores architectures and 
  for 2020 exascale platforms, supercomputers with millions of cores,
  are expected in order to reach the exaflops. Thus, developers have
  to take into account this massive parallelism when writing
  programs. Furthermore, they also have to take care about things such
  the dependencies of the instructions to fully occupy the pipeline. If
  there is any vector support the developer should adapt his code to work on
  vectors instead of single variables. In addition the architecture provides
  different cache hierarchy and it is crucial to exploit data locality
  to use them efficiently. 
  Finally to address the computer power need, GPUs have become very
  popular. Unfortunately, this add a little more complexity as they
  require a way of programming which is different from the CPUs. As a
  result, performance optimization is difficult to achieve. To build
  efficient HPC platforms, architects have to come up with unique
  combinations of a variety of hardware, which complicates the
  application optimization. Hence one end up with optimization working
  well on one supercomputer and bad on another one as the code is
  specific to one platform and porting applications is extremely time
  consuming and can also be very tricky.  
** Code generation and opportunities  
  In the last decade many attempt have been made to automate the
  generation of optimized code. Performing optimization is one of the
  primary functions of compiler. They are capable of detecting
  instructions that can be vectorized or reorganized to favor
  super-scalar execution. They 
  are also capable of many loop optimizations such loop unrolling to
  improve pipeline efficiency,
  nest transformation to improve data locality and expose parallelism,
  etc... It exists many other optimization opportunities but it is to
  difficult to exploit them automatically because the compiler does
  not have the necessary information at compile time. Moreover, it is
  sometimes necessary to rewrite the code in a slightly different way
  to enable the compiler to be effective. That is why frameworks such
  Orio\cite{Hartono:2009:AEP:1586640.1587666} 
  for source-to-source transformation have been developed. This
  approach generally use annotations to describe the transformations
  to apply. It
  allows to bring user's knowledge in the process of generation of an
  optimized code. The main advantage of such approach is that the
  original code is left unchanged, which is generally highly
  appreciated by the application developer. The drawbacks are that the
  it is restricted to one language because the input and output
  languages are the same therefore it is difficult to exploit different
  accelerators. Also it does not allow operations that would change the
  data structure layout such as transposing a matrix beforehand. The
  meta-programming approach goes further by giving more flexibility to
  the programmer as it provides a higher level of abstraction. It
  consists in using high level languages to descriptions the
  computation and the optimizations. This allow the programmer to
  propose optimizations that the compiler would not be allowed to
  do. But it requires to rewrite the application.   
** Optimizing: the auto-tuning approach
  Whatever the chosen approach, the application developer is left with
  numerous optimization options: there are the compiler flags, code
  generation parameters (e.g., the size of the a tile, block or
  vector). Each combination of parameters leads to unique binary code
  whose performance has to be evaluated. The auto-tuning approach considers
  all this as a huge optimization problem and tries to find the best
  combination of parameters. The search space can be huge, and an
  exhaustive search is thus prohibited. Hence many techniques have been
  used in the literature such as genetic algorithm, simulated
  annealing, tabu search or machine learning. But these  methods have
  several limitations. First, the number of combination 
  tested is often large, thus the time to perform the optimization can
  still be very long. In addition with such fully automated
  approaches, it is difficult to know whether further optimizations
  could be expected or not and how to get them. Because it is
  complicated to estimate the quality of an optimization. Comparing to
  the peak performance is generally meaningless and it is hard to know
  how the combination really performed because the best optimization
  is unknown. As a result the user is excluded from the tuning process
  by the lack of valuable feedback.
** Our Goal
   The idea we explored in this internship was to give some power back
   to the user by investigating the design of a semi-automatic
   optimization framework, where the application developer has control
   and understands how the optimizations works and guides it. For
   this, we relied on BOAST\cite{}, a metaprogramming framework in
   ruby that can generate portable code in C, Fortran and OpenCL. It
   provides a domain specific language to describe the kernel and the
   optimizations and embeds a complete tool chain to compile, run,
   benchmark and check the validity of a kernel. We investigate
   various statistical techniques inspired from the design of
   experiments that emphasizes on reducing experiment cost.
** My contribution
   My contribution during this internship was to prototype and
   evaluate an approach that takes into account a performance model
   hypothesized by in order to guide the tuning process.  

   Our approach consists in the following steps:
   1. Propose a model
   2. Explore the search space at very specific places that allow to
      evaluate the quality of the model. 
   3. Find the more accurate and simplest model by refinement and
      removing useless factors
   4. Determine the optimal parameters of the refined model and
      restrict the search space accordingly.
   5. Back to 1 until we are able to fix all the factors values.
   
   In a first time, we wanted to see if simple linear regression was
   suited to the code performance modeling problem. 
   # To model 
   # computer phenomena, linear models are generally enough to get
   # accurate prediction because the models are not too 
   # complex. 
   We tested this approach on a simple kernel that computes 
   the Laplacian of an image. 
   # We found that the linear regression is
   # able to be accurate enough while having simple models that traduce
   # how the different optimization parameters can acts. However we also
   # figured out regression of expectation is not suited with current
   # architectures as it was the case two decades ago\cite{}. Regression
   # of expectation suppose that our data are homoscedastics and follows
   # the same error law. There are no guaranty about it, thus there are
   # cases where the minimum does not follow the same evolution as the
   # mean. As we are interested at the minimum value the regression of
   # expectation cannot  be used to model the evolution of the minimum
   # when the data are heteroscedastics and do not follow the same error
   # law. Hence to circumvent this burden, quantile regression seemed
   # more suited, and we tried to use it in our initial approach. 
   # Quantile regression created some additional difficulties compared
   # to the standard linear regression. We use the iterated weighted
   # least squared to compute it. Even if the coefficient computed are
   # accurate, the main concern with this methods is that it is
   # difficult to make inferences because we have biased 
   # R-squared and standard error. This was mainly problematic for the
   # validation and refinement of the model.

   In a second time, we try find a way to reduce the number of points
   needed for checking and instanciating the model. To do so, sampling the
   search space correctly is crucial. That is why, we investigated
   techniques inspired from the design of experiments.  

   # One important point to find correct model is that the model and the
   # sampling should start with the least underlying hypothesis as
   # possible because over-specification could induce some biais. The
   # idea is to start with generic model and strategy such a LHS design
   # and when we have some certainty about the model, try more specific
   # sample by adding point with a D-Optimal design.

** Structure  of the report
   The second part of this report exposes context of this work. The
   third part describes the problem of the optimization auto-tuning
   problem. The fourt part presents the state of the in auto-tuning
   and design of experiments. The fifth part exposes the how this
   work was made. The sixth part explains the approach we used. 
   The seventh part explains how measurements was made. The eigth part
   shows the results we manage to have compared to other
   techniques. The nineth part provide a detailed analysis of the
   results. And finally the tenth part show what can be done to
   improve our process and results.
* TODO Context
** HPC architectures
  HPCs are complex machines and it is not straightforward to use them
  correctly. Indeed a not carefully tuned code is likely to
  have poor performances. Optimizing the code correctly by taking into
  account the characteristics of both the application and the machine
  can bring major speedup and increasing 
  the performance by a factor 10 is not rare. The current trend in HPCs is to
  have CPUs with an ever increasing amount of cores to
  reduce the frequency in order to reduce the power consumption and
  the heat. Thus to obtain performances it is mandatory to exploit
  correctly the parallelism of the platform. The computation has to be
  described in a parallel way. Translating automatically a sequential
  application into a parallel one generally brings poor
  performances. Hence, the developer has to define which are the parts that
  can be performed in parallel and how they are parallelized. The code
  has to be written in a way such that the work is *distributed among all*
  *the cores* available and keep them busy when I/Os occur to have the
  less possible cores idle. It is important to use the correct amount of
  *threads*. Too many threads often leads to more overhead due to the
  management of the threads. Too little and all the cores are not
  exploited correctly. Also the more the threads are independent from
  each other, the better, which means there should be as little
  synchronization as possible.  

  *Pipelining* is another kind of parallelism in which the treatment of
  instructions is split into a sequence of steps (fetch, decode,
  execute, etc...) and goes through a pipeline. Multiple instructions
  can be in the pipeline at the same time but only at different state
  of the processing, like in an assembly line. A correct scheduling of
  the instructions in the pipeline leads to a better occupancy of
  it. *Instruction Level Parallelism* is a mechanism that can
  change the order of the instructions to have a better overlapping of
  instructions in the pipeline. But for example conditions are a
  disaster because it can hinder this. In addition some
  CPUs have vector support. Such processors can manipulate not only
  scalar variables but also directly *vector* variables. The vector is loaded
  into a vector register and the same instruction is applied on the
  entire vector. This saves the cost of decoding the same instruction multiple
  times.

  Today, CPUs have become so fast that one of the main bottleneck is
  the memory. Thus accessing data in memory is much more expensive than
  performing computations. To deal with this problem the solution
  found is to use different *memory hierarchy*. The statement is that
  a data that is currently used is more likely to be re-used in a near
  future. Hence the idea is to keep that are the most frequently used
  data as close as possible to the CPUs, that is why CPUs embeds cache
  memories. In a processor there can be up to 4 level of cache
  (the registers, L1, L2, L3) and the closest to the
  CPU have the lowest latency possible but are also the smallest.
  Thus the pattern to access data has to be chosen carefully so that
  the most used data stay close to the computation units.

  Another recent characteristic of HPCs is the increasing use of
  *GPUs* because for computation that can be well parallelized which
  is the case generally the case with scientific computation, they
  are faster than CPUs. However GPUs do not work exactlty the same as
  CPUs and need to be programmed in a quite different manner, the
  cache and number of cores are different.
  # the architectures of the GPUs is
  # different from the one of the CPUs, the amount of cache memory is
  # limited and there are more compute units (for example the Nvidia
  # Tesla K40 has 2,880 cores). 
  # Thus optimizations that bring good
  # performances on CPUs may bring poor performances on GPUs such the
  # size of the vectors or the number of threads.

   #+BEGIN_LaTeX
   \begin{figure}[tbh]
   \centering
   \includegraphics[width=.8\linewidth]{./img/performance_platform_correlation.jpg}
   \caption{\label{fig:correlation}Performances correlation accross platforms}
   \end{figure}
   #+END_LaTeX

  As we saw, optimizing code for HPC applications can be very
  chalenging but porting applications accross platforms is even
  harder and highly time consuming as the optimization are low level
  and HPCs can be very different and complex. Optimization that gives
  good performances on one platform may not work so well on another
  \ref{fig:correlation}. As developers cannot spend months to port the
  application on another machine it is necessary to use tools that
  facilitate the porting and the optimization of scientific
  applications.  

** Obtaining efficient code
*** Compilation
    Many works have been made around compilers to optimize the code
    automatically. They are able to modify the order of the
    instructions to find better sequences to maximize the occupancy
    of the pipeline. In addition, automatic parallelism techniques
    are able to find sequential code that can be vectorized or
    multi-threaded. They can also perform loop transformation to
    reduce the overhead of loops, henance data locality and facilitate
    the parallelization using loop unroll / nest transformation
    techniques. But this require to still write the code with care to
    ease the job of the compiler. For instance automatic parallelism
    is difficult to apply when there are global or shared variable,
    indirect address are used, etc... Further more, compilers generally
    do not have global vision of the code and lack informations of
    compile time and thus perform only local optimization. In
    addition, they also can be limited by semantic rules. As a result,
    they are not able to evaluate which transformation to choose among
    all correct transformations and they just take the one that is
    semantically correct.
*** Source-to-source transformation
    Source-to-source transformation frameworks ease the task of both
    the developer and the compiler by taking a source code, working on
    Abstract Syntax Trees and applying transformation such automatic
    parallelization to generate a modified version of the original
    code. Unlike with compilers, the developer can specify how he
    wants the transformation to be done, for instance how many time
    the loop is unrolled. Then the framework ensures that the
    transformation is valid and generates a code that the compiler can
    easily work with. This relieve the compiler from the complicated
    tasks such the loop transform or the automatic parallelization and
    this gives the possibity the user guide the transformation by
    giving more information. The disadvantage of such tools it that
    they generally target one language and one compiler and can be
    still limited by semantic rules.  
*** Meta-programming: BOAST
    Meta-programming is a slightly different approach from
    source-to-source transformation in which the developer use high
    level language to make a description of his kernel and the
    possible optimizations (e.g. the size of a vector, the tiling,
    etc...). The advantage is that it is not linked to one output
    language or compiler. It also gives more control to the user as
    there is no checker that verify the correctness of the
    transformation, thus he can exactly specify how the transformation
    is performed. Hence, the developer has to know what he is doing
    and it can be error prone. In this work we used the
    meta-programming framework BOAST\cite{videau:hal-00953119}. BOAST gives the ability
    to user to meta-program his kernels in ruby with a Description
    Specific Language (DSL), then BOAST can generate it in many target
    languages (C, Fortran, Cuda, OpenCL), compilate it and benchmark
    the resulting executable.  
** Problem analysis
  In a word, optimizing HPC applications is tricky and porting is even
  more difficult but tools exist to assist the developer in this
  complicated task. However a major problem remains, generally the
  developer know what should be vectorized or what should be
  parallelized but he does not know what is the best size of the 
  vector or the best number of threads or what is the combination of
  compilation flags that brings the best speedup. This problem consist
  in tuning correctly the different optimization parameters of the
  applications. 

  The tuning of applications is a non-trivial problem, because the
  search space of the different combinations of parameters can be
  huge. For instance there are about 500 compilation flags for GCC and
  testing all the combinations (i.e, 2^500 ~ 10^50 combinations) to find
  the best one is simply impossible. Thus
  it is formulated as a mathematical optimization problem where the
  optimization function gives metrics of combination of parameters
  \vec{x}.  
  
  #+BEGIN_LaTeX
  \( \displaystyle\min_{x} {f(\vec{x}): \vec{x} \in \mathcal{D} \subset \!R^{n} } \)
  #+END_LaTeX  

  This function is empirical because the performances of a
  combination cannot be computed, measurements have to be done to
  evaluate the objective function at point x. It needs to generate the
  code variant, compile it and run it. Sometimes the problem can have
  constraints because some points are unfeasible, this means they
  cause the compilation to fail or the program to crash. In addition
  parameters can be discrete and continuous. 
 
* TODO State of the art
** Auto-tuning
   In auto-tuning one can two major categories of approaches. Some
   have focused on the use of machine learning techniques to build
   models make predictions. While others have worked more around the
   optimization side to find more suited search techniques that are
   able to find the near-optimal solution by exploring as little
   points of the search space as possible. 
*** Machine learning
   This technique is generally used to identify category of programs
   that have the same characteristics by building models over large
   training sets, and to determine what is the best action to apply
   for this category of programs. Thus, there is a will of
   generalization, the knowledge is reuse from previous experiences. 

   This approach has been proven successful by the project Milepost
   GCC from Grigori Fursin\cite{fursin:hal-00685276}, which is now part of GCC. He used
   machine learning to learn characteristics of programs and the
   distributions of combinations that gives the most speedup. The idea
   is that good performing combinations have high probability to bring
   good speedup for similar programs. This allowed to reuse knowledge
   across programs.
   
   Stefan Wild et al. focused porting of optimization between similar
   platforms\cite{RoyBalHovWil2015}. They study the correlation between platform and
   the performance of combinations parameters. They used machine
   learning to build performance model of platform and this model
   to approximate performance of another platform. The more the
   combinations performance are correlated between two platforms the
   more the accurate the predictions. They managed to find correlations
   between intel CPU, IBM Power but this approach fails with too
   dissimilar platforms (ARM in their case).

   As efficiency of a search strategy is dependent on the structure of
   the search, machine learning can be used to learn what search
   methods to use according to the characteristics of the search
   space. That is the approach taken by the auto-tuning framework
   Opentuner\cite{Ansel:2014:OEF:2628071.2628092}.  

   The main drawback with machine learning techniques is that they
   need to be trained on a large amount of instances to be effective
   enough. To mitigate this problem, some, such the framework
   Nitro\cite{Muralidharan:2014:NFA:2650283.2650550} uses active
   learning to distribute the training overhead.

   Another approach is to distribute the training overhead over the
   different users, it is called
   crowdtuning\cite{memon:hal-00944513}. Informations are collected in
   a shared database and machine learning is applied to continuously
   update the model. 
*** Optimization techniques
****                                                               :noexport:
     #+begin_src R :results output graphics :file ./img/convex_function.pdf :exports results :width 8 :height 6 :session
       library(polynom)
       plot(poly.calc(1:2), xlim=range(-10:10))
     #+end_src

     #+RESULTS:
     [[file:./img/convex_function.pdf]]

     #+begin_src R :results output graphics :file ./img/non_convex_function.pdf :exports results :width 8 :height 6 :session
       library(polynom)
       plot(poly.calc(-1:5))
     #+end_src

     #+RESULTS:
     [[file:./img/non_convex_function.pdf]]

     #+begin_src R :results output graphics :file ./img/non_smooth_function.pdf :exports results :width 8 :height 6 :session
       library(polynom)
       f <-function(x){
         ifelse(x < 2, abs(x), x+1)
       }
       
       plot(f, xlim=range(-5,5))
     #+end_src

     #+RESULTS:
     [[file:./img/non_smooth_function.pdf]]

     #+begin_src R :results output graphics :file ./img/auto_tuning_function.pdf :exports results :width 8 :height 6 :session
       library(polynom)
       
       g <- function(x){
         ifelse(x > 1 & x < 2, NA, as.function(poly.calc(-3:5))(x) + rnorm(1,sd=.9)*1000 )
       }

       plot(g, xlim=range(-3.05,3.05))
     #+end_src

     #+RESULTS:
     [[file:./img/auto_tuning_function.pdf]]



**** 
   Many optimization techniques are applied to the auto-tuning
   problems. Some of them use the derivatives such gradient
   descent which is a kind of local search techniques. It exploits the
   locality of the search space and has particularity to converge
   quickly to a the optimal solution but it requires that the search
   space has a specific geometry and convexity of the objective
   function\ref{fig:convex_function}. But these hypothesis are not
   necessarily true. The objective function may not be convex
   \ref{fig:non_convex_function}, hence with many local optimum and a
   local search would be stuck in a local optimum. The problem is that
   local optimum can be far from the global optimum. That is why, to
   escape from this, global search and randomization are more suited
   such the simulated annealing, or genetic algorithm (kind of
   multiple local search). The derivative may also be not available,
   for this reason, derivative based searches are
   inefficient\ref{fig:non_smooth_function}. The auto-tuning problem
   combines all these characteristics\ref{fig:auto_tuning_function}.     


   #+CAPTION: Objective function characterics
   #+LABEL: fig:obj-func-ex

   #+BEGIN_LaTeX
   \begin{figure}[htb]
   \centering
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/convex_function.pdf}
   \caption{\label{fig:convex_function}Convex function}
   \end{minipage}
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/non_convex_function.pdf}
   \caption{\label{fig:non_convex_function}Non-convex function}
   \end{minipage}
   \end{figure}
   \begin{figure}[htb]
   \centering
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/non_smooth_function.pdf}
   \caption{\label{fig:non_smooth_function}Non-smooth function}
   \end{minipage}
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/auto_tuning_function.pdf}
   \caption{\label{fig:auto_tuning_function}Objective function in auto-tuning}
   \end{minipage}
   \end{figure}
   #+END_LaTeX

   Another concern is that, the objective function is an empirical
   function, hence it can be necessary to build a surrogate. This is
   usefull to remove the noise and as a result it facilitates the
   search. Also as the derivative estimation may not be always possible and
   derivative-based searches cannot work, the alternative is to use
   derivative-free based searches such as Nelder Mead Simplex.
   The previous search methods are used in
   Orio\cite{Hartono:2009:AEP:1586640.1587666}, a source to source 
   auto-tuner. It uses random search and simulated annealing as global
   search methods and Nelder Mead Simplex as local search. 

   The efficiency of the previous approach is highly dependent on how
   much the hypothesis about he search space are wrong and sometimes
   it is difficult know how it looks. For this reason some have worked
   on generic heuristics that combine all or part of the previous aspects
   such as pattern search\cite{Hooke:1961:DSS:321062.321069} which is
   a derivative-free based search that combines global search that
   explore the space in a finite set of direction to find  
   regions of interest and local search to examine regions of
   interest. This kind of methods allow to make less hypothesis and
   require less knowledge about the search space. This approach has
   been used in OPAL\cite{orban2011templating}, a meta-programming
   framework. It uses the mesh-adaptive
   direct-search\cite{Audet04meshadaptive}, it is an extention of the
   pattern search. It can explore in an infinte set of directions
   unlike pattern search and use derivative information when available
   to speedup the search.

   While some people developed framework to characterize the search
   space such as ASK\cite{deoliveiracastro:hal-00952307} in order to
   have a better understanding of it. This tool emphasis on the
   sampling because it is crucial for build an accurate model. It
   provides a complex sampling pipeline with different surrogate
   methods (Bayesian regression, interpolation, etc...)

   In some cases, the problem structure is well know and one has an
   idea of where is the optimal solution but the structure of the
   space in this neighborhood is too complex. The approach
   taken in Atlas \cite{Whaley:1998:ATL:509058.509096} is to focus
   only in one part of the search space to perform an exhaustive
   search. But this require know the problem well and where to search.

   In general auto-tuners exclude the user from the optimization
   process. It means that it is difficult for him to know if the
   result can be further improved, and has no clue about the quality
   of the solution. Our goal is to give more feedback and control
   through an semi-automatic and interactive approach to
   the user to guide him in the tuning the his application. Our
   approach is global and allow the user to evaluate the relevance of
   his hypothesis. We the feedback provided he is able to prune the
   search space to allow very low cost optimization.

   In the past a similar approach have been tempted by
   Brewer\cite{Brewer:1995:HOV:209937.209946} where linear regression
   of expectation have been used for the modelization to predict the
   objective function. It worked fine platform CM-5, simulated version
   of Intel Paragon and network of station based on FORE ATM, but
   these platforms are pretty old. To our knowledge this approach has
   not been used recently in the tuning of applications, we wanted to
   understand why and see if it is suited to the complexity of the
   current platforms.

** Design of experiments
   When there are lots of factors, covering the entire space of
   possible values is prohibitive. The goal experimental design is to
   build experiments in order to study the behavior of a system for a
   low experiments cost. For this reason many techniques has been
   developed to sample the space wisely.
 
   The efficiency of One-Factors-At-a-Time (OFAT) is the method of
   changing one factor at a time when the others are kept fixed. It is
   very limited when there are many factors, because it requires high
   number of experiments and it cannot find interactions between 
   factors. For these reasons factorial designs are generally more
   suited. They vary many parameters at the same time, hence
   interactions can be trapped, the estimate of the impact of the
   factors is more precise with a lower experiment cost.
  
   There are different kind of factorial designs. The first one is the
   full factorial design which consider the entire space. The simplest
   way of doing full factorial design is to chose points in the space 
   uniformly\ref{fig:doe-examples}. The drawback is that the points
   are not well distributed, there are part of space where there are
   lots of points and some other where there just few. The Latin
   Hyper-cube Sampling design provides a better coverage of the space
   by dividing the space into pieces of equal sizes and taking the
   same number of points at random in these areas. This method is made
   for continuous factors.  

   On other kind of factorial designs is the fractional factorial
   designs. Instead of considering the whole space it consider only a
   part of it. This part is chosen according to the statement that
   main effect and low order interactions (Sparsity of effect
   principle) are enough to explain the system. One of them is the
   screening design that consider only the lowest and highest values
   for factors.

   Optimal design is another category of factorial design. It samples
   the space such way that the variance is minimum, hence the
   estimation of the factors as the minimum bias. The points are taken
   according statistical model that means that means that the model
   must be already known. The advantage of optimal designs over
   non-optimal is that the need less experiment, as the sampling is
   localized. The D-Optimal design is one of them, it chooses the
   points such that the generalized variances of the least squares
   estimate of a model is minimized.
    
   The tuning of applications is in fact running multiple experiences in an
   automated or semi-automatic process. We thing that techniques from
   experimental design can help us to sample the space efficiently to
   achieve the optimization with low experimental cost.

   #+CAPTION: Space coverage by different DoE
   #+LABEL: fig:doe-examples
   #+begin_src R :results output graphics :file img/DoE_examples.png :exports results :width 600 :height 400 :session
     library(DoE.base)   
     library(DoE.wrapper)   
     library(ggplot2)

     library(grid)
     library(gridExtra)

     random <- data.frame(X1=runif(200,0,4),X2=runif(200,0,4))
     lhs <- lhs.design( type= "maximin" , nruns= 200 ,nfactors= 2 ,digits= NULL ,seed=20049 , factor.names=list(X1=c(0,4), X2=c(1,4) ) )
     Dopt <- Dopt.design(50, data=lhs, formula="~ X1 + X2 + I(1/X2)", nRepeat=5, randomize=TRUE)

     p1 <- qplot(data=random) +
         geom_point(aes(x=X1,y=X2)) +
         ggtitle("Random")

     p2 <- qplot(data=lhs) +
         geom_point(aes(x=X1,y=X2)) +
         ggtitle("lhs")

     p3 <- qplot(data=Dopt) +
         geom_point(aes(x=X1,y=X2)) +
         ggtitle("D-optimal")
         
     grid.arrange(p1, p2, p3, ncol=2, top=textGrob("")) 

   #+end_src

   #+RESULTS:
   [[file:img/DoE_examples.png]]

* DONE Methodology
** DONE Reproducible research
  Such experimental process mandate rigorous methodology.
  In order for this work to be usefull for someone else a laboratory
  book is available publicly on
  github\footnote{https://github.com/swhatelse/M2\_internship}. It 
  contains detailed about installation and configuration steps. It
  keeps tracks of every experiments including their description and
  analysis. Now it has more than 27K lines with more 14K lines of code
  and analysis. It is structured in a chronological way and thus
  follows the natural evolution of the work. This gives the possibility to
  easily understand what has been done at each step and why.
  Every pieces of codes I wrote is explained using literate programming, which
  is straightforward using the org-mode of emacs.
  The github repository also contains the complet set of scripts and
  data used for experiments giving the possibility to anyone to re-run
  the same experiments using the same data.

** DONE Case study
   # Maybe cite Brice paper for this part
   In order to elaborate our approach, we took a very simple example
   which is a kernel that computes the Laplacian of an image. We want
   the minimize the time to compute a pixel and there are multiple
   optimization that can be done to enhance the performance of this
   kernel. The parameters and their values we used to tune this
   applications are the following: 

     | Parameters         | Values                            |
     |--------------------+-----------------------------------|
     | /                  | <                                 |
     | vector length      | 1,2,4,8,16                        |
     | load overlap       | true,false                        |
     | temporary size     | 2,4                               |
     | elements number    | from 1 to 24                      |
     | y component number | from 1 to 6                       |
     | threads number     | 32,64,128,256,512,1024            |
     | local work size y  | 1,2,4,8,16,32,64,128,256,512,1024 |

   The first parameter vector length allow to specify the size of the
   vectors used to performs the computation. The Laplacian can be
   easily vectorized and on hardware that provides vector support 
   it allows to save some decoding phase as the same instruction is
   applied to the entire vector. As each architecture have different
   size of vectors, and some do not provide vector support we need to
   try the different values of vector size.

   # Not satisfying yet
   The second parameter is related to the vectorization. As vectors
   are manipulated, when loading, some data overlap. Thus it is
   possible to synthetize the load from other data and consequently
   reduce the number of loads. 

   The third parameter allows to specify the size of the variables used
   for storing intermediary results. Using smaller type can reduce the
   pressure on the registers but casting variable can also be
   harmfull. Hence the default size is int (4) and we can also use
   short (2). 

   The fourth parameter splits the image into pieces of the size of
   elements number. This specifies the of component a threads will
   process, that is the amount of work per thread, and as a
   consequence defines the number of threads used to perform the 
   computation. More threads can lead to better parallelism but also
   more overhead due to the bigger number of threads to manage. 
   
   # Not satisfying yet
   The fifth parameter is used to specify how the work for a thread is
   organized by specifying the tiling. It gives how the components of
   the image are distributed in the y-axis.

   Finally the two last parameters allow to tune two OpenCL/Cuda
   parameters and describe the distribution of the work at coarse
   grain. In OpenCL and Cuda, threads are grouped and scheduled 
   by blocks on a compute unit. Which means that threads are not
   scheduled individually but by blocks. Thus we use the parameter
   threads number to specify the size of a group. The parameter local
   work size y determines how the threads are organized in a block and
   represent the number of threads in the y-axis. These parameters
   have an impact on the scheduling, the data sharing and the
   occupancy of the compute units. Thus they can lead to better usage
   of the resources.

   All the combinations of these parameters would gives a search space
   of 190080 points. However some points are unfeasible. For instance,
   having more component numbers in the y-axis (y component number)
   than number of component (elements number) itself makes no
   sense. We also have constraint the size of the kernel because it is
   limited to the available resources on the device. Exceeding the
   resources cause the kernel to crash. That is why use constraints to
   reject all the that would produce a kernel to big or that is not
   correct. Finally it remains 23100 points in the search space.

   The experiments are run on one machine with GPU Nvidia K40 using the
   driver 340.32 and two CPUs Intel E5-2630.
** DONE Controlling measurement
   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data_detailed_v2.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file img/warm_up.pdf :exports none :width 8 :height 8 :session
     library(plyr)
     library(ggplot2)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     d2 <- df[df$lws_y == 2 & df$elements_number == 1 & df$threads_number == 32,]
     
     df2 = ddply(d2,.(run_index,vector_length,image_size_index), summarize, 
                      mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))
     
     
     ggplot(d2) +
          geom_line(aes(x=run_index, y=time_per_pixel, color=factor(load_overlap),linetype=factor(temporary_size))) + 
          geom_point(aes(x=run_index, y=time_per_pixel, shape=factor(load_overlap))) + 
          theme(legend.position="top") + 
          facet_grid(vector_length ~ image_size_index, scales="free_y", labeller=label_both) 
   #+end_src

   #+RESULTS:
   [[file:img/warm_up.pdf]]
   
   Current hardware has became more and more complex and provides
   features such that power saving, frequency scaling, etc... Thus it
   is possible to have measurements that are different from an
   experiment to another even if the set of inputs is the same exactly the
   same. For instance, frequency scaling mechanism could chose to scale
   down the frequency of the CPU because of the temperature inside the
   computer case has increased which would have an impact on the
   compute time. To have trusted measurements we are concerned about
   kind of problems because the metric in our case which is the time to
   compute a pixel, is sensitive to this. Thus we have to protected
   against variability between the same measurements and especially
   the warm-up effect. This phenomena can occurs on devices providing
   energy saving features. This kind of devices generally have a
   performance mode and an idle mode. As long as the device does not
   have a lot of work it stays in idle mode but at a some threshold it
   switches to the performances mode. Thus the device does not provides
   all its capabilities immediately, hence the warm-up effect.

   The measurement process is made as follows:
   1. Generation of the next a version of the code
   2. Compilation
   3. Bench-marking on several image sizes multiple times.
  
   As the code is executed on a GPU, the latter has no work to do
   during the code generation and compilation phases. For this reason
   we suspected that warm-up effect can occurs at this moment and also
   after an image is loaded. We tried to see if on the GPU Nvidia K40
   there this effect is present. We also tried to quantify it along
   with the variability we could have between the different runs of
   the same version of the code in order to protect against it. The
   figure\ref{warmup}, illustrates what we expected, there is a power
   saving mechanism on Nvidia K40 which turns the GPU into idle mode
   when the computational intensity is bellow a threshold. This effect
   occurs on the first size of image tested, which is just after the
   code generation and compilation phases. The more run are performed the
   better the performances. It also could have been the case when
   going from one image size to another, the GPU could have switched
   to idle mode while the loading of the image but is not the case the
   GPU does not have the time to switch to idle mode. So prevent to
   protected against warm-up effect we just need to make at least four
   runs on the first size of image and we keep the run the gives
   minimal time to compute one pixel. However we also did the same
   four all the size of images. An other concern is the variability
   between multiple execution of the same version of code but as we
   can see, it is only due to the warm-effect in the first image
   size. On the other size of images we have almost no variability.

   #+BEGIN_LaTeX
   \begin{figure}[htb]
   \centering
   \includegraphics[height=.5\textheight]{./img/warm_up.pdf}
   \caption{\label{fig:warmup}Warm-up effect}
   \end{figure}
   #+END_LaTeX
* DONE Envisioned general approach
   When using fully automated tools, the user has no feedback about
   the optimization process and does not have a lot of control. How
   good is the optimized version of the code? How is it possible to
   improve it? What does the search space look like? What are the
   parameters that have a big impact (high-order parameters) and those
   which have a small impact (low-order parameters)? All this
   questions are necessary to understand the structure of the
   problem and provide valuable information for the expert to be able
   to prune the search space correctly and to choose the most suited
   search techniques. Thus we investigated the design of a
   semi-automated approach where the user tunes his application in an
   interactive way. All along the tuning process this method provides
   valuable information to user to guide him and exploit his
   knowledge. Of course, this assumes that the application developer
   understands well his kernel and knows the reason each code
   optimization he used.  

   As objective the function is empirical and is costly to evaluate, our
   approach consist in sampling the space with only few points to
   build a model in order to approximate it at low cost. We focused
   on linear regression because usually, it is enough to model
   accurately computer phenomenon. However a correct modeling goes
   to together with efficient sampling techniques. That is why we used
   techniques inspired from design of experiments where the goal is to
   maximize the amount of information and minimize the number of
   points.

   The figure\ref{fig:process} show the work-flow of our approach: 
   1. The user interrogate the search space for example to find what
      are the parameters that have the most impact and their
      interactions, check his hypothesis about the model, etc...
   2. The search space is sampled by taking into account the objective
      of the user. For instance if the user wants to have a first
      overview of the high-order parameters or if he wants to refine
      his model or if he needs to obtain more information about a precise
      part of the search space.
   3. Using linear regression a model is built based on the hypothesis
      provided by the user. It also determines what are the parameters
      that have the most impact. Parameters that have less impact are
      removed from the model and will be re-injected later when higher
      order parameters are fixed.
   4. The best value for the studied parameters are predicted from the
      model.
   5. The result of the regression and estimated best value for the
      parameters are return to the user. At this step, either he is
      satisfied by the result of the regression and he can prune the
      search space by fixing the parameters to the estimated values.
      Or he can ask to test another model, ask more points in a
      particular area to refine the model, etc...
   6. This process iterate until all parameters are fixed.

   In short the tuning is done through an iterative and instrumented
   process where the user refine is model according to the extracted
   information.
     
 
   #+BEGIN_LaTeX
   \begin{figure}[tbh]
   \centering
   \includegraphics[width=.8\linewidth]{./img/process.pdf}
   \caption{\label{fig:process}Workflow}
   \end{figure}
   #+END_LaTeX
* DONE Results
***                                                                :noexport:
    #+begin_src sh :results output :exports none
      ruby ../../../scripts/format_data.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_cleaned.yaml 
    #+end_src

    #+RESULTS:

    #+begin_src R :results output graphics :file ./img/results_hist.pdf :exports none :width 6 :height 8 :session
      df_all_methods <- read.csv("../../../data/2016_04_08/pilipili2/18_08_24/all_search_1000.csv", strip.white=T, header=T)  
        library(ggplot2)
        library(plyr)

        df_mean = ddply(df_all_methods,.(method), summarize, 
                        mean = mean(slowdown))

        df_median = ddply(df_all_methods,.(method), summarize, 
                          median = median(slowdown))

        df_err = ddply(df_all_methods,.(method), summarize,
                      mean = mean(slowdown), err = 2*sd(slowdown)/sqrt(length(slowdown)))

        ggplot(df_all_methods ) + 
            facet_grid(method~.) +
            theme_bw() +
            coord_cartesian(xlim=c(.9,3), ylim=c(0,1000)) +
            geom_histogram(aes(slowdown),binwidth=.05,color="white", fill="gray48") +
            geom_rect(data = df_err, aes(xmin=mean-err, xmax=mean+err, ymin=0, ymax=1000, fill="red"), alpha=0.3) +
            geom_vline( aes(xintercept = median), df_median, color="darkgreen", linetype=2 ) +
            geom_vline( aes(xintercept = mean), df_mean, color="red", linetype=2 ) +
            labs(y="Frequency", x="Slowdown compared best combination of the entire search space") +
            scale_fill_discrete(name="",breaks=c("red"), labels=c("Mean error")) +
            ggtitle("") + 
            theme(legend.position="top")
    #+end_src

    #+RESULTS:
    [[file:./img/results_hist.pdf]]

    #+begin_src R :results output :session :exports both
      summary(df_all_methods[df_all_methods$method == "RS",]$slowdown)
      summary(df_all_methods[df_all_methods$method == "LHS",]$slowdown)
      summary(df_all_methods[df_all_methods$method == "GR",]$slowdown)
      summary(df_all_methods[df_all_methods$method == "GRM",]$slowdown)
      summary(df_all_methods[df_all_methods$method == "GA",]$slowdown)
      summary(df_all_methods[df_all_methods$method == "LM",]$slowdown)
      summary(df_all_methods[df_all_methods$method == "RQ",]$slowdown)

      mean(df_all_methods[df_all_methods$method == "RS",]$point_number)
      mean(df_all_methods[df_all_methods$method == "LHS",]$point_number)
      mean(df_all_methods[df_all_methods$method == "GR",]$point_number)
      mean(df_all_methods[df_all_methods$method == "GRM",]$point_number)
      mean(df_all_methods[df_all_methods$method == "GA",]$point_number)
      mean(df_all_methods[df_all_methods$method == "LM",]$point_number)
      mean(df_all_methods[df_all_methods$method == "RQ",]$point_number)
    #+end_src

    #+RESULTS:
    #+begin_example
       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      1.000   1.027   1.079   1.102   1.178   1.388
       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      1.000   1.087   1.188   1.175   1.244   1.524
       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      1.000   1.348   1.799   6.460   6.314 124.800
       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      1.000   1.072   1.187   1.227   1.333   3.164
       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      1.000   1.018   1.086   1.118   1.187   1.646
       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      1.012   1.012   1.012   1.022   1.012   3.771
       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      1.012   1.012   1.012   1.019   1.012   2.064
    [1] 120
    [1] 98.918
    [1] 22.171
    [1] 120
    [1] 120
    [1] 119
    [1] 119
#+end_example

*** 
    To evaluate our solution, we compared it against the following search
    methods that have already been used in auto-tuning:
    - RS: is the uniform random search that take points randomly in the
      search space with equal probabilities. 
    - GA: BOAST embeds an implementation of a genetic algorithm. We
      used a population size of 20, number of generations of 5 and
      mutation rate of 0.1. Among the different configuration tested
      it was the one that gives the best results, however it could be
      possible to obtain better results by tuning further the options.
    - LHS: it is not methods use to search, it is a sampling
      techniques which take point randomly but which also maximize the
      distance between the point to cover the full search
      space. However we want to see how a search based on it would
      perform. 
    - GR: we implemented a greedy search which is a derivative-free
      local search. From a starting point it explores all the possible
      directions at distance one and goes to the direction that gives
      the best improvement. This kind of algorithm is very efficient
      on convex objective function or if we already know where to
      search.
    
    There are many ways of performing linear regression. We evaluated
    two of them: 
    - LM: it uses the least square regression which gives an estimate
      of the mean.
    - RQ: it uses the quantile regression which gives an estimate of
      a given quantile.

    We measured each methods one 1000 time with about 120 (0.5% of
    the search space) points for GA, RS, and LHS and we
    evaluated the slowdown achieved compared to the best solution
    available in the entire search space. For GR only 1 random point
    is chosen as starting point.

    We evaluated our approach using the random uniform sampling
    techniques to sample the search space. It starts with 50 random
    points and adds just enough points after pruning to perform the
    regression. The total number of points used is 119. Our approach
    is intended to be semi-automatic but for evaluation purpose we
    automated the process. For this we decided to apply exactly the
    same strategy each time. We fixed the parameters in the same
    order, thus pruning decisions are the same and we used exactly the
    same model without considering the structure of the random set and
    same things for the sampling decisions.

    Figure\ref{fig:search_comparison} shows the results of the
    different methods. With this search space, the local search GR is
    inefficient, half of the time we get a slowdown of higher than
    x2. It can be every far from the optimal solution, up to x125
    slower\ref{tab:comparison-table}. Combining multiple local
    searches with random starting points greatly improves the result,
    the worst solution is never slower than a factor of 3.164 and half
    of the time the slowdown is below 18.7%. The LHS search does a
    little bit better better with a slowdown that is never higher than
    52.4%. 50% of the time we can get a slowdown lower than 18.8%.
    The uniform random search RS is very efficient here. Half of the time
    we get a slowdown that is less than 7.9% and we do not get a
    maximum slowdown of 38.8%.
    The genetic algorithm GA gives even better results with a slowdown
    which is less than 7.3% half of the time, a mean slowdown of 10.2%
    and a maximum slowdown of 38.8%. 
    The technique using least square regression is extremely efficient,
    most of the time it converges to the same solution 
    which has a slowdown of 1.2%. However it never find the best
    solution.With a mean slowdown of 2.2% it is 7 times better than
    GA. It only failed three times giving a solutions with a slowdown of
    a factor 2.064 which is worse than worst solution of the GA.
    Using quantile regression we managed to improve the results of the
    classical least square regression. Almost every time it converge
    to the same solution as with LM and the worst solution has a
    slowdown of of a factor 2.064. However like LM it does not find
    the optimal solution.

    In brief, the regression of expectation gives almost every time
    the exactly same results which is very close to the best solution
    of the entire search space but it never reaches it. With the
    quantile regression we managed to improve the worst solution but
    we could not improve the best solution return by LM. However both
    LM and RQ very rarely gives worse solution than GA or RS.
     
   #+BEGIN_LaTeX
   \begin{figure}[htb]
   \centering
   \includegraphics[width=\linewidth]{./img/results_hist.pdf}
   \caption{\label{fig:search_comparison}Search comparison}
   \end{figure}
   #+END_LaTeX

    #+CAPTION: This table shows the minimum, first quartile, median, mean, third quartile and maximum slowdown including the mean number of points used by each method
    #+NAME:   tab:comparison-table
    | Method |  Min. | 1st Qu. | Median |  Mean | 3rd Qu. |    Max. | Mean Cost |
    |--------+-------+---------+--------+-------+---------+---------+-----------|
    | /      |    <> |      <> |     <> |    <> |      <> |      <> |           |
    | RS     | 1.000 |   1.027 |  1.079 | 1.102 |   1.178 |   1.388 |       120 |
    | LHS    | 1.000 |   1.087 |  1.188 | 1.175 |   1.244 |   1.524 |    98.918 |
    | GR     | 1.000 |   1.348 |  1.799 | 6.460 |   6.314 | 124.800 |    22.171 |
    | GRM    | 1.000 |   1.072 |  1.187 | 1.227 |   1.333 |   3.164 |       120 |
    | GA     | 1.000 |   1.018 |  1.086 | 1.118 |   1.187 |   1.646 |       120 |
    | LM     | 1.012 |   1.012 |  1.012 | 1.022 |   1.012 |   3.771 |       119 |
    | RQ     | 1.012 |   1.012 |  1.012 | 1.019 |   1.012 |   2.064 |       119 |

* Analysis
  This part gives a study of the search space and an explanation of
  the results of our approach. We also explain why the quantile
  regression is more suited than regression of expectation in
  optimization purpose. In order to perform our experiment we
  automatized our approach by blindly using the same model and the
  same pruning strategy without considering the working set of
  points. This gives us an overview of how it could perform but it 
  is not intended to use like this. Hence this part explains the case
  where the prediction is made correctly and when it fails.
** DONE Characteristics of the search space
***                                                                :noexport:
   #+begin_src sh :results output :exports none
       ruby ../scripts/clean_data.rb ../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end.yaml
   #+end_src

   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_cleaned.yaml
   #+end_src

   #+RESULTS:
**** Repartition of combination
   #+begin_src R :results output graphics :file ./img/search_combination_rep_slowdown.png :exports results :width 800 :height 600 :session
     library(ggplot2)
     library(grid)
     library(gridExtra)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)

     slowdown <-  df$time_per_pixel / min(df$time_per_pixel)
     df$slowdown <- slowdown

     p2 <- qplot(data=df) +
         geom_histogram(aes(x=slowdown,y=..density.. * 0.05), binwidth=.05) +
         theme(axis.text.x = element_text(angle = 70, hjust = 1, face="bold", size=12)) +
         geom_vline(xintercept = median(slowdown), color="darkgreen", linetype=2) +
         geom_vline(xintercept = quantile(slowdown, prob=c(0.25,0.75)), color="blue", linetype=2) +
         geom_vline(xintercept = mean(slowdown), color="red", linetype=2) +
         ggtitle("Density of the combinations slowdown compared to the best") +
         labs(y="Density", x="Slowdown")

     p3 <- qplot(data=df) +
         geom_histogram(aes(x=slowdown,y=..density.. * 0.05), binwidth=.05) +
         theme(axis.text.x = element_text(angle = 70, hjust = 1, face="bold", size=12)) +
         geom_vline(xintercept = median(slowdown), color="darkgreen", linetype=2) +
         geom_vline(xintercept = quantile(slowdown, prob=c(0.25,0.75)), color="blue", linetype=2) +
         geom_vline(xintercept = mean(slowdown), color="red", linetype=2) +
         ggtitle("Density of the combinations slowdown compared to the best") +
         coord_cartesian(xlim=c(.9,17)) +
         labs(y="Density", x="Slowdown")

     grid.arrange(p2, p3,  ncol=2, top=textGrob("Repartition of the performance combination")) 
   #+end_src
**** Chance of getting the best solution
     Number of random try to get solution with a slowdown lower than
     10%  with a probability  of 0.9
     #+begin_src R :results output :session :exports both
       df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
       df$slowdown <- df$time_per_pixel / min(df$time_per_pixel)
       nrow(df[df$slowdown <= 1.1,])
       p1 <- nrow(df[df$slowdown <= 1.1,]) / nrow(df) 
       p1
       log(0.1)/log(1-p1)
     #+end_src

     #+RESULTS:
     : [1] 170
     : [1] 0.007352941
     : [1] 311.9989

     #+begin_src R :results output :session :exports both
       nrow(df[df$slowdown <= 1.012,])
       p2 <- nrow(df[df$slowdown <= 1.012,]) / nrow(df) 
       p2
       log(0.1)/log(1-p2)
     #+end_src

     #+RESULTS:
     : [1] 8
     : [1] 0.0003460208
     : [1] 6653.32

       #+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session
       x <- 1:7000
        library(ggplot2)

        d1 <- data.frame(points_number=1:7000, proba=1-(1-p1)**x, slowdown=rep(1.1,7000))
        d2 <- data.frame(points_number=1:7000, proba=1-(1-p2)**x, slowdown=rep(1.012,7000))
        d3 <- rbind(d1,d2)
        ggplot(d3) +
             aes(x=points_number,y=proba,group=slowdown) +
             theme_bw() +
             geom_line() +
             labs(y="1-(1-p1)^x", x="Number of points") +
             scale_linetype_manual(name="Slowdown",values=c("1.2%","10"))
      #+end_src

       #+RESULTS:
       [[file:/tmp/babel-36309sl/figure3630H3w.png]]

**** Best combinations
     #+begin_src R :results output :session :exports both
       df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
       w <- getOption("width")
       options(width=150)
       sorted <- df[order(df$time_per_pixel),] 
       sorted$slowdown <- sorted$time_per_pixel / sorted$time_per_pixel[1]
       head(sorted[,!names(sorted) %in% c("vector_recompute","time_per_pixel")],20)
       options(width=w)
     #+end_src

     #+RESULTS:
     #+begin_example
           elements_number y_component_number vector_length temporary_size load_overlap threads_number lws_y slowdown
     13752               6                  6             1              2         true           1024     2 1.000000
     9843                6                  6             1              2        false            256     4 1.005562
     17930               5                  5             1              2         true            256     4 1.007540
     10660               5                  5             1              2        false            256     4 1.009100
     19494               6                  6             1              2        false           1024     8 1.009796
     14258               6                  6             1              2        false            128     4 1.010568
     12261               6                  6             1              2         true           1024     4 1.011509
     22098               5                  5             1              2        false            512     4 1.011710
     17258               6                  6             1              2         true            256     1 1.012177
     6468                6                  6             1              2        false            128     2 1.012663
     11332               6                  6             1              2         true            128     2 1.013555
     21903               6                  6             1              2         true            256     2 1.013578
     12450               5                  5             1              2         true            512     4 1.013662
     16041               6                  6             1              4        false           1024     4 1.013695
     8830                6                  6             1              4         true            512     8 1.014100
     12719               6                  6             1              4         true           1024     4 1.014177
     19283               6                  6             1              2        false            256     1 1.014598
     9305                6                  6             1              2         true           1024     8 1.014882
     16588               6                  6             1              2         true            128     1 1.015087
     17018               6                  6             1              2         true            512     8 1.015301
#+end_example

**** Heteroscedasticity
   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data.rb ../../../data/2016_03_11/pilipili2/19_13_54/Data19_13_54_linear.yaml
   #+end_src

   #+begin_src R :results output graphics :file ./img/heteroscedasticity.png :exports results :width 700 :height 400 :session
     library(ggplot2)
     library(grid)
     library(gridExtra)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)

      p1 <- qplot() + 
          geom_point( aes(x=df$vector_length, y=df$time_per_pixel), alpha=0.1 ) + 
          ggtitle("Impact of the vector length") +
          labs(y="time per pixel in seconds", x="vector length") +
          theme(axis.text=element_text(size=12),
                axis.title=element_text(size=14,face="bold"))

      p2 <- qplot() + 
          geom_point(aes(x=df$x_component_number, y=df$time_per_pixel),alpha=0.1) + 
          ggtitle("Impact of number of component on the x-axis") +
          labs(y="time per pixel in seconds", x="x component number") +
          theme(axis.text=element_text(size=12),
                axis.title=element_text(size=14,face="bold"))

     grid.arrange(p1,p2,  ncol=2, top="") 

   #+end_src
*** 
   #+RESULTS:
   [[file:./img/search_combination_rep_slowdown.png]]

  By studying the characteristics of the search space we can
  understand the structure of the problem in order to be able to
  understand the results of the different search techniques. 
  The figure\ref{} and the table\ref{search-space-characteristics}
  show the distribution of the combinations over the 
  search space in term of slowdown. This search space contains a lot
  of good combinations, half of them have a slowdown that
  is less x6.1 which is x2.8 faster than the mean slowdown. However
  there are few bad ones with the worst at a slowdown of x382. Thus
  the probability of finding a good performing combination is high,
  this is the reason why randomized algorithms such as the RS, GA and
  LHS good results. Among the 23120 combinations there are 170 which
  have a slowdown lower than 10%. In order get less than this slowdown
  with a probability of 0.9 with the random search we would need to
  pick at least 312 points at random. And if we hope to get the same
  level of performance as with LM, we would need 6654 points at least.
  Yet, this search space remains complicated,
  because as we saw previously our local search GR failed which means
  there are a lot of local optimum in which it is stuck and some are
  far from the optimal one. The table\ref{tab:top-20} shows the best
  20 combinations, they are very similar, they all have vector length
  of size 1 and a number of elements and a y component number of 5
  or 6. Which means that they are very located but there is still some
  local optimum is this area because we try to run the GR and the
  result of LM it did not make any progress.

  #+CAPTION: This table presents the slowdown characteristics of the search space
  #+NAME:   tab:search-space-characteristics
  | Min  | 1st Q. | Median | Mean   | 3rd Q. |     Max |
  |------+--------+--------+--------+--------+---------|
  | /    | <>     | <>     | <>     | <>     |         |
  | 1.00 | 2.599  | 6.116  | 17.276 | 17.177 | 382.168 |

  #+CAPTION: This table presents the top 20 of the best combinations
  #+NAME:   tab:top-20
  | elements | y component | vector | temporary | load    | threads | lws_y | slowdown |
  | number   | number      | length | size      | overlap | number  |       |          |
  | <c>      | <c>         | <c>    | <c>       | <c>     | <c>     | <c>   | <c>      |
  | /        | <>          | <>     | <>        | <>      | <>      | <>    |          |
  |----------+-------------+--------+-----------+---------+---------+-------+----------|
  | 6        | 6           | 1      | 2         | true    | 1024    | 2     | 1.000000 |
  | 6        | 6           | 1      | 2         | false   | 256     | 4     | 1.005562 |
  | 5        | 5           | 1      | 2         | true    | 256     | 4     | 1.007540 |
  | 5        | 5           | 1      | 2         | false   | 256     | 4     | 1.009100 |
  | 6        | 6           | 1      | 2         | false   | 1024    | 8     | 1.009796 |
  | 6        | 6           | 1      | 2         | false   | 128     | 4     | 1.010568 |
  | 6        | 6           | 1      | 2         | true    | 1024    | 4     | 1.011509 |
  | 5        | 5           | 1      | 2         | false   | 512     | 4     | 1.011710 |
  | 6        | 6           | 1      | 2         | true    | 256     | 1     | 1.012177 |
  | 6        | 6           | 1      | 2         | false   | 128     | 2     | 1.012663 |
  | 6        | 6           | 1      | 2         | true    | 128     | 2     | 1.013555 |
  | 6        | 6           | 1      | 2         | true    | 256     | 2     | 1.013578 |
  | 5        | 5           | 1      | 2         | true    | 512     | 4     | 1.013662 |
  | 6        | 6           | 1      | 4         | false   | 1024    | 4     | 1.013695 |
  | 6        | 6           | 1      | 4         | true    | 512     | 8     | 1.014100 |
  | 6        | 6           | 1      | 4         | true    | 1024    | 4     | 1.014177 |
  | 6        | 6           | 1      | 2         | false   | 256     | 1     | 1.014598 |
  | 6        | 6           | 1      | 2         | true    | 1024    | 8     | 1.014882 |
  | 6        | 6           | 1      | 2         | true    | 128     | 1     | 1.015087 |
  | 6        | 6           | 1      | 2         | true    | 512     | 8     | 1.015301 |


   We can also notice in the figure\ref{} that the variability is not
   the same everywhere, hence our random variables are
   heteroscedastics. This  because the noise does not 
   follow the same law for the different value of the same
   parameters. This noise is due to complex interactions between
   parameters.  


   #+RESULTS:
   [[file:./img/heteroscedasticity.png]]

** DONE Differences between regression of expectation and quantile regression
   The previous results showed that using linear regression gives
   often good results. But there is a significant difference between
   regression of expectation and quantile regression. It is due to the
   fact that they do not predict the same thing.
*** Linear regression of expectation: why it can be inefficient
   #+begin_src sh :results output :exports none
       ruby ../../../scripts/format_data.rb ../../../data/2016_03_11/pilipili2/19_13_54/Data19_13_54_linear.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file img/lm.png :exports results :width 800 :height 400 :session 
     library(ggplot2)
     library(plyr)
     library(gridExtra)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     err_x_comp = ddply(df,c("x_component_number"), summarize,
                        mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))


     err_v_len = ddply(df,c("vector_length"), summarize,
                       mean = mean(time_per_pixel), err = 2*sd(time_per_pixel)/sqrt(length(time_per_pixel)))

     p1 <- qplot(df$vector_length, df$time_per_pixel) + 
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_errorbar(data=err_v_len,aes(x=vector_length,y=mean, ymin=mean-err, ymax=mean+err),colour="red") +
         ggtitle("Impact of the vector length") +
         labs(y="time per pixel in seconds", x="vector length") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))

     p2 <- qplot(df$x_component_number, df$time_per_pixel) + 
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_errorbar(data=err_x_comp,aes(x=x_component_number,y=mean, ymin=mean-err, ymax=mean+err),colour="red") +
         ggtitle("Impact of number of component on the x-axis") +
         labs(y="time per pixel in seconds", x="x component number") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))

     grid.arrange(p1, p2, ncol=2, top="") 

   #+end_src
   
   #+CAPTION: Linear regression and non-uniform noise
   #+LABEL: fig:lm-1
   #+RESULTS:
   [[file:img/lm.png]]
   
   Linear regression of expectation has already been used successfully
   with auto-tuning problems by Brewer in the
   1990s\cite{Brewer:1995:HOV:209937.209946}. But this method have
   been put aside for no real reasons to our knowledge. Using this
   method to study the impact of the parameters with using linear
   models to approximate the behavior of the search space coupled with
   efficient sampling strategies seemed very appealing to us. 

   This techniques assumes that the noise is uniform and more
   specifically follows the Gaussian law, in this case we say that the
   variable are homoscedastics. However the figure\ref{fig:lm-1} shows
   that in the case of our Laplacian kernel, which is a very simple case,
   we have heteroscedasticity, which means that the noise is
   non-uniform because it is due to complex interaction between
   parameters. Heteroscedasticity is problematic because
   the least square is not the Best Linear Unbiased Estimator in this
   case and it biases the variance and thus the coefficient of
   determination which makes it more difficult to evaluate the
   accuracy of the model. In addition, we want to predict the minimum
   value of the objective function not the mean. With non-uniform
   noise the evaluation of the minimum value does not follows the
   evolution of the mean.
   
   The reason why linear regression of expectation have been
   efficient in Brewer's work\cite{Brewer:1995:HOV:209937.209946} is
   probably because at that time the architecture of computers was
   less complicated than today and the noise was uniform. If the error
   law is the same everywhere as in the left in figure\ref{fig:lm-1}
   we can still have the minimum values that follow the same evolution
   as the mean and we can still predict the minimum. The resulting
   model and approximation can still be correct and we can easily know
   what is the best size of vector. But we would still need to make
   assumptions that about the error and we do not know anything it. In
   the right in figure\ref{fig:lm-1}, the evolution of the mean and
   the evolution of the minimum is not correlated and the best value
   is mispredicted.     

   We conclude that in the case of heteroscedasticity and non-uniform
   error law, linear regression tracks the general tendency of impact
   of the parameters. But in our case in which we are interested about
   the minimum which is uncorrelated to the mean, the linear
   regression of expectation cannot lead to the global optimum and we
   need another estimator for the minimum. 

*** The choice of quantile regression
****                                                               :noexport:
   #+begin_src sh :results output :exports none
     ruby ../../../scripts/format_data.rb ../../../data/2016_03_11/pilipili2/19_13_54/Data19_13_54_linear.yaml
   #+end_src

   #+RESULTS:

   #+begin_src R :results output graphics :file img/why_we_choose_quantile_reg.pdf :exports results :width 6 :height 4 :session
     library(ggplot2)

     df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
     attach(df)

     ggplot(df) + 
         aes(x=x_component_number, y=time_per_pixel) +
         geom_point(alpha=0.1) + 
         geom_hline(yintercept=min(df$time_per_pixel), color="red", linetype=2) +
         geom_smooth(method="lm", formula = y ~ x + I(1/x), aes(colour="Least square regression")) +           
         stat_quantile(quantiles=0.05, formula = y ~ x + I(1/x), aes(colour="Quantile regression")) +
         ggtitle("Impact of number of component on the x-axis") +
         labs(y="Time per pixel in seconds", x="x component number") +
         theme(axis.text=element_text(size=12),
               axis.title=element_text(size=14,face="bold"))
   #+end_src

   #+RESULTS:
   [[file:img/why_we_choose_quantile_reg.pdf]]

**** 
    Quantile regression gives the estimate of quantile and has been
    proven successful in the ecologic field\cite{FEE:FEE200318412}
    where complex interactions between factors lead to non-uniform
    noise which is exactly the case of our Laplacian kernel. Moreover
    it has the ability to estimate multiple tendency from the minimum
    to the maximum. As we want to minimize the time to compute a pixel
    we need to estimate the minimum and regression on the 5th
    percentile is a generally a good estimation of 
    it\cite{books/daglib/0076234}. 
    Figure\ref{fig:qr-example} illustrates the comparison
    between regression of expectation and quantile regression. The
    regression of expectation estimates that the best performing
    version has a x_component_number of 4 which is not true. While
    the quantile regression of the 5th succeed in predicting that the
    best performing version has a x_component_number of 1. So the
    regression of expectation may not find the minimum while the
    quantile regression does if the model is correct.
   
   #+BEGIN_LaTeX
   \begin{figure}[htb]
   \centering
   \includegraphics[width=.9\linewidth]{./img/why_we_choose_quantile_reg.pdf}
   \caption{\label{fig:qr-example}Linear regression vs quantile regression}
   \end{figure}
   #+END_LaTeX

** DONE LM: Success and "failures"
***                                                                :noexport:
**** Bad case
    #+begin_src sh :results output :exports none
      ruby ../../../scripts/format_data.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_cleaned.yaml 
    #+end_src

    #+RESULTS:

    #+begin_src R :results output :session :exports both
      df <- read.csv("/tmp/test.csv", strip.white=T, header=T)
      logs <- readRDS("../../../data/2016_04_08/pilipili2/18_08_24/lm_random_logs_new_strat_1000.rds")
      df_lm_random <- read.csv("../../../data/2016_04_08/pilipili2/18_08_24/lm_random_new_strat_1000.csv", strip.white=T, header=T)
      df_lm_random[df_lm_random$time_per_pixel == max(df_lm_random$time_per_pixel),][1,]
      df_lm_random[df_lm_random$time_per_pixel == max(df_lm_random$time_per_pixel),][1,]$time_per_pixel / min(df$time_per_pixel)
      working_set <- logs[[544]]$starting_set

      model <- time_per_pixel ~ elements_number + y_component_number + vector_length + threads_number + lws_y + load_overlap + temporary_size
      fit <- lm(model,working_set)
      summary(fit)

      model <- time_per_pixel ~ vector_length  + lws_y 
      fit <- lm(model,working_set)
      summary(fit)
    #+end_src

    #+RESULTS:
    #+begin_example
        elements_number y_component_number vector_length temporary_size
    544              24                  6            16              2
        vector_recompute load_overlap threads_number lws_y time_per_pixel
    544             true        false             64     1   4.393478e-10
        point_number method
    544          119     LM
    [1] 3.771183

    Call:
    lm(formula = model, data = working_set)

    Residuals:
           Min         1Q     Median         3Q        Max 
    -3.205e-09 -1.315e-09 -4.193e-10  3.619e-10  7.396e-09 

    Coefficients:
                         Estimate Std. Error t value Pr(>|t|)    
    (Intercept)         4.503e-09  1.348e-09   3.340  0.00176 ** 
    elements_number    -9.059e-11  8.247e-11  -1.098  0.27829    
    y_component_number -2.341e-10  3.055e-10  -0.766  0.44787    
    vector_length       1.941e-12  6.063e-11   0.032  0.97462    
    threads_number     -2.445e-12  1.348e-12  -1.813  0.07691 .  
    lws_y               8.072e-12  1.854e-12   4.354 8.41e-05 ***
    load_overlaptrue    2.150e-10  6.540e-10   0.329  0.74402    
    temporary_size     -4.210e-10  3.362e-10  -1.252  0.21738    
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 2.255e-09 on 42 degrees of freedom
    Multiple R-squared:  0.3723,	Adjusted R-squared:  0.2677 
    F-statistic: 3.559 on 7 and 42 DF,  p-value: 0.004306

    Call:
    lm(formula = model, data = working_set)

    Residuals:
           Min         1Q     Median         3Q        Max 
    -3.108e-09 -1.197e-09 -7.358e-10 -5.630e-11  8.891e-09 

    Coefficients:
                    Estimate Std. Error t value Pr(>|t|)   
    (Intercept)    1.594e-09  5.691e-10   2.800  0.00738 **
    vector_length -2.316e-11  5.953e-11  -0.389  0.69904   
    lws_y          5.572e-12  1.625e-12   3.428  0.00127 **
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 2.395e-09 on 47 degrees of freedom
    Multiple R-squared:  0.2081,	Adjusted R-squared:  0.1744 
    F-statistic: 6.174 on 2 and 47 DF,  p-value: 0.004161
    #+end_example

    #+begin_src R :results output graphics :file ./img/bad_prediction.pdf :exports both :width 4 :height 4 :session
      library(ggplot2)
      ggplot() +
      theme_bw() +
      geom_point(aes(x=fitted(fit), y=fitted(fit) + residuals(fit))) +
      geom_line( aes(x=c(-1,1), y=c(-1,1)), linetype=2) +
      labs(y="Real", x="Predicted") +
      ggtitle("Prediction vs reality") +
      coord_cartesian(xlim=c(0,7.5e-9), ylim=c(0,7.5e-9))
    #+end_src

    #+RESULTS:
    [[file:./img/bad_prediction.pdf]]

**** Good case
    #+begin_src sh :results output :exports none
      ruby ../../../scripts/format_data.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_cleaned.yaml 
    #+end_src

    #+RESULTS:

    #+begin_src R :results output :session :exports both
      df <- read.csv("/tmp/test.csv", strip.white=T, header=T)
      logs <- readRDS("../../../data/2016_04_08/pilipili2/18_08_24/lm_random_logs_new_strat_1000.rds")
      df_lm_random <- read.csv("../../../data/2016_04_08/pilipili2/18_08_24/lm_random_new_strat_1000.csv", strip.white=T, header=T)
      working_set <- logs[[2]]$starting_set

      model <- time_per_pixel ~ vector_length  + lws_y 
      fit <- lm(model,working_set)
      summary(fit)
    #+end_src

    #+RESULTS:
    #+begin_example

    Call:
    lm(formula = model, data = working_set)

    Residuals:
           Min         1Q     Median         3Q        Max 
    -1.537e-09 -5.136e-10 -4.230e-11  1.875e-10  3.356e-09 

    Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
    (Intercept)   -4.712e-12  2.211e-10  -0.021  0.98308    
    vector_length  8.794e-11  3.025e-11   2.907  0.00555 ** 
    lws_y          1.987e-11  2.882e-12   6.894 1.18e-08 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 9.821e-10 on 47 degrees of freedom
    Multiple R-squared:  0.5617,	Adjusted R-squared:  0.5431 
    F-statistic: 30.12 on 2 and 47 DF,  p-value: 3.812e-09
#+end_example

    #+begin_src R :results output graphics :file ./img/good_prediction.pdf :exports both :width 4 :height 4 :session
      library(ggplot2)
      ggplot() +
      theme_bw() +
      geom_point(aes(x=fitted(fit), y=fitted(fit) + residuals(fit))) +
      geom_line( aes(x=c(-1,1), y=c(-1,1)), linetype=2) +
      labs(y="Real", x="Predicted") +
      ggtitle("Prediction vs reality") +
      coord_cartesian(xlim=c(0,6e-9), ylim=c(0,6e-9))
    #+end_src

    #+RESULTS:
    [[file:./img/good_prediction.pdf]]

****  Lucky case
    #+begin_src sh :results output :exports none
      ruby ../../../scripts/format_data.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_cleaned.yaml 
    #+end_src

    #+begin_src R :results output :session :exports both
      df <- read.csv("/tmp/test.csv", strip.white=T, header=T)
      logs <- readRDS("../../../data/2016_04_08/pilipili2/18_08_24/lm_random_logs_new_strat_1000.rds")
      df_lm_random <- read.csv("../../../data/2016_04_08/pilipili2/18_08_24/lm_random_new_strat_1000.csv", strip.white=T, header=T)
      working_set <- logs[[1]]$starting_set

      model <- time_per_pixel ~ vector_length  + lws_y 
      fit <- lm(model,working_set)
      summary(fit)

      model <- time_per_pixel ~ threads_number  + lws_y 
      fit <- lm(model,working_set)
      summary(fit)
    #+end_src

    #+RESULTS:
    #+begin_example

    Call:
    lm(formula = model, data = working_set)

    Residuals:
           Min         1Q     Median         3Q        Max 
    -6.610e-09 -1.287e-09 -6.651e-10  9.950e-11  1.290e-08 

    Coefficients:
                   Estimate Std. Error t value Pr(>|t|)   
    (Intercept)   5.685e-10  8.229e-10   0.691  0.49310   
    vector_length 1.438e-10  8.941e-11   1.608  0.11453   
    lws_y         1.507e-11  4.596e-12   3.278  0.00197 **
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 3.181e-09 on 47 degrees of freedom
    Multiple R-squared:  0.2005,	Adjusted R-squared:  0.1665 
    F-statistic: 5.894 on 2 and 47 DF,  p-value: 0.005202

    Call:
    lm(formula = model, data = working_set)

    Residuals:
           Min         1Q     Median         3Q        Max 
    -6.249e-09 -1.440e-09 -7.497e-10  1.096e-09  1.158e-08 

    Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
    (Intercept)     2.621e-09  5.982e-10   4.381 6.57e-05 ***
    threads_number -3.816e-12  1.216e-12  -3.138 0.002936 ** 
    lws_y           1.826e-11  4.463e-12   4.093 0.000166 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 2.971e-09 on 47 degrees of freedom
    Multiple R-squared:  0.3026,	Adjusted R-squared:  0.2729 
    F-statistic:  10.2 on 2 and 47 DF,  p-value: 0.0002096
#+end_example

    #+begin_src R :results output graphics :file ./img/lucky_prediction.pdf :exports both :width 4 :height 4 :session
      model <- time_per_pixel ~ vector_length  + lws_y 
      fit <- lm(model,working_set)

      library(ggplot2)
      ggplot() +
          theme_bw() +
          geom_point(aes(x=fitted(fit), y=fitted(fit) + residuals(fit))) +
          geom_line( aes(x=c(-1,1), y=c(-1,1)), linetype=2) +
          labs(y="Real", x="Predicted") +
          ggtitle("Prediction vs reality") +
          coord_cartesian(xlim=c(0,2e-8), ylim=c(0,2e-8))
    #+end_src

    #+RESULTS:
    [[file:./img/lucky_prediction.pdf]]

*** 
    To study in more detailed the results of our approach, we took the
    worst and the best solutions to understand why some time it
    fails to converge to the same value and how look like a correct and
    an incorrect optimization process.

    The worst solution\ref{tab:worst-solution} failed in the first
    stage of the optimization process. Indeed, vector length and lws y
    are the first factors we fixed in our strategy. It predicted a
    good value for lws y, but instead of predicting that the best size
    of vector is 1, the model predicted a size of 16 which is the worst
    value. There are several hints that gives indications about the
    quality of the prediction. 

    #+CAPTION: This table presents the worst solution of LM
    #+NAME:   tab:worst-solution
      | elements | y component | vector | temporary | load    | threads | lws y | slowdown |
      | number   | number      | length | size      | overlap | number  |       |          |
      | <c>      | <c>         | <c>    | <c>       | <c>     | <c>     | <c>   | <c>      |
      |----------+-------------+--------+-----------+---------+---------+-------+----------|
      | /        | <>          | <>     | <>        | <>      | <>      | <>    | <        |
      | 24       | 6           | 16     | 2         | false   | 64      | 1     | 3.771183 |

    First, the *coefficient of* *determination* tells us about the fit of
    a model, the closer to 1 it is the better the model explains the
    variance. However when comparing model this is not sufficient. It
    is possible to have an acceptable R-squared with a too high
    uncertainty on the coefficients, e.g. by over-fitting, this will
    gives a bad prediction.    
 
    The *p-values* give information about the importance of a
    factor in the objective function, this gives us clues about the
    structure of the search space. The closer to zero it is the bigger
    the impact, thus we only keep factors for which the p-values is
    lower than 0.001.It is important to go from high order factors to
    low order factors in order to refine the model properly. Trying
    fix a factor that has a small impact too early would lead to a
    premature pruning of the search and as a results lots of good
    combinations would be missed.

    Then the order of magnitude of the *standard error* compared to
    *estimate of the coefficients* has to be taken into
    consideration. The coefficients gives the slope which indicates
    what kind of impact the factor has. For example, in our case
    vector length has a linear impact, a positive coefficient means
    the lower vector length is the lower the time to compute a pixel. 
    Comparing the coefficients with the standard errors gives
    information about the uncertainty in the estimate of the
    coefficient, the bigger is the standard error compared to the
    coefficient the more inaccurate the prediction.  
    
    An additional way of accessing the quality of a model is to
    compare the predicted values with the real ones, the more they are
    correlated the more accurate the model. The most important is
    correlation between the low values as we are interested mostly by
    the minimum values.

    All these information allows us to compare different models and
    evaluate the quality of the prediction. The
    table\ref{tab:incorrect-fit} shows the results of fitting the
    model bellow in the worst case:
    #+BEGIN_LaTeX
    \[vector length + lws y\]
    #+END_LaTeX 
    
    First, with this set of points, the factor vector length was not
    significant enough and was fixed prematurely. Second, the standard
    error is equal to 5.953e-11 and the estimate is equal to
    -2.316e-11, the standard error is too big compared to the
    estimate. Which means that the coefficient is between -8.269e-11
    and 3.637e-11. Thus we do not know if the coefficient is positive
    or negative, hence we do not know if the best value for vector
    length should be the smallest or the biggest possible. The
    figure\ref{fig:bad_prediction} shows the 
    prediction against the real values, good predicted values are not
    that good and some can be very bad. This kind of situations can be
    detected easily by the user and thus he can take correct decision. The
    explanation of why this worst case happened is because our
    automated experiment process did not make the previous analysis
    and applied the same strategy whether the model is correct or not.   
 
  #+CAPTION: This table presents the fit of an incorrect model
  #+NAME:   tab:incorrect-fit
    |               |       Coef | Std. err. | p-values |
    |---------------+------------+-----------+----------|
    | /             |         <> |        <> |        < |
    | vector_length | -2.316e-11 | 5.953e-11 |  0.69904 |
    | lws y         |  5.572e-12 | 1.625e-12 |  0.00127 |

   The good case happens when our strategy is applied like we
   expected. When using the model vector length + lws y the
   fit is acceptable with a coefficient of determination of
   0.5431. The prediction is only made with relevant factors for
   example in the table\ref{tab:correct-fit} the p-values of vector
   length and lws y are below 0.001. The standard errors compared to
   the estimates of the coefficients are good enough, e.g. for lws y the
   coefficient is 1.987e-11 the standard error 2.882e-12, then the real
   coefficient is between 1.6988e-11 and 2.2752e-11, there is a low
   uncertainty, we are sure that the coefficient is positive and we
   know the best value for lws y is the lower as
   possible. Figure\ref{fig:good_prediction} shows that even if the
   prediction is not perfect, the correlation between low values is
   good enough.

  #+CAPTION: This table presents the fit of a correct model
  #+NAME:   tab:correct-fit
    |               |      Coef | Std. err. | p-values |
    |---------------+-----------+-----------+----------|
    | /             |        <> |        <> |        < |
    | vector length | 8.794e-11 | 3.025e-11 |  0.00555 |
    | lws y         | 1.987e-11 | 2.882e-12 | 1.18e-08 |

   It exists also lucky cases, where the applied model was not very good but
   which still gives the same solution as with the favorable cases. One
   of these cases\ref{tab:lucky-fit} when fitting the model vector
   length + lws y has a poor R-squared of 0.1665. Only lws y is
   significant. The standard error of vector length is high,
   however even with uncertainty we can tell that the estimate of the
   coefficient will be positive. This lack of accuracy in the
   coefficient is problematic with factors which have complex impact
   and interactions but with factors which have a simple linear
   interaction such as vector length it is enough to know if the
   coefficient is negative or positive. Also when comparing the
   predicted values and the observed ones\ref{fig:lucky_prediction} it
   looks like favorable cases, there is a strong correlation for the
   low values. 

  #+CAPTION: This table presents the fit an incorrect model but which gives the values
  #+NAME:   tab:lucky-fit
    |               |      Coef | Std. err. | p-values |
    |---------------+-----------+-----------+----------|
    | /             |        <> |        <> |        < |
    | vector_length | 1.438e-10 | 8.941e-11 |  0.11453 |
    | lws y         | 1.507e-11 | 4.596e-12 |  0.00197 |

   The reason why in favorable cases and lucky cases we get the same
   results if because in the first stage, we fix vector length and lws
   y to the same values. This strongly directs the process to the same
   solution, because each time it restricts the search space to the
   same set of point. Pruning the search space reduces the noise due
   to the complex interactions between factors. When fixing vector
   length at the value 1 and lws y at the value 1 it remains only 576
   points in the search space. The process converges to the same
   information about the search space, that is why we get the same
   results in favorable cases and lucky cases. 

   The reason why we never get better solution with a slowdown lower
   than 1.012177 comes also from the first stage of the process. In
   the previous table of the 20 best performing
   combinations\ref{tab:top-20} there is no better solution with a lws
   y of 1. At this step it is not possible to estimate it very
   precisely because of the noise and the most suited model is a
   simple linear one. The estimation of lws y is made too
   early. Trying to fix it later with less may probably gives a better
   result but we did not find a case where it is possible.
    
   #+BEGIN_LaTeX
   \begin{figure}[htb]
   \centering
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/bad_prediction.pdf}
   \caption{\label{fig:bad_prediction}Incorrect model}
   \end{minipage}
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/good_prediction.pdf}
   \caption{\label{fig:good_prediction}Correct model}
   \end{minipage}
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/lucky_prediction.pdf}
   \caption{\label{fig:lucky_prediction}Lucky case}
   \end{minipage}
   \end{figure}
   #+END_LaTeX

  In short the reason why sometime we get worse values comes from the
  automatization of our process and involving the user in the
  optimization process improves the quality of the results. And the
  reason why we converge to the same solution most of the time come
  from our pruning strategy and the model we apply.

** Model choice, refinement and pruning decision                   :noexport:
   Usually the expert understands every optimization of its kernel and
   has ideas about their potential effects. He does not only need to check his
   hypothesis but also to have an accurate estimation of the impact of
   these optimization parameters.
*** Starting hypothesis
   These hypothesis facilitate the task of finding the model.
   In the case of our Laplacian kernel the elements_number parameter
   is the number of component a thread compute and thus determines the
   number of threads used to perform the computation of the
   Laplacian. Using more threads allows to compute more component at
   in parallel. However it can also lead to a less efficient sharing
   of the cache resources as it will increase the number of memory
   loads. Also the higher the number of threads the more important the
   overhead due to their management. Hence the impact of the number of
   elements can be modeled as follow:
   #+BEGIN_LaTeX
   \[ \displaystyle elements number + \frac{1}{elements number} \]
   #+END_LaTeX  
  
   The parameter y_component_number determines the number of components
   on the y-axis a thread compute. This tilling optimization may take
   advantage of the organization of memory banks on Nvidia GPUs and
   thus improve the data usage. The impact is suspect to have an
   almost quadratic shape and we can try either a model like this:
   #+BEGIN_LaTeX
   \[ \displaystyle y component number + y component number^2 \]
   #+END_LaTeX  
  
   Or either like this:
   
   #+BEGIN_LaTeX
   \[ \displaystyle y component number + \frac{1}{y component number} \]
   #+END_LaTeX  

   The parameter vector_length allow to specify the size of vector
   variables. However on Nvidia GPUs does not provides vector support
   and variable are manipulate like scalar variable. Hence we can
   suppose that the effect of vector length should be negative and we
   can for example try a simple linear model.

   The threads_number parameter determines the size of a work
   group. Threads in a group can share data, bigger groups means
   better data usage. However smaller groups gives more scheduling
   opportunity but there might be an overhead due to a higher number
   of work groups to manage. We can either try this model:

   #+BEGIN_LaTeX
   \[ \displaystyle threads number + threads number^2 \]
   #+END_LaTeX  
  
   Or this one:
   
   #+BEGIN_LaTeX
   \[ \displaystyle threads number + \frac{1}{threads number} \]
   #+END_LaTeX  

   The lws_y parameter organizes the threads on a work group in 2
   dimensions and tells how many threads there is on the y-axis. For
   this parameter it can be difficult what kind of impact it could
   have. It can have a linear or quadratic in impact. Between the two
   it is better to start the simpler one, the simple linear model.

   The parameters load_overlap and temporary_size are factors with
   only two levels thus only a simple linear model is needed.   

   If the application developer does not have a vague idea of the
   model, he can start we a simple linear combination of the
   parameters and refine it later. 

*** Finding and refining the model
****                                                               :noexport:
     #+begin_src sh :results output :exports both
       ruby ../../../scripts/format_data.rb ../../../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_cleaned.yaml
     #+end_src

     #+RESULTS:

     Functions to find the best values for factors:
     #+begin_src R :results output :session :exports both
       objective_predict <- function(fit,x){
           names <- colnames(x)
           s <- paste("values <-data.frame(", paste(paste(names,names,sep="=x$"),collapse=","), ")")
           eval(parse(text=s))
           as.numeric(predict(fit, values, interval="none"))
       }
       
       objective_predict_one <- function(fit,x,colname){
           s <- paste("values <-data.frame(", paste(paste(colname,"=x",sep=""),collapse=","), ")")
           eval(parse(text=s))
           as.numeric(predict(fit, values, interval="none"))
       }
       
       find_best <- function(model,subset,full_set,colnames){
           fit <- lm(data=subset,formula=formula(model))
           if( length(colnames) > 1) {
               return(full_set[objective_predict(fit,full_set[,colnames]) == min(objective_predict(fit,full_set[,colnames])), colnames][1,])
           }
           else{
               return(full_set[objective_predict_one(fit, full_set[,colnames], colnames) == min(objective_predict_one(fit, full_set[,colnames], colnames)), colnames][1])
           }
       }       
     #+end_src                                                              

     #+RESULTS:

     Loading LM worst case:
     #+begin_src R :results output :session :exports both
       logs <- read.csv("../../../data/2016_04_08/pilipili2/18_08_24/lm_random_logs_2.csv",strip.white=T,header=T)
       df_lm_random <- read.csv("../../../data/2016_04_08/pilipili2/18_08_24/lm_random_improve_trial_2.csv",strip.white=T,header=T)
       df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
       working_set <- logs[logs$run == df_lm_random[df_lm_random$time_per_pixel == max(df_lm_random$time_per_pixel),]$run,]
       set.seed(1) 
     #+end_src                                                              

     #+RESULTS:

     #+begin_src R :results output :session :exports both
       vector_length_val = as.numeric(levels(as.factor(df$vector_length)))
       threads_number_val = as.numeric(levels(as.factor(df$threads_number)))
       lws_y_val = as.numeric(levels(as.factor(df$lws_y)))

       form_v_len <- time_per_pixel ~ vector_length 
       form_th_nb <- time_per_pixel ~ threads_number 
       form_lws_y <- time_per_pixel ~ lws_y 

       model_v_len <- lm(data=working_set,formula=form_v_len)
       model_th_nb <- lm(data=working_set,formula=form_th_nb)
       model_lws_y <- lm(data=working_set,formula=form_lws_y)

     #+end_src

     #+RESULTS:
     
     Looking for relevant parameters and removing useless one:
     #+begin_src R :results output :session :exports both
       summary(lm(time_per_pixel ~ threads_number + vector_length + lws_y + elements_number + y_component_number + temporary_size + load_overlap, working_set))
       summary(lm(time_per_pixel ~ threads_number + vector_length + lws_y + elements_number, working_set))
       summary(lm(time_per_pixel ~ threads_number + vector_length + lws_y, working_set))
     #+end_src                                                              

     #+RESULTS:
     #+begin_example

     Call:
     lm(formula = time_per_pixel ~ threads_number + vector_length + 
         lws_y + elements_number + y_component_number + temporary_size + 
         load_overlap, data = working_set)

     Residuals:
            Min         1Q     Median         3Q        Max 
     -6.232e-09 -1.239e-09 -6.730e-11  6.137e-10  2.105e-08 

     Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
     (Intercept)        -4.497e-10  1.481e-09  -0.304  0.76219    
     threads_number     -2.706e-12  1.015e-12  -2.667  0.00922 ** 
     vector_length       1.945e-10  5.991e-11   3.246  0.00170 ** 
     lws_y               1.221e-11  2.569e-12   4.751 8.49e-06 ***
     elements_number    -2.269e-10  7.762e-11  -2.924  0.00447 ** 
     y_component_number  6.975e-10  2.679e-10   2.604  0.01094 *  
     temporary_size      2.251e-10  3.375e-10   0.667  0.50661    
     load_overlaptrue    7.050e-10  6.887e-10   1.024  0.30901    
     ---
     Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

     Residual standard error: 3.102e-09 on 82 degrees of freedom
     Multiple R-squared:  0.3564,	Adjusted R-squared:  0.3015 
     F-statistic: 6.488 on 7 and 82 DF,  p-value: 4.317e-06

     Call:
     lm(formula = time_per_pixel ~ threads_number + vector_length + 
         lws_y + elements_number, data = working_set)

     Residuals:
            Min         1Q     Median         3Q        Max 
     -6.820e-09 -1.457e-09 -4.633e-10  6.700e-10  2.195e-08 

     Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
     (Intercept)      2.060e-09  8.016e-10   2.570  0.01191 *  
     threads_number  -3.062e-12  1.007e-12  -3.041  0.00313 ** 
     vector_length    1.897e-10  6.097e-11   3.112  0.00253 ** 
     lws_y            1.215e-11  2.622e-12   4.632 1.29e-05 ***
     elements_number -9.060e-11  5.935e-11  -1.527  0.13058    
     ---
     Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

     Residual standard error: 3.181e-09 on 85 degrees of freedom
     Multiple R-squared:  0.2987,	Adjusted R-squared:  0.2657 
     F-statistic:  9.05 on 4 and 85 DF,  p-value: 3.87e-06

     Call:
     lm(formula = time_per_pixel ~ threads_number + vector_length + 
         lws_y, data = working_set)

     Residuals:
            Min         1Q     Median         3Q        Max 
     -6.206e-09 -1.405e-09 -5.497e-10  5.942e-10  2.238e-08 

     Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
     (Intercept)     1.278e-09  6.212e-10   2.057  0.04270 *  
     threads_number -2.996e-12  1.014e-12  -2.956  0.00403 ** 
     vector_length   1.998e-10  6.107e-11   3.272  0.00154 ** 
     lws_y           1.146e-11  2.603e-12   4.402 3.07e-05 ***
     ---
     Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

     Residual standard error: 3.205e-09 on 86 degrees of freedom
     Multiple R-squared:  0.2795,	Adjusted R-squared:  0.2543 
     F-statistic: 11.12 on 3 and 86 DF,  p-value: 3.073e-06
#+end_example

     We perform the regression with threads_number, vector_length and
     lws_y and we can check the fitting visually:
     #+begin_src R :results output graphics :file ./img/reg_threads_number.pdf :exports both :width 6 :height 4 :session
       ggplot(working_set) +
           aes(x=threads_number, y=time_per_pixel) +
           geom_point() + 
           theme_bw() +
           geom_smooth(method="lm", formula = y ~ x) 
     #+end_src

     #+RESULTS:
     [[file:./img/reg_threads_number.pdf]]

     #+begin_src R :results output graphics :file ./img/reg_vector_length.pdf :exports both :width 6 :height 4 :session
       ggplot(working_set) +
           aes(x=vector_length, y=time_per_pixel) +
           geom_point() + 
           theme_bw() +
           geom_smooth(method="lm", formula = y ~ x) 
     #+end_src

     #+RESULTS:
     [[file:./img/reg_vector_length.pdf]]

     #+begin_src R :results output graphics :file ./img/reg_lws_y.pdf :exports both :width 6 :height 4 :session
       ggplot(working_set) +
           aes(x=lws_y, y=time_per_pixel) +
           geom_point() + 
           theme_bw() + 
           geom_smooth(method="lm", formula = y ~ x) 
     #+end_src

     #+RESULTS:
     [[file:./img/reg_lws_y.pdf]]

     Using the model we find the best values for these factors:
     #+begin_src R :results output :session :exports both
       model <- time_per_pixel ~ threads_number + vector_length + lws_y
       best1 <- find_best(model, working_set, df, c("threads_number","vector_length","lws_y"))
       best1
     #+end_src

     #+RESULTS:
     :      threads_number vector_length lws_y
     : 6467           1024             1     1

     Pruning the search space and re-sampling accordingly because
     there is no more point left in the working set:
     #+begin_src R :results output :session :exports both
       subset <- working_set[working_set$threads_number == best1$threads_number & 
                             working_set$vector_length == best1$vector_length & 
                             working_set$lws_y == best1$lws_y,]
       
       pruned_full_space <- df[df$threads_number == best1$threads_number & 
                               df$vector_length == best1$vector_length & 
                               df$lws_y == best1$lws_y,]
       
       nrow(subset)
       nrow(pruned_full_space)

      tmp2 <- pruned_full_space[sample(1:nrow(pruned_full_space), size = 20, replace = FALSE),]
      subset <- subset[, !names(working_set) %in% c("run") ] 
      subset <- rbind(subset, tmp2)     
     #+end_src

     #+RESULTS:
     : [1] 0
     : [1] 96

     Looking for the remaining important factors, testing models and
     looking for interactions between parameters:
     #+begin_src R :results output :session :exports both
     model <- time_per_pixel ~ elements_number + y_component_number + temporary_size + load_overlap
     fit <- lm(model,subset)
     summary(fit)
     model <- time_per_pixel ~ elements_number + y_component_number 
     fit <- lm(model,subset)
     summary(fit)
     model <- time_per_pixel ~ elements_number + I(elements_number^2) + y_component_number 
     fit <- lm(model,subset)
     summary(fit)
     model <- time_per_pixel ~ (elements_number + y_component_number)^2
     fit <- lm(model,subset)
     summary(fit)
     #+end_src

     #+RESULTS:
     #+begin_example

     Call:
     lm(formula = model, data = subset)

     Residuals:
            Min         1Q     Median         3Q        Max 
     -4.583e-11 -2.525e-11  6.210e-13  1.481e-11  6.592e-11 

     Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
     (Intercept)         3.661e-10  2.946e-11  12.430 2.67e-09 ***
     elements_number     1.065e-11  2.036e-12   5.233 0.000101 ***
     y_component_number -5.898e-11  6.991e-12  -8.437 4.45e-07 ***
     temporary_size     -1.909e-11  7.932e-12  -2.407 0.029426 *  
     load_overlaptrue    1.453e-11  1.533e-11   0.948 0.358369    
     ---
     Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

     Residual standard error: 3.287e-11 on 15 degrees of freedom
     Multiple R-squared:  0.831,	Adjusted R-squared:  0.7859 
     F-statistic: 18.44 on 4 and 15 DF,  p-value: 1.17e-05

     Call:
     lm(formula = model, data = subset)

     Residuals:
            Min         1Q     Median         3Q        Max 
     -4.339e-11 -3.000e-11 -1.697e-11  3.861e-11  6.645e-11 

     Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
     (Intercept)         3.154e-10  1.927e-11  16.365 7.72e-12 ***
     elements_number     8.934e-12  2.237e-12   3.994 0.000939 ***
     y_component_number -5.380e-11  7.716e-12  -6.973 2.25e-06 ***
     ---
     Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

     Residual standard error: 3.793e-11 on 17 degrees of freedom
     Multiple R-squared:  0.745,	Adjusted R-squared:  0.715 
     F-statistic: 24.83 on 2 and 17 DF,  p-value: 9.041e-06

     Call:
     lm(formula = model, data = subset)

     Residuals:
            Min         1Q     Median         3Q        Max 
     -4.427e-11 -3.004e-11 -8.193e-12  3.853e-11  5.612e-11 

     Coefficients:
                            Estimate Std. Error t value Pr(>|t|)    
     (Intercept)           2.928e-10  2.751e-11  10.643 1.15e-08 ***
     elements_number       1.664e-11  7.106e-12   2.342   0.0324 *  
     I(elements_number^2) -2.990e-13  2.619e-13  -1.142   0.2704    
     y_component_number   -5.611e-11  7.912e-12  -7.092 2.55e-06 ***
     ---
     Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

     Residual standard error: 3.759e-11 on 16 degrees of freedom
     Multiple R-squared:  0.7642,	Adjusted R-squared:  0.7199 
     F-statistic: 17.28 on 3 and 16 DF,  p-value: 2.84e-05

     Call:
     lm(formula = model, data = subset)

     Residuals:
            Min         1Q     Median         3Q        Max 
     -4.989e-11 -2.858e-11 -1.482e-11  3.831e-11  6.263e-11 

     Coefficients:
                                          Estimate Std. Error t value Pr(>|t|)    
     (Intercept)                         3.353e-10  3.950e-11   8.489 2.55e-07 ***
     elements_number                     4.416e-12  8.108e-12   0.545    0.594    
     y_component_number                 -5.769e-11  1.033e-11  -5.583 4.12e-05 ***
     elements_number:y_component_number  8.537e-13  1.470e-12   0.581    0.570    
     ---
     Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

     Residual standard error: 3.869e-11 on 16 degrees of freedom
     Multiple R-squared:  0.7502,	Adjusted R-squared:  0.7034 
     F-statistic: 16.02 on 3 and 16 DF,  p-value: 4.461e-05
#+end_example

     Lets fix elements_number and y_component_number:
     #+begin_src R :results output :session :exports both
       model <- time_per_pixel ~ elements_number + y_component_number      
       best2 <- find_best(model, subset, pruned_full_space, c("elements_number","y_component_number"))
       best2
     #+end_src

     #+RESULTS:
     :       elements_number y_component_number
     : 14403               6                  6

     #+begin_src R :results output graphics :file ./img/reg_elements_number.pdf :exports both :width 6 :height 4 :session
       library(ggplot2)
       ggplot(subset) +
           aes(x=elements_number, y=time_per_pixel) +
           geom_point() + 
           theme_bw() +
           geom_smooth(method="lm", formula = y ~ x) 
     #+end_src

     #+RESULTS:
     [[file:./img/reg_elements_number.pdf]]

     #+begin_src R :results output graphics :file ./img/reg_y_component_number.pdf :exports both :width 6 :height 4 :session
       library(ggplot2)
       ggplot(subset) +
           aes(x=y_component_number, y=time_per_pixel) +
           geom_point() + 
           theme_bw() +
           geom_smooth(method="lm", formula = y ~ x) 
     #+end_src

     #+RESULTS:
     [[file:./img/reg_y_component_number.pdf]]


     Pruning the search space according to the best values for the
     factors elements_number and y_component_number:
     #+begin_src R :results output :session :exports both
       subset <- subset[subset$elements_number == best2$elements_number & 
                             subset$y_component_number == best2$y_component_number,]
       
       pruned_full_space <- pruned_full_space[pruned_full_space$elements_number == best2$elements_number & 
                               pruned_full_space$y_component_number == best2$y_component_number,]
       
       nrow(subset)
       nrow(pruned_full_space)
     #+end_src

     #+RESULTS:
     : [1] 0
     : [1] 4

     It remains only 4 points in the search space, as we did not
     exceed the budget of 120 points we take them and keep the best
     one: 
     #+begin_src R :results output :session :exports both
      tmp2 <- pruned_full_space[sample(1:nrow(pruned_full_space), size = nrow(pruned_full_space), replace = FALSE),]
      subset <- subset[, !names(working_set) %in% c("run") ] 
      subset <- rbind(subset, tmp2)     
      solution <- subset[subset$time_per_pixel == min(subset$time_per_pixel),]
      solution
     #+end_src

     #+RESULTS:
     :       elements_number y_component_number vector_length temporary_size
     : 19156               6                  6             1              4
     :       vector_recompute load_overlap threads_number lws_y time_per_pixel
     : 19156             true        false           1024     1    1.25374e-10

     The improvement: 
     #+begin_src R :results output :session :exports both
       df_lm_random[df_lm_random$time_per_pixel == max(df_lm_random$time_per_pixel),]$slowdown / (solution$time_per_pixel / min(df$time_per_pixel))
     #+end_src

     #+RESULTS:
     : [1] 1.953286

     For a slowdown compare to the best of:
     #+begin_src R :results output :session :exports both
       solution$time_per_pixel / min(df$time_per_pixel)
     #+end_src

     #+RESULTS:
     : [1] 1.076159

**** 
    In order to illustrate how it should be done, we choose to take the
    case from the results where the LM algorithm gave the worst
    result and tried to improve it. For this we used exactly the same
    random starting set and the same number of points allocation at
    each step. 
    The first step is to find the relevant parameters by doing a
    regression using the simplest model, the linear combination of
    factors. By taking into consideration the p-values and the
    standard we can identify the parameters that have the biggest
    impact and then we can try to see which model fit the most based
    on our hypothesis. Once we have a correct
    model\ref{fig:reg_threads_number} we can predict the best values
    the different factors, prune the search space accordingly and
    restart until all the factors have been fixed. 
    By doing this we achieved a speedup of x1.953 on the worst case
    which gives a slowdown of x1.076 compare to the best solution of
    the entire search space which is very close to the median of GA.

   #+BEGIN_LaTeX

   \begin{figure}[htb]
   \centering
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/reg_threads_number.pdf}
   \caption{\label{fig:reg_threads_number}Linear regression on threads_number}
   \end{minipage}
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/reg_vector_length.pdf}
   \caption{\label{fig:reg_vector_length}Linear regression on vector_length}
   \end{minipage}
   \end{figure}
   
   \begin{figure}[htb]
   \centering
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/reg_lws_y.pdf}
   \caption{\label{fig:reg_lws_y}Linear regression on lws_y}
   \end{minipage}
   \end{figure}

   #+END_LaTeX
    
** Sampling strategy                                               :noexport:
   The accuracy of a model is highly dependent of the set of points
   used to build it. Design of experiments methods propose techniques
   for sampling the space with the objective of maximizing the
   informations and reducing the number of points needed. For this
   reason we investigated some of this tools to find what are the most
   suited and how we can use them. To see which are able to bring the
   most information, we did the sampling and the linear regression 100
   times to see which converge the closest to the regression linear
   regression a full search space. 
*** Principle
   Our approach is an iterative process in which the user questions
   the search space to build and refine his model. Even if the user
   has a clear idea about the model he should start the process with
   the most generic techniques because not intuitive good points can be
   missed and expressing hypothesis too early will direct the
   exploration and would induce some biais. As explained earlier the
   first step is to estimate factors that have a significant impact. For
   this it is interesting to have points that are far enough to avoid
   perturbation due to noise and to get the global impact. Then to find
   any tendency, points should be choosen in way that provides a good
   covering of the space. And finally when the structure starts to
   appear or if there is a lot of variability we can focus the
   sampling where additional information is needed.
*** Screening design
****                                                               :noexport:
     #+begin_src R :results output :session :exports both
     
     #+end_src
**** 
    The screening is generally a good starting to interrogate the
    search space as it allows to detect parameters that the biggest
    impact. For this it considers the points that are the furthest from
    each other. It samples only the minimum and maximum values of the
    factors. Which means it only take points that are at 
    the border of the search space. With our search space, this gives
    a total of 128 possible combinations. However because of the
    constraints of our search space we only have at most 32 points. By
    using only about 20 points we were able to find that vector_length
    and lws_y have the most significant impact. By modifying this
    method to take into account constraints in order choose points at
    the border of the constrained domain could gives better results.
   
*** Random
****                                                               :noexport:
     #+begin_src R :results output graphics :file ./img/random_space_coverage.pdf :exports both :width 6 :height 4 :session
       df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
       set.seed(1)
       random_set <- df[sample(1:nrow(df), size = 100, replace = FALSE),]
       plot(random_set[,!names(random_set) %in% c("vector_recompute", "time_per_pixel")])
     #+end_src

     #+RESULTS:
     [[file:./img/random_space_coverage.pdf]]

**** 
    Uniform sampling chooses points at random with the same
    probability. It is the simplest sampling method and even if in
    general it does not have the best space covering, in our case
    which has a constraints and is discrete it provides a good
    covering.

    #+BEGIN_LaTeX
    \begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{./img/./img/random_space_coverage.pdf}
    \caption{\label{fig:random_space_coverage}Space coverage of the random sampling}
    \end{figure}
    #+END_LaTeX

*** LHS
*** D-Optimal
*** Managing the budget of points
* Technical difficulties
  With our approach we manage to get very good results, however there
  are some points that are still problematic. First one is due to
  the characteristics of our search space which is discrete and
  constrained. The other come from the use of quantile regression. 
** Constrained and discrete search space
*** DONE Model optimization
****                                                               :noexport:
    #+begin_src sh :results output :exports both
      ruby ../scripts/format_data.rb ../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_cleaned.yaml
    #+end_src
    
    #+RESULTS:
    
    #+begin_src R :results output :session :exports both
      df <- read.csv("/tmp/test.csv",strip.white=T,header=T)
    #+end_src
    
    #+RESULTS:

    #+begin_src R :results output graphics :file img/search_space_3D.pdf :exports both :width 6 :height 6 :session                                                                   
      f <- function(x) { x * (0.05 - 1 * (x < 0)) }
      g <- function(x) { f(x)/x^2 }
      h <- function(x) {pmin(g(x),1e15)}

      model <- time_per_pixel ~ elements_number + I(1/elements_number) + 
          y_component_number + I(1/y_component_number) + 
          elements_number:y_component_number

      fit <- lm(data=df,formula=model)
      for(i in 1:200){
          E <- residuals(fit)
          fit <- lm(data=df,formula=model,weight=g(E))
      }

      library(plot3D)
      f <- function(x){
          as.numeric(predict(fit,data.frame(elements_number=x[1], y_component_number=x[2]),interval="none"))
      }
      combinations <- matrix(NA,nrow=24,ncol=6)
      for(i in 1:24){
          for(j in 1:6){
              combinations[i,j] <- f(c(i,j))
          }
      }
      persp3D(xlab="elements_number", ylab="y_component_number", zlab="time per pixel in s", z=combinations, phi=20, theta=220, clab=c("time per pixel in s"),colkey = list(side=1,length = 0.5))
    #+end_src
    
    #+RESULTS:
    [[file:img/search_space_3D.pdf]]
    
    #+begin_src R :results output graphics :file img/search_space_3D_constrained.pdf :exports both :width 6 :height 6 :session
      g <- function(x){
          ifelse(x[1] <= x[2] * 4, f(x), NA)
      }

      resi = 5
      resj = 5
      offi = 3
      offj = 3
      combinations <- matrix(NA,nrow=24*resi,ncol=6*resj)
      for(i in 1:(24*resi)){
          for(j in 1:(6*resj)){
              combinations[i,j] <- g(c((i+offi)/resi,(j+offj)/resj))
          }
      }

      persp3D(x=1:(24*resi)/resi, y=1:(6*resj)/resj, xlab="elements_number", ylab="y_component_number", zlab="time per pixel in s", z=combinations, phi=20, theta=220, clab=c("time per pixel in s"), colkey = list(side=1,length = 0.5))
    #+end_src
    
    #+RESULTS:
    [[file:img/search_space_3D_constrained.pdf]]

    #+begin_src R :results output graphics :file img/search_space_3D_constrained_ugly.pdf :exports both :width 6 :height 6 :session
      g <- function(x){
          ifelse(x[1] <= x[2] * 4 & x[1] %% x[2] == 0, f(x), NA)
      }

      resi = 1
      resj = 1
      offi = 1
      offj = 1
      x <- c()
      y <- c()
      z <- c()

      combinations <- matrix(NA,nrow=24*resi,ncol=6*resj)
      count <- 1
      for(i in 1:(24*resi)){
          for(j in 1:(6*resj)){
              x[count] <- i
              y[count] <- j
              z[count] <- g(c((i+offi)/resi,(j+offj)/resj))
              count <- count +1
          }
      }

      scatter3D(x=x, y=y, xlab="elements_number", ylab="y_component_number", zlab="time per pixel in s", z=z, phi=20, pch=20, theta=220, clab=c("time per pixel in s"), type="h", colkey = list(side=1,length = 0.5))
    #+end_src

    #+RESULTS:
    [[file:img/search_space_3D_constrained_ugly.pdf]]

    
    #+begin_src R :results output graphics :file img/search_space_3D_barrier.pdf :exports both :width 6 :height 6 :session
      h <- function(x){
          ifelse(x[1] <= x[2] * 4, f(x), (1 + .05*abs(x[1]-4*x[2]) ) * f(x))
      }

      combinations <- matrix(NA,nrow=24,ncol=6)
      for(i in 1:24){
          for(j in 1:6){
              combinations[i,j] <- h(c(i,j))
          }
      }

      persp3D(xlab="elements_number", ylab="y_component_number", zlab="time per pixel in s", z=combinations, phi=20, theta=190, clab=c("time per pixel in s"),colkey = list(side=1,length = 0.5))

    #+end_src
    
    #+RESULTS:
    [[file:img/search_space_3D_barrier.pdf]]

**** 
    Once the model is instantiated to find the optimal values it is
    necessary to perform a search. The predicted function is
    continuous\ref{fig:search_space_3d} whereas our search space is
    discrete\ref{fig:search_space_3d_constrained2}. Hence continuous
    optimizations techniques does not work correctly with such search
    space. 
    Our search space also has multiple constraints that restricts greatly
    the set of feasible points. There are constraints that ensure the
    consistency of the combinations of factors. For example
    elements_number being the number of component for a thread and
    y_component_number the number of component on the y-axis for a
    thread, we cannot have more component on the y-axis than the
    number of component itself, for this reason combinations such as
    elements_number = 1 and y_component_number = 2 are unfeasible. 
    Another constraint on elements_number and y_component_number is
    that elements_number is a multiple of y_component_number in order
    to work only on square or rectangular tiles.
    So we have this:
    #+BEGIN_LaTeX
     \[elements\,number \geq y\,component\,number\]    
     \[elements\,number\quad mod\quad y\,component\,number = 0\]    
    #+END_LaTeX
    There is also another kind of constraints more complex that reject
    combinations that would generate a too big kernel. In OpenCL and
    Cuda, kernel are limited by physical resources of the device, a
    kernel too big simply does not compile.

    For these reason it is necessary to use discrete and constrained
    optimization techniques. However we did not use yet such methods in
    order to avoid bias that could be induced by search methods that
    could bias our analysis. We put aside this aspect and for the
    moment we perform an exhaustive search only on feasible points
    using our predict objective function to estimate the minimum
    values of factors.  

   #+BEGIN_LaTeX
   \begin{figure}[htb]
   \centering
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/search_space_3D.pdf}
   \caption{\label{fig:search_space_3d}Search space 3D}
   \end{minipage}
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/search_space_3D_constrained_ugly.pdf}
   \caption{\label{fig:search_space_3d_constrained2}Constrained search space}
   \end{minipage}
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/search_space_3D_constrained.pdf}
   \caption{\label{fig:search_space_3d_constrained1}Constrained Search space but rounded}
   \end{minipage}
   \begin{minipage}{.45\linewidth}
   \includegraphics[width=\linewidth]{./img/search_space_3D_barrier.pdf}
   \caption{\label{fig:search_space_3d_barrier}Barrier approach}
   \end{minipage}
   \end{figure}
   #+END_LaTeX
*** TODO Sampling techniques
****                                                               :noexport:
     #+begin_src sh :results output :exports both
       ruby ../scripts/format_data.rb ../data/2016_04_08/pilipili2/18_08_24/test_space_2016_04_02_end_cleaned.yaml
     #+end_src
     
     #+RESULTS:
     
     #+begin_src R :results output graphics :file ./img/lhs_cover_constraints.pdf :exports both :width 6 :height 4 :session
       library(DoE.base)
       library(DoE.wrapper)

       df <- read.csv("/tmp/test.csv",strip.white=T,header=T)

       elements_number_val <- as.numeric(levels(as.factor(df$elements_number)))
       y_component_number_val <- as.numeric(levels(as.factor(df$y_component_number)))
       vector_length_val <- as.numeric(levels(as.factor(df$vector_length)))
       threads_number_val <- as.numeric(levels(as.factor(df$threads_number)))
       lws_y_val <- as.numeric(levels(as.factor(df$lws_y)))
       temporary_size_val <- as.numeric(levels(as.factor(df$temporary_size)))
       load_overlap_val <- levels(df$load_overlap)

       Design.1 <- lhs.design( type= "maximin" , nruns= 441 ,nfactors= 7 ,digits= NULL ,seed=20049 , factor.names=list(idx_elements_number = c(1,length(elements_number_val)), 
                                                                                                                       idx_y_component_number = c(1,length(y_component_number_val)),
                                                                                                                       idx_vector_length = c(1,length(vector_length_val)), 
                                                                                                                       idx_threads_number = c(1,length(threads_number_val)),
                                                                                                                       idx_temporary_size = c(1,length(temporary_size_val)), 
                                                                                                                       idx_lws_y = c(1,length(lws_y_val)), 
                                                                                                                       idx_load_overlap = c(1,length(load_overlap_val)) 
                                                                                                                       ) 
                              )

       Design.1.rounded <- round(Design.1) # Ideally we would like to express factor(local_work_size_index) but the Dopt.design does not know how to handle it

       set <- data.frame()
       for(i in 1:nrow(Design.1.rounded)){
           set <- rbind(set, df[ df$elements_number == elements_number_val[Design.1.rounded$idx_elements_number[i]]
                                & df$y_component_number == y_component_number_val[Design.1.rounded$idx_y_component_number[i]]
                                & df$vector_length == vector_length_val[Design.1.rounded$idx_vector_length[i]]
                                & df$threads_number == threads_number_val[Design.1.rounded$idx_threads_number[i]]
                                & df$lws_y == lws_y_val[Design.1.rounded$idx_lws_y[i]]
                                & df$temporary_size == temporary_size_val[Design.1.rounded$idx_temporary_size[i]]
                                & df$load_overlap == load_overlap_val[Design.1.rounded$idx_load_overlap[i]], ])
       }

       plot(set[,!names(set) %in% c("vector_recompute", "time_per_pixel")])
     #+end_src

     #+RESULTS:
     [[file:./img/lhs_cover_constraints.pdf]]

     #+begin_src R :results output graphics :file ./img/rs_cover_constraints.pdf :exports both :width 6 :height 4 :session
       random_set <- df[sample(1:nrow(df), size = 100, replace = FALSE),]
       plot(random_set[,!names(random_set) %in% c("vector_recompute", "time_per_pixel")])
     #+end_src

     #+RESULTS:
     [[file:./img/rs_cover_constraints.pdf]]
     
**** Screening design 
    The screening is generally a good starting to interrogate the
    search space as it allows to detect parameters that the biggest
    impact we wanted to use in a preliminary phase to detect estimate
    significant factors. For this it considers the points that are the furthest from
    each other. It samples only the minimum and maximum values of
    factors. Which means it only takes points that are at 
    the border of the search space. With our search space without
    constraints, this gives a total of 128 possible
    combinations. However because of the constraints of our search
    space we can only have at most 32 points. Lots of important points
    combining high values are lost hence lots of information, while
    points at the frontier could have been used instead. 

**** LHS
    We planned to use LHS sampling as a basis because of its good
    space covering characteristics. However it is not made for
    discrete and constrained problem. To use it we had to discretise
    the values which makes it lose its most interesting property which
    is the maximum distance between points. Hence we end up we multiple
    replicates. In addition, as it does not handle constraints to 
    select points, we need generate first a set of points with LHS and
    then we reject points using constraints. But to obtain
    approximately the wanted number of points we need to ask more
    points but we can not end up with the exact number of
    points. Finally in our case, the random
    sampling\ref{fig:rs_cover_constraints} provided the same 
    covering space capability\ref{fig:lhs_cover_constraints}  
    and we could obtain exactly the wanted 
    number of points. That is why we chose the random sampling over
    LHS. 

   #+BEGIN_LaTeX
   \begin{figure}[htb]
   \centering
   \begin{minipage}{\linewidth}
   \includegraphics[width=\linewidth]{./img/lhs_cover_constraints.pdf}
   \caption{\label{fig:lhs_cover_constraints}LHS space covering}
   \end{minipage}
   \begin{minipage}{\linewidth}
   \includegraphics[width=\linewidth]{./img/rs_cover_constraints.pdf}
   \caption{\label{fig:rs_cover_constraints}Random sampling space covering}
   \end{minipage}
   \end{figure}
   #+END_LaTeX    
** TODO Quantile regression
   R provides a function for computing quantile regression through the
   package quantreg. However with our data, sometimes it failed to compute the
   estimate of coefficients, it could not compute the standard
   error, the p-values, etc... which is crucial to access the quality
   of a model. Thus we had to implement another methods to perform
   quantile regression. There several ways to do it, we used a method
   that performs weighted least square regression iteratively to add
   more weight to the wanted quantile until the regression converges
   to it. We managed to get the correct coefficient, nevertheless we
   do not yet master this technique completely.  We do not know how to
   determine correctly the minimum number of iterations needed to
   converge to the solution. It is possible to add bound on weight but
   we do not know how to determine them correctly yet.
   The other difficulty with this method is that we do not really know
   how to interpret the standard error, how to compute confidence
   intervals and the R-squared values are too optimistic. Hence it is
   more complicated to make inferences about models. 
* TODO Future work
* Conclusion
* Thanks
#+LaTeX: \nocite{*}
#+LaTeX: \def\raggedright{}
\bibliographystyle{plain}
\bibliography{../../biblio.bib}


* Emacs Setup 							   :noexport:
  This document has local variables in its postembule, which should
  allow Org-mode to work seamlessly without any setup. If you're
  uncomfortable using such variables, you can safely ignore them at
  startup. Exporting may require that you copy them in your .emacs.

# Local Variables:
# eval:    (require 'org-install)
# eval:    (org-babel-do-load-languages 'org-babel-load-languages '( (sh . t) (R . t) (perl . t) (ditaa . t) ))
# eval:    (setq org-confirm-babel-evaluate nil)
# eval:    (unless (boundp 'org-latex-classes) (setq org-latex-classes nil))
# eval:    (add-to-list 'org-latex-classes '("memoir" "\\documentclass[smallextended]{memoir} \n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n  \\usepackage{graphicx}\n  \\usepackage{hyperref}" ("\\chapter{%s}" . "\\chapter*{%s}") ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("acm-proc-article-sp" "\\documentclass{acm_proc_article-sp}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (setq org-alphabetical-lists t)
# eval:    (setq org-src-fontify-natively t)
# eval:   (setq org-export-babel-evaluate nil)
# eval:   (setq ispell-local-dictionary "english")
# eval:   (eval (flyspell-mode t))
# eval:    (setq org-latex-listings 'minted)
# eval:    (setq org-latex-minted-options '(("bgcolor" "white") ("style" "tango") ("numbers" "left") ("numbersep" "5pt")))
# End:
